# Plan de D√©veloppement Sentiment Score - MyWebIntelligence API

**Date de cr√©ation**: 18 octobre 2025
**Version**: 1.1
**Statut**: Plan d√©taill√© pour impl√©mentation

---

## üéØ Objectif

Impl√©menter un syst√®me complet de d√©tection et d'analyse du sentiment des expressions crawl√©es dans l'API MyWebIntelligence, en respectant l'architecture du double crawler (sync/async) et les principes de parit√© legacy.

---

## ‚ö° D√©cisions Cl√©s (TL;DR)

### 1. Approche Technique : **Hybride TextBlob + OpenRouter**
- **Par d√©faut** : TextBlob (10 MB, 50ms, gratuit, FR/EN)
- **Haute qualit√©** : OpenRouter LLM (d√©j√† int√©gr√©, $0.003/analyse)
- **S√©lection** : Via flag `llm_validation` existant (pas de nouveau param√®tre !)

### 2. Installation Minimale
```bash
pip install textblob textblob-fr  # +15 MB total
python -m textblob.download_corpora
# OpenRouter d√©j√† disponible, rien √† installer !
```

### 3. Int√©gration Simple
- **1 nouveau champ** en DB : `sentiment_model` (pour tracer TextBlob vs LLM)
- **R√©utilise flag existant** : `llm_validation` contr√¥le aussi le sentiment
- **Double crawler** : M√™me logique dans async + sync (comme toujours)

### 4. Usage API
```bash
# Standard (TextBlob - gratuit)
POST /api/v2/lands/36/crawl {"llm_validation": false}

# Premium (LLM - haute qualit√©)
POST /api/v2/lands/36/crawl {"llm_validation": true}
```

### 5. Co√ªt Estim√©
- TextBlob : 0‚Ç¨
- OpenRouter (si 10% en LLM) : ~15‚Ç¨/mois pour 5000 analyses
- **Total** : <20‚Ç¨/mois

---

---

## üìä √âtat des Lieux

### ‚úÖ Existant
- **Champ `sentiment_score`** : Pr√©sent dans le mod√®le `Expression` (ligne 233, `models.py`)
  ```python
  sentiment_score = Column(Float, nullable=True)  # Analyse de sentiment
  ```
- **Valeur actuelle** : Toujours `null` dans toutes les expressions existantes
- **Type** : `Float` (accepte valeurs n√©gatives et positives)

### ‚ùå Manquant
- Service d'analyse de sentiment
- Int√©gration dans les crawlers (sync + async)
- Biblioth√®que d'analyse (TextBlob √† installer)
- **1 nouveau champ** : `sentiment_model` (pour tracer TextBlob vs LLM)
- Tests unitaires et d'int√©gration
- Documentation API

### üìã Nouveaux Champs N√©cessaires

Ajouter √† la table `expressions` :

```python
# Dans models.py (√† ajouter apr√®s sentiment_score ligne 233)
sentiment_label = Column(String(20), nullable=True)        # "positive", "neutral", "negative"
sentiment_confidence = Column(Float, nullable=True)         # 0.0 √† 1.0
sentiment_status = Column(String(30), nullable=True)        # "computed", "failed", "unsupported_lang", etc.
sentiment_model = Column(String(100), nullable=True)        # "textblob" ou "llm/claude-3.5-sonnet"
sentiment_computed_at = Column(DateTime, nullable=True)     # Timestamp du calcul
```

**Migration requise** : Alembic pour ajouter les 5 nouveaux champs.

---

## üèóÔ∏è Architecture Cible

### Composants Principaux

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SENTIMENT PIPELINE                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ  1. SentimentModelProvider (core/)                          ‚îÇ
‚îÇ     ‚îî‚îÄ Chargement lazy des mod√®les multilingues            ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  2. SentimentService (services/)                            ‚îÇ
‚îÇ     ‚îú‚îÄ D√©tection langue (r√©utilise module existant)        ‚îÇ
‚îÇ     ‚îú‚îÄ Appel mod√®le appropri√©                              ‚îÇ
‚îÇ     ‚îú‚îÄ Calibration score                                    ‚îÇ
‚îÇ     ‚îî‚îÄ Gestion fallbacks                                    ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  3. Int√©gration Crawlers                                    ‚îÇ
‚îÇ     ‚îú‚îÄ crawler_engine.py (async)                           ‚îÇ
‚îÇ     ‚îî‚îÄ crawler_engine_sync.py (sync) ‚ö†Ô∏è DOUBLE CRAWLER     ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  4. API Endpoints                                           ‚îÇ
‚îÇ     ‚îú‚îÄ GET /expressions/{id} (avec sentiment)              ‚îÇ
‚îÇ     ‚îî‚îÄ POST /lands/{id}/reprocess-sentiment (batch)        ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìã Plan de D√©veloppement D√©taill√©

---

## üîç √âTAPE 1 ‚Äì Cartographie Existant (Dur√©e: 1-2h)

### Objectif
Auditer l'infrastructure existante pour identifier les points d'int√©gration et les d√©pendances disponibles.

### Actions

#### 1.1 Audit Mod√®le de Donn√©es
- [x] **V√©rifier champ `sentiment_score` dans `Expression`**
  - Fichier: `MyWebIntelligenceAPI/app/db/models.py:233`
  - Type: `Float`
  - Nullable: `True`
  - Index: Non (√† consid√©rer si requ√™tes fr√©quentes)

- [ ] **V√©rifier sch√©mas Pydantic**
  - Fichier: `MyWebIntelligenceAPI/app/schemas/expression.py`
  - Action: S'assurer que `ExpressionOut` expose `sentiment_score`
  - V√©rifier √©galement `ExpressionCreate`, `ExpressionUpdate`

- [ ] **√âvaluer besoin de nouveaux champs**
  ```python
  # Proposition d'ajout dans models.py
  sentiment_label = Column(String(20), nullable=True)      # "positive", "neutral", "negative"
  sentiment_confidence = Column(Float, nullable=True)       # 0.0 √† 1.0
  sentiment_status = Column(String(20), nullable=True)      # "computed", "failed", "unsupported_lang"
  sentiment_model = Column(String(100), nullable=True)      # Mod√®le utilis√©
  sentiment_computed_at = Column(DateTime, nullable=True)   # Timestamp
  ```

#### 1.2 Tracer Flux d'Enrichissement
- [ ] **Identifier points d'injection dans crawlers**
  - `crawler_engine.py` (async): Ligne ~250-300 (apr√®s extraction contenu)
  - `crawler_engine_sync.py` (sync): Ligne ~200-250 (apr√®s extraction contenu)
  - ‚ö†Ô∏è **CRITIQUE**: Modifier LES DEUX crawlers (voir `ERREUR_DOUBLE_CRAWLER.md`)

- [ ] **Examiner pipeline readable**
  - Services: `readable_service.py`, `readable_celery_service.py`
  - Opportunit√©: Ajouter sentiment lors du traitement readable
  - Fichiers: `MyWebIntelligenceAPI/app/services/readable_*.py`

- [ ] **Identifier workers Celery**
  - Tasks: `MyWebIntelligenceAPI/app/tasks/`
  - V√©rifier si sentiment peut √™tre calcul√© de mani√®re asynchrone
  - Consid√©rer task d√©di√©e `sentiment_task.py`

#### 1.3 Inventaire D√©pendances & Contraintes
- [ ] **V√©rifier d√©pendances Python disponibles**
  ```bash
  # V√©rifier requirements.txt
  grep -E "(transformers|torch|tensorflow|scikit-learn)" MyWebIntelligenceAPI/requirements.txt
  ```

- [ ] **√âvaluer contraintes infrastructure**
  - CPU/RAM disponible dans containers Docker
  - Temps de r√©ponse acceptable (< 2s par expression id√©alement)
  - Espace disque pour mod√®les (~500MB par mod√®le)

- [ ] **Tester d√©tection de langue existante**
  ```python
  # Dans text_processing.py ou utils
  from app.core.text_processing import detect_language
  # V√©rifier si d√©j√† impl√©ment√© et performant
  ```

### Tests √âtape 1
```bash
# 1. Rechercher "sentiment" dans toute la codebase
rg "sentiment" MyWebIntelligenceAPI/ --type py

# 2. V√©rifier mod√®le Expression
grep -A 5 "sentiment_score" MyWebIntelligenceAPI/app/db/models.py

# 3. Inspecter fixtures de test
docker exec mywebclient-db-1 psql -U mwi_user -d mwi_db -c \
  "SELECT id, url, sentiment_score FROM expressions LIMIT 5;"
```

### Livrables √âtape 1
- [x] Liste des fichiers √† modifier avec num√©ros de lignes pr√©cis
- [ ] Document des d√©pendances disponibles/manquantes
- [ ] Sch√©ma du flux d'injection sentiment (diagramme)
- [ ] Rapport de contraintes (CPU, RAM, latence)

---

## üìö √âTAPE 2 ‚Äì Jeu de Validation & Exigences M√©tier (Dur√©e: 2-3h)

### Objectif
Constituer un corpus annot√© pour validation et d√©finir les crit√®res de qualit√©.

### Actions

#### 2.1 Constituer Corpus Annot√©
- [ ] **S√©lectionner 50-60 expressions r√©elles**
  ```sql
  -- √âchantillon √©quilibr√© FR/EN avec contenu vari√©
  SELECT id, url, title, readable, lang
  FROM expressions
  WHERE readable IS NOT NULL
    AND lang IN ('fr', 'en')
    AND word_count > 100
  ORDER BY RANDOM()
  LIMIT 30; -- 30 FR

  SELECT id, url, title, readable, lang
  FROM expressions
  WHERE readable IS NOT NULL
    AND lang = 'en'
    AND word_count > 100
  ORDER BY RANDOM()
  LIMIT 30; -- 30 EN
  ```

- [ ] **Annoter manuellement chaque texte**
  - Label: `positive`, `neutral`, `negative`
  - Score cible: `-1.0` (tr√®s n√©gatif) √† `+1.0` (tr√®s positif)
  - Confiance: Niveau de certitude de l'annotation (high/medium/low)
  - Format: CSV ou JSON
  ```json
  {
    "expression_id": 12345,
    "url": "https://example.com/article",
    "text_snippet": "Premi√®res 200 chars...",
    "manual_label": "positive",
    "manual_score": 0.75,
    "annotator_confidence": "high",
    "language": "fr",
    "notes": "Commentaires optionnels"
  }
  ```

- [ ] **Sauvegarder corpus dans Git**
  - Emplacement: `MyWebIntelligenceAPI/tests/data/sentiment_corpus.json`
  - Version control pour tra√ßabilit√©

#### 2.2 D√©finir Exigences M√©tier
- [ ] **Format de sortie**
  ```python
  # Dans Expression model
  sentiment_score: float      # -1.0 √† +1.0 (ou 0-1 selon mod√®le)
  sentiment_label: str        # "positive", "neutral", "negative"
  sentiment_confidence: float # 0.0 √† 1.0 (probabilit√© du mod√®le)
  sentiment_status: str       # "computed", "failed", "unsupported_lang", "no_content"
  ```

- [ ] **Seuils de d√©cision**
  - **Positif**: score > 0.6 (configurable)
  - **Neutre**: score entre -0.6 et 0.6
  - **N√©gatif**: score < -0.6
  - **Confiance minimale**: 0.5 (sinon status = "low_confidence")

- [ ] **Seuils d'alertes (optionnel)**
  - Expressions tr√®s n√©gatives (score < -0.8) ‚Üí flag pour review
  - Expressions polaris√©es (|score| > 0.9) ‚Üí potentiel contenu sensible

- [ ] **Tol√©rance latence**
  - **Crawl en temps r√©el**: < 500ms par expression (objectif stretch)
  - **Crawl batch/async**: < 2s par expression (acceptable)
  - **Reprocessing historique**: Pas de limite (job Celery)

#### 2.3 Cas Limites √† G√©rer
- [ ] **Langue non support√©e** (ex: allemand, espagnol)
  - Comportement: `sentiment_score = null`, `sentiment_status = "unsupported_lang"`

- [ ] **Contenu vide ou trop court**
  - Comportement: `sentiment_score = null`, `sentiment_status = "no_content"`
  - Seuil: < 50 caract√®res de contenu lisible

- [ ] **Texte mixte multilingue**
  - Comportement: Utiliser langue dominante d√©tect√©e
  - Fallback: Si langue dominante non FR/EN ‚Üí unsupported

- [ ] **Erreur mod√®le**
  - Comportement: `sentiment_score = null`, `sentiment_status = "failed"`
  - Logging d√©taill√© pour investigation

### Tests √âtape 2
```bash
# Reviewer annotations manuelles
python MyWebIntelligenceAPI/tests/scripts/validate_corpus.py

# V√©rifier distribution des labels
jq '.[] | .manual_label' tests/data/sentiment_corpus.json | sort | uniq -c

# Statistiques du corpus
jq '[.[] | .manual_score] | add/length' tests/data/sentiment_corpus.json
```

### Livrables √âtape 2
- [ ] Corpus annot√© (`sentiment_corpus.json`) avec 50-60 exemples
- [ ] Document sp√©cifications sentiment (format, seuils, alertes)
- [ ] Checklist des cas limites et comportements attendus

---

## ü§ñ √âTAPE 3 ‚Äì Choix et Encapsulation du Mod√®le (Dur√©e: 3-4h)

### Objectif
S√©lectionner et int√©grer un mod√®le de sentiment multilingue optimis√©.

### Actions

#### 3.1 S√©lection du Mod√®le

##### Option A: TextBlob (RECOMMAND√â - Ultra L√©ger) ‚≠ê
**Biblioth√®que**: `textblob` (dictionnaire + r√®gles)
- ‚úÖ **Avantages**:
  - **Ultra l√©ger**: 10 MB total (vs 1.25 GB pour transformers)
  - **Tr√®s rapide**: <50ms par analyse
  - **RAM minimale**: ~50 MB (vs 1-2 GB pour torch)
  - Simple √† int√©grer
  - Support FR avec extension `textblob-fr`
- ‚ùå **Inconv√©nients**:
  - Pr√©cision mod√©r√©e (bas√© sur dictionnaires)
  - Moins bon que deep learning sur textes complexes
- **Installation**:
  ```bash
  pip install textblob textblob-fr
  python -m textblob.download_corpora  # Une fois
  ```
- **Cas d'usage id√©al**:
  - Textes courts/moyens
  - Besoin de performance
  - Contraintes RAM/disque

##### Option B: VADER (Textes Courts/Informels)
**Biblioth√®que**: `vaderSentiment` (r√®gles + lexique)
- ‚úÖ **Avantages**:
  - **Minuscule**: 2 MB total
  - **Extr√™mement rapide**: <10ms
  - Excellent pour r√©seaux sociaux (√©mojis, slang)
  - G√®re ponctuation et casse
- ‚ùå **Inconv√©nients**:
  - **Anglais uniquement** (dealbreaker pour FR)
  - Moins bon sur textes formels/longs
- **Installation**:
  ```bash
  pip install vaderSentiment
  ```
- **Cas d'usage id√©al**:
  - Contenu EN uniquement
  - Tweets, commentaires courts

##### Option C: API OpenRouter (D√©j√† Int√©gr√©!) üéØ
**Service**: R√©utiliser `llm_validation_service.py` existant
- ‚úÖ **Avantages**:
  - **Z√©ro d√©pendance suppl√©mentaire** (d√©j√† dans le projet)
  - **Tr√®s haute qualit√©** (Claude 3.5 Sonnet)
  - **Multilingue parfait** (FR/EN/ES/...)
  - D√©j√† configur√© et test√©
- ‚ùå **Inconv√©nients**:
  - **Co√ªt**: ~$0.003 par analyse (15‚Ç¨ pour 5000 expressions)
  - **Latence**: 500ms-2s (r√©seau)
  - Requiert `OPENROUTER_ENABLED=true` et budget API
- **Installation**:
  ```bash
  # AUCUNE ! D√©j√† disponible
  ```
- **Cas d'usage id√©al**:
  - Haute qualit√© requise
  - Volume mod√©r√© (<10k expressions/mois)
  - Budget API disponible

##### Option D: Transformers (Haute Pr√©cision) ‚ö†Ô∏è LOURD
**Mod√®le**: `cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual`
- ‚úÖ **Avantages**:
  - Tr√®s haute pr√©cision (deep learning)
  - Support FR + EN + 6 autres langues
  - √âtat de l'art pour sentiment
- ‚ùå **Inconv√©nients**:
  - **Tr√®s lourd**: ~1.25 GB (torch + transformers)
  - **RAM √©lev√©e**: 1-2 GB par worker
  - **Temps de chargement**: 30-60s au d√©marrage
  - Latence mod√©r√©e (200-500ms)
- **Installation**:
  ```bash
  pip install transformers torch sentencepiece
  ```
- **Cas d'usage id√©al**:
  - Infrastructure robuste (GPU, RAM abondante)
  - Pr√©cision maximale requise
  - Volume tr√®s √©lev√© justifiant l'investissement

---

### üéØ **D√âCISION RECOMMAND√âE: Approche Hybride**

**Strat√©gie Intelligente** (meilleur compromis):

1. **Par D√©faut**: TextBlob (Option A) - L√©ger, rapide, gratuit
   - Utilis√© quand `llm_validation: false` (d√©faut)
   - Stocke `sentiment_model: "textblob"`

2. **Haute Qualit√©**: OpenRouter (Option C) - Pr√©cision maximale
   - **R√©utilise le flag existant** `llm_validation: true` du crawl
   - Utilis√© pour contenus critiques ou validation
   - Stocke `sentiment_model: "llm/claude-3.5-sonnet"`

**Avantages Hybride**:
- ‚úÖ D√©marrage imm√©diat avec TextBlob (l√©ger)
- ‚úÖ **Pas de nouveau param√®tre** - r√©utilise `llm_validation` existant
- ‚úÖ Coh√©rence : m√™me flag pour validation pertinence + sentiment
- ‚úÖ Co√ªt ma√Ætris√© (LLM seulement si n√©cessaire)
- ‚úÖ Flexibilit√© max

**Code de s√©lection** (r√©utilise flag existant):
```python
# Dans crawler_engine.py et crawler_engine_sync.py
if llm_validation and settings.OPENROUTER_ENABLED:
    # Haute qualit√© (payant) - validation + sentiment LLM
    result = await analyze_sentiment_llm(text, language)
else:
    # Standard (gratuit, rapide) - TextBlob
    result = analyze_sentiment_textblob(text, language)
```

**Exemple d'utilisation API**:
```bash
# Crawl standard avec TextBlob
curl -X POST "/api/v2/lands/36/crawl" \
  -d '{"limit": 10, "llm_validation": false}'
# ‚Üí sentiment via TextBlob (gratuit, rapide)

# Crawl haute qualit√© avec LLM
curl -X POST "/api/v2/lands/36/crawl" \
  -d '{"limit": 10, "llm_validation": true}'
# ‚Üí validation pertinence + sentiment via Claude (payant, pr√©cis)
```

#### 3.2 Impl√©menter `SentimentModelProvider` (Approche Hybride)

**Nouveau fichier**: `MyWebIntelligenceAPI/app/core/sentiment_provider.py`

```python
"""
Sentiment Analysis Provider for MyWebIntelligence API

Hybrid approach:
- TextBlob (default): Lightweight, fast, free
- OpenRouter (optional): High quality, LLM-based

Supports FR/EN with automatic fallback.
"""

import logging
from typing import Optional, Dict, Any, Literal

from app.config import settings

logger = logging.getLogger(__name__)

SentimentLabel = Literal["positive", "neutral", "negative"]

class SentimentModelProvider:
    """
    Hybrid sentiment analysis provider.

    Methods:
    - TextBlob: Fast, lightweight (default)
    - OpenRouter: High quality (optional, requires API key)
    """

    # Configuration
    SUPPORTED_LANGUAGES = ["fr", "en"]  # TextBlob supports these well
    TEXTBLOB_AVAILABLE = False
    OPENROUTER_AVAILABLE = False

    def __init__(self):
        """Initialize provider and check available methods."""
        # Check TextBlob availability
        try:
            from textblob import TextBlob
            self.TEXTBLOB_AVAILABLE = True
            logger.info("‚úÖ TextBlob sentiment provider available")
        except ImportError:
            logger.warning("‚ùå TextBlob not installed (pip install textblob textblob-fr)")

        # Check OpenRouter availability
        if settings.OPENROUTER_ENABLED and settings.OPENROUTER_API_KEY:
            self.OPENROUTER_AVAILABLE = True
            logger.info("‚úÖ OpenRouter sentiment provider available")

        if not self.TEXTBLOB_AVAILABLE and not self.OPENROUTER_AVAILABLE:
            logger.error("‚ùå No sentiment provider available!")

    def is_language_supported(self, lang: str) -> bool:
        """Check if language is supported."""
        return lang.lower() in self.SUPPORTED_LANGUAGES

    def _analyze_textblob(self, text: str, language: str) -> Dict[str, Any]:
        """
        Analyze sentiment using TextBlob.

        Fast, lightweight, rule-based approach.
        """
        try:
            from textblob import TextBlob

            # Use French-specific TextBlob if needed
            if language == "fr":
                try:
                    from textblob_fr import PatternTagger, PatternAnalyzer
                    blob = TextBlob(text, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())
                except ImportError:
                    logger.warning("textblob-fr not available, using English analyzer")
                    blob = TextBlob(text)
            else:
                blob = TextBlob(text)

            # Get polarity (-1.0 to +1.0)
            polarity = blob.sentiment.polarity

            # Determine label with thresholds
            if polarity > 0.1:
                label = "positive"
            elif polarity < -0.1:
                label = "negative"
            else:
                label = "neutral"

            return {
                "score": round(polarity, 3),
                "label": label,
                "confidence": round(abs(polarity), 3),  # Use absolute polarity as confidence
                "status": "computed",
                "model": "textblob"
            }

        except Exception as e:
            logger.error(f"TextBlob analysis failed: {e}")
            return {
                "score": None,
                "label": None,
                "confidence": None,
                "status": "failed",
                "model": None
            }

    async def _analyze_openrouter(self, text: str, language: str) -> Dict[str, Any]:
        """
        Analyze sentiment using OpenRouter LLM.

        High quality, but slower and costs money.
        """
        try:
            from app.services.llm_validation_service import LLMValidationService
            import json

            # Prepare prompt
            prompt = f"""Analyze the sentiment of the following text in {language.upper()}.

Text: {text[:1000]}

Respond with ONLY a valid JSON object (no markdown, no extra text):
{{"sentiment": "positive" or "neutral" or "negative", "score": -1.0 to 1.0, "confidence": 0.0 to 1.0}}

Example response:
{{"sentiment": "positive", "score": 0.75, "confidence": 0.9}}"""

            # Call LLM service
            llm_service = LLMValidationService()
            response_text = await llm_service._call_openrouter(
                prompt=prompt,
                temperature=0.0  # Deterministic for sentiment
            )

            # Parse JSON response
            # Remove markdown code blocks if present
            response_clean = response_text.strip()
            if response_clean.startswith("```"):
                # Extract JSON from markdown
                lines = response_clean.split("\n")
                response_clean = "\n".join(lines[1:-1])

            result = json.loads(response_clean)

            return {
                "score": round(float(result["score"]), 3),
                "label": result["sentiment"],
                "confidence": round(float(result["confidence"]), 3),
                "status": "computed",
                "model": f"llm/{settings.OPENROUTER_MODEL}"
            }

        except Exception as e:
            logger.error(f"OpenRouter analysis failed: {e}", exc_info=True)
            return {
                "score": None,
                "label": None,
                "confidence": None,
                "status": "failed",
                "model": None
            }

    async def analyze_sentiment(
        self,
        text: str,
        language: str = "en",
        use_llm: bool = False
    ) -> Dict[str, Any]:
        """
        Analyze sentiment of text.

        Args:
            text: Text to analyze (cleaned, readable content preferred)
            language: ISO 639-1 language code

        Returns:
            Dict with:
                - score: float (-1.0 to +1.0)
                - label: str ("positive", "neutral", "negative")
                - confidence: float (0.0 to 1.0)
                - status: str ("computed", "unsupported_lang", "no_content", "failed")
                - model: str (model name used)

        Example:
            >>> provider = SentimentModelProvider()
            >>> result = provider.analyze_sentiment("This is great!", "en")
            >>> print(result)
            {
                'score': 0.95,
                'label': 'positive',
                'confidence': 0.95,
                'status': 'computed',
                'model': 'cardiffnlp/twitter-xlm-roberta...'
            }
        """
        # Validation
        if not text or len(text.strip()) < 10:
            return {
                "score": None,
                "label": None,
                "confidence": None,
                "status": "no_content",
                "model": None
            }

        if not self.is_language_supported(language):
            logger.warning(f"Unsupported language: {language}")
            return {
                "score": None,
                "label": None,
                "confidence": None,
                "status": "unsupported_lang",
                "model": None
            }

        try:
            # Load model (cached)
            model = self._load_model()

            # Truncate very long texts (keep first 512 tokens worth ~2000 chars)
            text_truncated = text[:2000] if len(text) > 2000 else text

            # Analyze
            result = model(text_truncated)[0]

            # Extract and normalize
            raw_label = result["label"]
            confidence = result["score"]

            normalized_label = self._normalize_label(raw_label)
            sentiment_score = self._score_to_range(normalized_label, confidence)

            logger.debug(
                f"Sentiment analyzed: {normalized_label} "
                f"(score={sentiment_score:.2f}, conf={confidence:.2f})"
            )

            return {
                "score": round(sentiment_score, 3),
                "label": normalized_label,
                "confidence": round(confidence, 3),
                "status": "computed",
                "model": self.DEFAULT_MODEL
            }

        except Exception as e:
            logger.error(f"Sentiment analysis failed: {e}", exc_info=True)
            return {
                "score": None,
                "label": None,
                "confidence": None,
                "status": "failed",
                "model": None
            }
```

#### 3.3 Int√©gration Pipeline de Normalisation

**Modifier**: `MyWebIntelligenceAPI/app/utils/text_utils.py`

```python
def prepare_text_for_sentiment(content: str, max_length: int = 2000) -> str:
    """
    Prepare text for sentiment analysis.

    - Strip HTML tags (if any remain)
    - Remove excessive whitespace
    - Truncate to max_length
    - Preserve paragraph structure

    Args:
        content: Raw or readable content
        max_length: Maximum characters to keep

    Returns:
        Cleaned text ready for sentiment analysis
    """
    import re
    from bs4 import BeautifulSoup

    # Remove HTML tags if present
    soup = BeautifulSoup(content, 'html.parser')
    text = soup.get_text(separator=' ', strip=True)

    # Normalize whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    # Truncate intelligently (try to break at sentence)
    if len(text) > max_length:
        text = text[:max_length]
        last_period = text.rfind('.')
        if last_period > max_length * 0.8:  # If period found in last 20%
            text = text[:last_period + 1]

    return text
```

### Tests √âtape 3
```python
# MyWebIntelligenceAPI/tests/unit/test_sentiment_provider.py

import pytest
from app.core.sentiment_provider import SentimentModelProvider

@pytest.fixture
def provider():
    return SentimentModelProvider()

def test_positive_sentiment_en(provider):
    result = provider.analyze_sentiment(
        "This movie is absolutely fantastic! I loved every moment.",
        language="en"
    )
    assert result["status"] == "computed"
    assert result["label"] == "positive"
    assert result["score"] > 0.5
    assert result["confidence"] > 0.7

def test_negative_sentiment_fr(provider):
    result = provider.analyze_sentiment(
        "Ce film est vraiment nul. Une perte de temps totale.",
        language="fr"
    )
    assert result["status"] == "computed"
    assert result["label"] == "negative"
    assert result["score"] < -0.5

def test_neutral_sentiment(provider):
    result = provider.analyze_sentiment(
        "The weather today is cloudy with some sun.",
        language="en"
    )
    assert result["status"] == "computed"
    # Note: neutral detection is harder, accept any label
    assert result["label"] in ["positive", "neutral", "negative"]

def test_unsupported_language(provider):
    result = provider.analyze_sentiment(
        "Êó•Êú¨Ë™û„ÅÆ„ÉÜ„Ç≠„Çπ„Éà",
        language="ja"
    )
    assert result["status"] == "unsupported_lang"
    assert result["score"] is None

def test_empty_content(provider):
    result = provider.analyze_sentiment("", language="en")
    assert result["status"] == "no_content"
    assert result["score"] is None

def test_very_short_content(provider):
    result = provider.analyze_sentiment("OK", language="en")
    assert result["status"] == "no_content"

def test_model_caching(provider):
    """Verify model is loaded once and cached."""
    result1 = provider.analyze_sentiment("Test 1", "en")
    result2 = provider.analyze_sentiment("Test 2", "en")

    # Both should succeed
    assert result1["status"] == "computed"
    assert result2["status"] == "computed"

    # Check cache
    assert len(provider._models) == 1

def test_label_normalization(provider):
    """Test internal label mapping."""
    assert provider._normalize_label("LABEL_2") == "positive"
    assert provider._normalize_label("LABEL_0") == "negative"
    assert provider._normalize_label("Positive") == "positive"

def test_score_conversion(provider):
    """Test score range conversion."""
    assert provider._score_to_range("positive", 0.9) == 0.9
    assert provider._score_to_range("negative", 0.8) == -0.8
    assert provider._score_to_range("neutral", 0.5) == 0.0

# Performance test
def test_latency_budget(provider):
    """Ensure sentiment analysis completes in < 2s."""
    import time
    text = "This is a test text for performance measurement. " * 20

    start = time.time()
    result = provider.analyze_sentiment(text, "en")
    duration = time.time() - start

    assert result["status"] == "computed"
    assert duration < 2.0  # Budget: 2 seconds max
```

**Ex√©cution**:
```bash
cd MyWebIntelligenceAPI
pytest tests/unit/test_sentiment_provider.py -v
```

### Livrables √âtape 3
- [ ] Fichier `sentiment_provider.py` avec classe compl√®te
- [ ] Tests unitaires (`test_sentiment_provider.py`) avec 10+ cas
- [ ] Documentation docstrings (Google style)
- [ ] Mesure de latence sur √©chantillon (moyenne < 500ms)

---

## üîß √âTAPE 4 ‚Äì Service d'Enrichissement (Dur√©e: 2-3h)

### Objectif
Cr√©er un service orchestrant d√©tection de langue + analyse sentiment + calibration.

### Actions

#### 4.1 Cr√©er `SentimentService`

**Nouveau fichier**: `MyWebIntelligenceAPI/app/services/sentiment_service.py`

```python
"""
Sentiment Enrichment Service for MyWebIntelligence API

Orchestrates language detection + sentiment analysis for expressions.
Integrates with existing text_processing module.
"""

import logging
from typing import Optional, Dict, Any

from app.core.sentiment_provider import SentimentModelProvider
from app.core.text_processing import detect_language  # Assuming exists
from app.utils.text_utils import prepare_text_for_sentiment

logger = logging.getLogger(__name__)

class SentimentService:
    """
    Service for enriching expressions with sentiment scores.

    Workflow:
    1. Detect language (reuse existing module)
    2. Prepare text (clean, truncate)
    3. Call sentiment model
    4. Calibrate score if needed
    5. Return structured result
    """

    def __init__(self):
        """Initialize service with model provider."""
        self.provider = SentimentModelProvider()
        logger.info("SentimentService initialized")

    def enrich_expression_sentiment(
        self,
        content: Optional[str],
        readable: Optional[str] = None,
        language: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Enrich an expression with sentiment analysis.

        Args:
            content: Raw HTML content (fallback)
            readable: Readable markdown content (preferred)
            language: Detected language (ISO 639-1), auto-detect if None

        Returns:
            Dict ready for Expression update:
                - sentiment_score: float or None
                - sentiment_label: str or None
                - sentiment_confidence: float or None
                - sentiment_status: str
                - sentiment_model: str or None
                - sentiment_computed_at: datetime or None

        Example:
            >>> service = SentimentService()
            >>> result = service.enrich_expression_sentiment(
            ...     readable="This article is very positive!",
            ...     language="en"
            ... )
            >>> print(result["sentiment_score"])
            0.87
        """
        from datetime import datetime, timezone

        # Choose best text source
        text = readable if readable else content

        if not text or len(text.strip()) < 10:
            logger.debug("No usable content for sentiment analysis")
            return {
                "sentiment_score": None,
                "sentiment_label": None,
                "sentiment_confidence": None,
                "sentiment_status": "no_content",
                "sentiment_model": None,
                "sentiment_computed_at": None
            }

        # Detect language if not provided
        if not language:
            try:
                language = detect_language(text)
                logger.debug(f"Detected language: {language}")
            except Exception as e:
                logger.warning(f"Language detection failed: {e}")
                language = "en"  # Fallback to English

        # Prepare text
        clean_text = prepare_text_for_sentiment(text, max_length=2000)

        # Analyze sentiment
        result = self.provider.analyze_sentiment(clean_text, language)

        # Add timestamp if computed
        timestamp = None
        if result["status"] == "computed":
            timestamp = datetime.now(timezone.utc)

        return {
            "sentiment_score": result["score"],
            "sentiment_label": result["label"],
            "sentiment_confidence": result["confidence"],
            "sentiment_status": result["status"],
            "sentiment_model": result["model"],
            "sentiment_computed_at": timestamp
        }

    def should_compute_sentiment(
        self,
        existing_score: Optional[float],
        force_recompute: bool = False
    ) -> bool:
        """
        Determine if sentiment should be computed.

        Args:
            existing_score: Current sentiment_score in DB
            force_recompute: Override check

        Returns:
            True if should compute/recompute
        """
        if force_recompute:
            return True

        # Compute if never computed
        if existing_score is None:
            return True

        # Already computed and not forcing
        return False
```

#### 4.2 D√©finir Seuil de Confiance

**Dans `sentiment_service.py`**, ajouter:

```python
# Configuration thresholds
MIN_CONFIDENCE_THRESHOLD = 0.5  # Below this, set status = "low_confidence"

def enrich_expression_sentiment(...) -> Dict[str, Any]:
    # ... (code pr√©c√©dent)

    result = self.provider.analyze_sentiment(clean_text, language)

    # Apply confidence threshold
    if result["status"] == "computed" and result["confidence"] < MIN_CONFIDENCE_THRESHOLD:
        logger.info(
            f"Low confidence sentiment: {result['confidence']:.2f} < {MIN_CONFIDENCE_THRESHOLD}"
        )
        result["status"] = "low_confidence"
        # Keep score but flag as uncertain

    # ... (return)
```

#### 4.3 Instrumentation (M√©triques)

**Optionnel mais recommand√©**: Ajouter m√©triques Prometheus/StatsD

```python
# Dans sentiment_service.py
import time
from app.utils.metrics import metrics_client  # Assuming exists

def enrich_expression_sentiment(...) -> Dict[str, Any]:
    start_time = time.time()

    # ... (processing)

    result = self.provider.analyze_sentiment(clean_text, language)

    # Track metrics
    duration_ms = (time.time() - start_time) * 1000

    # Log processing time
    metrics_client.timing('sentiment.analysis.duration', duration_ms)
    metrics_client.increment(f'sentiment.status.{result["status"]}')

    if result["status"] == "computed":
        metrics_client.histogram('sentiment.score', result["score"])

    logger.info(f"Sentiment computed in {duration_ms:.0f}ms: {result['label']}")

    # ... (return)
```

### Tests √âtape 4

```python
# MyWebIntelligenceAPI/tests/unit/test_sentiment_service.py

import pytest
from app.services.sentiment_service import SentimentService

@pytest.fixture
def service():
    return SentimentService()

def test_enrich_with_readable_content(service):
    result = service.enrich_expression_sentiment(
        content=None,
        readable="This is a wonderful day with great weather!",
        language="en"
    )

    assert result["sentiment_status"] == "computed"
    assert result["sentiment_score"] > 0.3
    assert result["sentiment_label"] == "positive"
    assert result["sentiment_computed_at"] is not None

def test_enrich_with_fallback_to_content(service):
    """Should use content if readable is None."""
    result = service.enrich_expression_sentiment(
        content="<p>Amazing experience!</p>",
        readable=None,
        language="en"
    )

    assert result["sentiment_status"] == "computed"
    assert result["sentiment_label"] == "positive"

def test_enrich_with_language_autodetection(service):
    """Should auto-detect language if not provided."""
    result = service.enrich_expression_sentiment(
        readable="Ceci est un texte en fran√ßais."
    )

    # Should work even without explicit language
    assert result["sentiment_status"] in ["computed", "unsupported_lang"]

def test_enrich_no_content(service):
    result = service.enrich_expression_sentiment(
        content=None,
        readable=None
    )

    assert result["sentiment_status"] == "no_content"
    assert result["sentiment_score"] is None

def test_should_compute_new_expression(service):
    """Should compute for expressions without sentiment."""
    assert service.should_compute_sentiment(existing_score=None) is True

def test_should_not_recompute_existing(service):
    """Should not recompute unless forced."""
    assert service.should_compute_sentiment(existing_score=0.5) is False

def test_should_force_recompute(service):
    """Should recompute when forced."""
    assert service.should_compute_sentiment(
        existing_score=0.5,
        force_recompute=True
    ) is True

def test_low_confidence_handling(service, monkeypatch):
    """Should flag low confidence results."""
    # Mock provider to return low confidence
    def mock_analyze(text, lang):
        return {
            "score": 0.3,
            "label": "neutral",
            "confidence": 0.45,  # Below 0.5 threshold
            "status": "computed",
            "model": "mock"
        }

    monkeypatch.setattr(service.provider, "analyze_sentiment", mock_analyze)

    result = service.enrich_expression_sentiment(
        readable="Some text",
        language="en"
    )

    assert result["sentiment_status"] == "low_confidence"
    assert result["sentiment_score"] == 0.3  # Score kept but flagged
```

**Ex√©cution**:
```bash
pytest tests/unit/test_sentiment_service.py -v
```

### Livrables √âtape 4
- [ ] Fichier `sentiment_service.py` avec orchestration compl√®te
- [ ] Tests unitaires service (8+ cas de test)
- [ ] Documentation workflow dans docstrings
- [ ] M√©triques instrument√©es (optionnel)

---

## üï∑Ô∏è √âTAPE 5 ‚Äì Int√©gration Pipeline Crawler (Dur√©e: 3-4h)

### Objectif
‚ö†Ô∏è **CRITIQUE**: Int√©grer sentiment dans LES DEUX crawlers (async + sync).

### Actions

#### 5.1 Int√©gration dans `crawler_engine.py` (ASYNC)

**Fichier**: `MyWebIntelligenceAPI/app/core/crawler_engine.py`

**Localisation**: Apr√®s extraction de contenu (ligne ~250-300)

```python
# Dans la m√©thode AsyncCrawlerEngine._process_expression ou √©quivalent

from app.services.sentiment_service import SentimentService

class AsyncCrawlerEngine:
    def __init__(self, ...):
        # ... existing code ...
        self.sentiment_service = SentimentService()  # ‚úÖ NOUVEAU

    async def _process_expression(
        self,
        expr_url: str,
        expression: Expression,
        depth: int,
        # ...
    ):
        # ... existing extraction logic ...

        # After content extraction and metadata extraction
        extraction_result = await self.content_extractor.extract_content(
            html=response_html,
            url=expr_url
        )

        # Update expression with extracted data
        expression.content = extraction_result.get('content')
        expression.readable = extraction_result.get('readable')
        expression.title = extraction_result.get('title')
        # ... other fields ...

        # ‚úÖ NOUVEAU: Enrich with sentiment
        if settings.ENABLE_SENTIMENT_ANALYSIS:  # Master switch
            try:
                # Determine method: TextBlob (default) or LLM (if llm_validation=true)
                use_llm = llm_validation and settings.OPENROUTER_ENABLED

                sentiment_data = await self.sentiment_service.enrich_expression_sentiment(
                    content=expression.content,
                    readable=expression.readable,
                    language=expression.lang,  # Use detected language
                    use_llm=use_llm  # ‚úÖ R√©utilise le flag llm_validation
                )

                # Update expression with sentiment data
                expression.sentiment_score = sentiment_data["sentiment_score"]
                expression.sentiment_label = sentiment_data["sentiment_label"]
                expression.sentiment_confidence = sentiment_data["sentiment_confidence"]
                expression.sentiment_status = sentiment_data["sentiment_status"]
                expression.sentiment_model = sentiment_data["sentiment_model"]
                expression.sentiment_computed_at = sentiment_data["sentiment_computed_at"]

                logger.debug(
                    f"Sentiment enriched for {expr_url}: "
                    f"{sentiment_data['sentiment_label']} ({sentiment_data['sentiment_score']}) "
                    f"via {sentiment_data['sentiment_model']}"
                )
            except Exception as e:
                logger.error(f"Sentiment enrichment failed for {expr_url}: {e}")
                # Continue without sentiment (non-blocking)

        # ... rest of processing ...
```

#### 5.2 Int√©gration dans `crawler_engine_sync.py` (SYNC)

**‚ö†Ô∏è DOUBLE CRAWLER - NE PAS OUBLIER !**

**Fichier**: `MyWebIntelligenceAPI/app/core/crawler_engine_sync.py`

**M√äME LOGIQUE** que async, ajuster pour code synchrone:

```python
from app.services.sentiment_service import SentimentService

class SyncCrawlerEngine:
    def __init__(self, ...):
        # ... existing code ...
        self.sentiment_service = SentimentService()  # ‚úÖ NOUVEAU

    def _process_expression(
        self,
        expr_url: str,
        expression: Expression,
        depth: int,
        # ...
    ):
        # ... existing extraction logic ...

        extraction_result = self.content_extractor.extract_content_sync(
            html=response_html,
            url=expr_url
        )

        # ... update expression fields ...

        # ‚úÖ NOUVEAU: Enrich with sentiment (IDENTIQUE √† async)
        if settings.ENABLE_SENTIMENT_ANALYSIS:  # Master switch
            try:
                # Determine method: TextBlob (default) or LLM (if llm_validation=true)
                use_llm = llm_validation and settings.OPENROUTER_ENABLED

                # Note: sentiment_service methods are sync-compatible
                sentiment_data = self.sentiment_service.enrich_expression_sentiment(
                    content=expression.content,
                    readable=expression.readable,
                    language=expression.lang,
                    use_llm=use_llm  # ‚úÖ R√©utilise le flag llm_validation
                )

                expression.sentiment_score = sentiment_data["sentiment_score"]
                expression.sentiment_label = sentiment_data["sentiment_label"]
                expression.sentiment_confidence = sentiment_data["sentiment_confidence"]
                expression.sentiment_status = sentiment_data["sentiment_status"]
                expression.sentiment_model = sentiment_data["sentiment_model"]
                expression.sentiment_computed_at = sentiment_data["sentiment_computed_at"]

                logger.debug(
                    f"[SYNC] Sentiment enriched for {expr_url}: "
                    f"{sentiment_data['sentiment_label']} via {sentiment_data['sentiment_model']}"
                )
            except Exception as e:
                logger.error(f"[SYNC] Sentiment enrichment failed: {e}")

        # ... rest ...
```

#### 5.3 Configuration (R√©utilise Settings Existants)

**Fichier**: `MyWebIntelligenceAPI/app/config.py`

```python
class Settings(BaseSettings):
    # ... existing settings ...

    # Sentiment Analysis (‚úÖ R√©utilise OPENROUTER settings existants)
    ENABLE_SENTIMENT_ANALYSIS: bool = True  # ‚úÖ NOUVEAU - Master switch
    SENTIMENT_MIN_CONFIDENCE: float = 0.5
    SENTIMENT_SUPPORTED_LANGUAGES: str = "fr,en"  # Comma-separated

    # OpenRouter (‚úÖ D√âJ√Ä EXISTANT - utilis√© pour LLM validation + sentiment)
    OPENROUTER_ENABLED: bool = False
    OPENROUTER_API_KEY: str = ""
    OPENROUTER_MODEL: str = "anthropic/claude-3.5-sonnet"

    class Config:
        env_file = ".env"
```

**Fichier**: `MyWebIntelligenceAPI/.env`

```bash
# Sentiment Analysis Configuration
ENABLE_SENTIMENT_ANALYSIS=true  # ‚úÖ NOUVEAU - Active/d√©sactive sentiment globalement
SENTIMENT_MIN_CONFIDENCE=0.5
SENTIMENT_SUPPORTED_LANGUAGES=fr,en

# OpenRouter (‚úÖ D√âJ√Ä EXISTANT - pas de changement)
OPENROUTER_ENABLED=true
OPENROUTER_API_KEY=sk-or-v1-your-key
OPENROUTER_MODEL=anthropic/claude-3.5-sonnet
```

**Note importante** :
- `ENABLE_SENTIMENT_ANALYSIS` : Active/d√©sactive le sentiment globalement
- `llm_validation` (param√®tre crawl) : Choix TextBlob vs LLM
- Pas besoin de nouveau flag `enable_llm_sentiment` !

#### 5.4 V√©rification R√®gle Double Crawler

**Checklist OBLIGATOIRE** (voir `ERREUR_DOUBLE_CRAWLER.md`):

- [ ] ‚úÖ Modifier `crawler_engine.py` (async)
- [ ] ‚úÖ Modifier `crawler_engine_sync.py` (sync)
- [ ] ‚úÖ V√©rifier que les deux ont la M√äME logique sentiment
- [ ] ‚úÖ Tester avec Celery (pas seulement l'API directe)

**V√©rification**:
```bash
# Chercher "sentiment" dans les DEUX crawlers
grep -n "sentiment_service" MyWebIntelligenceAPI/app/core/crawler_engine.py
grep -n "sentiment_service" MyWebIntelligenceAPI/app/core/crawler_engine_sync.py

# Les deux doivent avoir des lignes similaires !
```

### Tests √âtape 5

#### Test d'Int√©gration Crawler Async

```python
# MyWebIntelligenceAPI/tests/integration/test_crawler_sentiment.py

import pytest
from app.core.crawler_engine import AsyncCrawlerEngine
from app.db.models import Expression, Land

@pytest.mark.asyncio
async def test_crawler_enriches_sentiment(db_session, test_land):
    """Verify async crawler adds sentiment to expressions."""

    # Create crawler
    crawler = AsyncCrawlerEngine(db_session=db_session)

    # Crawl a test URL with clear sentiment
    test_url = "https://httpbin.org/html"  # Neutral content

    # ... (setup expression in DB)

    # Crawl
    await crawler.crawl_url(test_url, land_id=test_land.id, depth=0)

    # Retrieve expression
    expression = db_session.query(Expression).filter_by(url=test_url).first()

    # Assertions
    assert expression is not None
    assert expression.sentiment_status in ["computed", "unsupported_lang", "no_content"]

    if expression.sentiment_status == "computed":
        assert expression.sentiment_score is not None
        assert expression.sentiment_label in ["positive", "neutral", "negative"]
        assert expression.sentiment_confidence is not None
        assert expression.sentiment_model is not None
        assert expression.sentiment_computed_at is not None
```

#### Test Crawler Sync (Celery)

```bash
# MyWebIntelligenceAPI/tests/test-crawl-sentiment.sh

#!/bin/bash
# Test sentiment integration in sync crawler (Celery)

# ... (authentication, create land)

# Crawl with sentiment enabled
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "limit": 3,
    "enable_sentiment": true
  }'

# Wait for completion
sleep 30

# Verify sentiment in DB
docker exec mywebclient-db-1 psql -U mwi_user -d mwi_db -c "
  SELECT
    id,
    url,
    sentiment_score,
    sentiment_label,
    sentiment_status
  FROM expressions
  WHERE land_id = ${LAND_ID}
  LIMIT 5;
"
```

### Livrables √âtape 5
- [ ] `crawler_engine.py` modifi√© avec sentiment
- [ ] `crawler_engine_sync.py` modifi√© (IDENTIQUE)
- [ ] Feature flag dans `config.py`
- [ ] Tests int√©gration (1 async + 1 sync)
- [ ] Documentation dans `AGENTS.md` (nouvelle section sentiment)

---

## üì° √âTAPE 6 ‚Äì Persistance & API (Dur√©e: 2h)

### Objectif
Exposer les champs sentiment dans les sch√©mas Pydantic et endpoints API.

### Actions

#### 6.1 Mettre √† Jour Sch√©mas Pydantic

**Fichier**: `MyWebIntelligenceAPI/app/schemas/expression.py`

```python
from typing import Optional
from datetime import datetime
from pydantic import BaseModel, Field

class ExpressionBase(BaseModel):
    # ... existing fields ...

    # Sentiment Analysis (‚úÖ NOUVEAU)
    sentiment_score: Optional[float] = Field(
        None,
        description="Sentiment score from -1.0 (negative) to +1.0 (positive)"
    )
    sentiment_label: Optional[str] = Field(
        None,
        description="Sentiment label: positive, neutral, or negative"
    )
    sentiment_confidence: Optional[float] = Field(
        None,
        description="Model confidence score (0.0 to 1.0)"
    )
    sentiment_status: Optional[str] = Field(
        None,
        description="Status: computed, failed, unsupported_lang, no_content, low_confidence"
    )
    sentiment_model: Optional[str] = Field(
        None,
        description="Model used for sentiment analysis"
    )
    sentiment_computed_at: Optional[datetime] = Field(
        None,
        description="Timestamp when sentiment was computed"
    )

class ExpressionOut(ExpressionBase):
    """Schema for Expression API output."""
    id: int
    url: str
    # ... other output fields ...

    # Sentiment fields inherited from Base

    class Config:
        from_attributes = True

class ExpressionCreate(BaseModel):
    """Schema for creating Expression."""
    # Sentiment fields NOT in create (computed post-creation)
    pass

class ExpressionUpdate(BaseModel):
    """Schema for updating Expression."""
    # ... existing updateable fields ...

    # Allow manual sentiment override (optional)
    sentiment_score: Optional[float] = None
    sentiment_label: Optional[str] = None
```

#### 6.2 V√©rifier Endpoints Exposent Sentiment

**Fichier**: `MyWebIntelligenceAPI/app/api/v2/endpoints/lands_v2.py`

```python
@router.get("/{land_id}/expressions", response_model=List[ExpressionOut])
async def get_land_expressions(
    land_id: int,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user),
    # ... filters ...
):
    """
    Get expressions for a land.

    Returns expressions with all fields including sentiment analysis.
    """
    # ... existing query ...

    # ExpressionOut schema will automatically include sentiment fields
    return expressions
```

**V√©rifier aussi**:
- `GET /api/v2/lands/{id}/expressions/{expr_id}`
- `POST /api/v1/export/direct` (CSV exports)

#### 6.3 Documenter dans OpenAPI

Pydantic g√©n√®re automatiquement la doc OpenAPI, mais v√©rifier:

```bash
# Acc√©der √† la doc
open http://localhost:8000/docs

# V√©rifier sch√©ma ExpressionOut inclut:
# - sentiment_score
# - sentiment_label
# - sentiment_confidence
# - sentiment_status
# - sentiment_model
# - sentiment_computed_at
```

#### 6.4 Mettre √† Jour Exports CSV/JSON

**Fichier**: `MyWebIntelligenceAPI/app/services/export_service.py`

```python
def export_expressions_csv(self, land_id: int, ...) -> str:
    """
    Export expressions to CSV.

    Includes sentiment fields in output.
    """
    # ... existing query ...

    # CSV headers
    headers = [
        'id', 'url', 'title', 'relevance', 'depth',
        'language', 'word_count',
        # ‚úÖ NOUVEAU
        'sentiment_score', 'sentiment_label',
        'sentiment_confidence', 'sentiment_status'
    ]

    # ... write CSV ...
```

### Tests √âtape 6

```python
# MyWebIntelligenceAPI/tests/api/test_expression_api_sentiment.py

import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
async def test_get_expression_includes_sentiment(
    client: AsyncClient,
    test_expression_with_sentiment
):
    """Verify GET /expressions/{id} returns sentiment fields."""

    response = await client.get(
        f"/api/v2/expressions/{test_expression_with_sentiment.id}"
    )

    assert response.status_code == 200
    data = response.json()

    # Check sentiment fields present
    assert "sentiment_score" in data
    assert "sentiment_label" in data
    assert "sentiment_confidence" in data
    assert "sentiment_status" in data

    # Verify values
    assert data["sentiment_score"] == pytest.approx(0.85, abs=0.01)
    assert data["sentiment_label"] == "positive"

@pytest.mark.asyncio
async def test_export_csv_includes_sentiment(client: AsyncClient, test_land):
    """Verify CSV export contains sentiment columns."""

    response = await client.post(
        "/api/v1/export/direct",
        json={
            "land_id": test_land.id,
            "export_type": "pagecsv"
        }
    )

    assert response.status_code == 200
    csv_content = response.text

    # Check CSV headers
    assert "sentiment_score" in csv_content
    assert "sentiment_label" in csv_content
    assert "sentiment_confidence" in csv_content

@pytest.mark.asyncio
async def test_expression_without_sentiment_returns_null(
    client: AsyncClient,
    test_expression_no_sentiment
):
    """Expressions without sentiment should return null values."""

    response = await client.get(
        f"/api/v2/expressions/{test_expression_no_sentiment.id}"
    )

    data = response.json()
    assert data["sentiment_score"] is None
    assert data["sentiment_label"] is None
    assert data["sentiment_status"] is None
```

### Livrables √âtape 6
- [ ] `expression.py` sch√©mas mis √† jour
- [ ] Endpoints API v√©rifi√©s (retournent sentiment)
- [ ] Export CSV/JSON inclut sentiment
- [ ] Documentation OpenAPI √† jour
- [ ] Tests API (3+ cas de test)

---

## üîÑ √âTAPE 7 ‚Äì Reprocessing Historique (Dur√©e: 3-4h)

### Objectif
Permettre le recalcul du sentiment sur expressions existantes (batch job).

### Actions

#### 7.1 Cr√©er Task Celery pour Batch

**Nouveau fichier**: `MyWebIntelligenceAPI/app/tasks/sentiment_task.py`

```python
"""
Celery tasks for sentiment analysis batch processing.
"""

import logging
from typing import Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select

from app.core.celery_app import celery_app
from app.db.models import Expression
from app.db.session import AsyncSessionLocal
from app.services.sentiment_service import SentimentService

logger = logging.getLogger(__name__)

@celery_app.task(bind=True, name="sentiment.batch_reprocess")
def batch_reprocess_sentiment(
    self,
    land_id: Optional[int] = None,
    limit: int = 100,
    force_recompute: bool = False
):
    """
    Batch reprocess sentiment for expressions.

    Args:
        land_id: If provided, process only this land
        limit: Max expressions to process per batch
        force_recompute: Recompute even if sentiment exists

    Returns:
        Dict with stats: processed, successful, errors
    """
    import asyncio

    async def _process():
        async with AsyncSessionLocal() as db:
            service = SentimentService()

            # Build query
            query = select(Expression)

            if land_id:
                query = query.filter(Expression.land_id == land_id)

            if not force_recompute:
                # Only expressions without sentiment
                query = query.filter(Expression.sentiment_score.is_(None))

            # Limit and order
            query = query.order_by(Expression.id).limit(limit)

            result = await db.execute(query)
            expressions = result.scalars().all()

            logger.info(
                f"Batch processing {len(expressions)} expressions for sentiment"
            )

            stats = {"processed": 0, "successful": 0, "errors": 0}

            for expr in expressions:
                try:
                    # Compute sentiment
                    sentiment_data = service.enrich_expression_sentiment(
                        content=expr.content,
                        readable=expr.readable,
                        language=expr.lang
                    )

                    # Update expression
                    expr.sentiment_score = sentiment_data["sentiment_score"]
                    expr.sentiment_label = sentiment_data["sentiment_label"]
                    expr.sentiment_confidence = sentiment_data["sentiment_confidence"]
                    expr.sentiment_status = sentiment_data["sentiment_status"]
                    expr.sentiment_model = sentiment_data["sentiment_model"]
                    expr.sentiment_computed_at = sentiment_data["sentiment_computed_at"]

                    stats["processed"] += 1
                    if sentiment_data["sentiment_status"] == "computed":
                        stats["successful"] += 1

                    # Update task progress
                    self.update_state(
                        state='PROGRESS',
                        meta={
                            'current': stats["processed"],
                            'total': len(expressions),
                            'status': f"Processing expression {expr.id}"
                        }
                    )

                except Exception as e:
                    logger.error(f"Error processing expression {expr.id}: {e}")
                    stats["errors"] += 1
                    continue

            # Commit all updates
            await db.commit()

            logger.info(f"Batch completed: {stats}")
            return stats

    # Run async code in Celery (sync context)
    return asyncio.run(_process())

@celery_app.task(name="sentiment.reprocess_expression")
def reprocess_single_expression(expression_id: int):
    """
    Reprocess sentiment for a single expression.

    Args:
        expression_id: Expression ID to process
    """
    import asyncio

    async def _process():
        async with AsyncSessionLocal() as db:
            service = SentimentService()

            # Get expression
            result = await db.execute(
                select(Expression).filter(Expression.id == expression_id)
            )
            expr = result.scalar_one_or_none()

            if not expr:
                raise ValueError(f"Expression {expression_id} not found")

            # Compute sentiment
            sentiment_data = service.enrich_expression_sentiment(
                content=expr.content,
                readable=expr.readable,
                language=expr.lang
            )

            # Update
            expr.sentiment_score = sentiment_data["sentiment_score"]
            expr.sentiment_label = sentiment_data["sentiment_label"]
            expr.sentiment_confidence = sentiment_data["sentiment_confidence"]
            expr.sentiment_status = sentiment_data["sentiment_status"]
            expr.sentiment_model = sentiment_data["sentiment_model"]
            expr.sentiment_computed_at = sentiment_data["sentiment_computed_at"]

            await db.commit()

            logger.info(
                f"Expression {expression_id} sentiment updated: "
                f"{sentiment_data['sentiment_label']}"
            )

            return sentiment_data

    return asyncio.run(_process())
```

#### 7.2 Cr√©er Endpoint API pour Lancement Batch

**Fichier**: `MyWebIntelligenceAPI/app/api/v2/endpoints/lands_v2.py`

```python
from app.tasks.sentiment_task import batch_reprocess_sentiment

@router.post("/{land_id}/reprocess-sentiment")
async def reprocess_land_sentiment(
    land_id: int,
    limit: int = 100,
    force_recompute: bool = False,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Reprocess sentiment for expressions in a land.

    Launches a Celery task to batch process expressions.

    Args:
        land_id: Land ID to process
        limit: Max expressions per batch (default 100)
        force_recompute: Recompute even if sentiment exists

    Returns:
        Task ID for monitoring progress
    """
    # Verify land exists and user has access
    land = await db.get(Land, land_id)
    if not land:
        raise HTTPException(status_code=404, detail="Land not found")

    if land.owner_id != current_user.id and not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Access denied")

    # Launch task
    task = batch_reprocess_sentiment.delay(
        land_id=land_id,
        limit=limit,
        force_recompute=force_recompute
    )

    logger.info(
        f"Sentiment reprocessing task launched for land {land_id}: {task.id}"
    )

    return {
        "task_id": task.id,
        "status": "processing",
        "land_id": land_id,
        "limit": limit,
        "force_recompute": force_recompute
    }
```

#### 7.3 Script CLI pour Administration

**Nouveau fichier**: `MyWebIntelligenceAPI/scripts/admin/reprocess_sentiment.py`

```bash
#!/usr/bin/env python3
"""
CLI tool to reprocess sentiment for expressions.

Usage:
    python scripts/admin/reprocess_sentiment.py --land-id 36 --limit 50
    python scripts/admin/reprocess_sentiment.py --all --force
"""

import argparse
import asyncio
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select

from app.db.session import AsyncSessionLocal
from app.db.models import Expression
from app.services.sentiment_service import SentimentService

async def reprocess_sentiment(
    land_id: int = None,
    limit: int = 100,
    force: bool = False
):
    """Reprocess sentiment for expressions."""

    async with AsyncSessionLocal() as db:
        service = SentimentService()

        # Build query
        query = select(Expression)

        if land_id:
            query = query.filter(Expression.land_id == land_id)

        if not force:
            query = query.filter(Expression.sentiment_score.is_(None))

        query = query.order_by(Expression.id).limit(limit)

        result = await db.execute(query)
        expressions = result.scalars().all()

        print(f"Processing {len(expressions)} expressions...")

        for i, expr in enumerate(expressions, 1):
            sentiment_data = service.enrich_expression_sentiment(
                content=expr.content,
                readable=expr.readable,
                language=expr.lang
            )

            expr.sentiment_score = sentiment_data["sentiment_score"]
            expr.sentiment_label = sentiment_data["sentiment_label"]
            expr.sentiment_confidence = sentiment_data["sentiment_confidence"]
            expr.sentiment_status = sentiment_data["sentiment_status"]

            if i % 10 == 0:
                print(f"  Processed {i}/{len(expressions)}...")

        await db.commit()
        print(f"‚úÖ Completed: {len(expressions)} expressions updated")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--land-id", type=int, help="Land ID to process")
    parser.add_argument("--limit", type=int, default=100, help="Max expressions")
    parser.add_argument("--force", action="store_true", help="Force recompute")

    args = parser.parse_args()

    asyncio.run(reprocess_sentiment(
        land_id=args.land_id,
        limit=args.limit,
        force=args.force
    ))
```

#### 7.4 Throttling & Gestion d'Erreurs

Dans `sentiment_task.py`, ajouter:

```python
@celery_app.task(
    bind=True,
    name="sentiment.batch_reprocess",
    max_retries=3,
    default_retry_delay=60  # 1 minute
)
def batch_reprocess_sentiment(self, ...):
    # ... code ...

    # Add delay between expressions to avoid overload
    import time

    for expr in expressions:
        try:
            # ... processing ...

            # Throttle: 100ms between expressions
            time.sleep(0.1)

        except Exception as e:
            logger.error(f"Error: {e}")
            stats["errors"] += 1

            # Retry task if too many errors
            if stats["errors"] > len(expressions) * 0.5:  # 50% error rate
                raise self.retry(exc=e)
```

### Tests √âtape 7

```python
# MyWebIntelligenceAPI/tests/integration/test_sentiment_batch.py

import pytest
from app.tasks.sentiment_task import batch_reprocess_sentiment

def test_batch_reprocess_dry_run(db_session, test_land):
    """Test batch reprocessing on small dataset."""

    # Create test expressions without sentiment
    # ... (setup)

    # Run batch (synchronously for test)
    result = batch_reprocess_sentiment(
        land_id=test_land.id,
        limit=5,
        force_recompute=False
    )

    assert result["processed"] == 5
    assert result["successful"] >= 4  # Allow 1 failure
    assert result["errors"] <= 1

def test_batch_reprocess_force(db_session, test_land):
    """Test force recompute on expressions with existing sentiment."""

    # ... (setup with existing sentiment)

    result = batch_reprocess_sentiment(
        land_id=test_land.id,
        limit=5,
        force_recompute=True
    )

    assert result["processed"] == 5
```

### Livrables √âtape 7
- [ ] Task Celery `sentiment_task.py` avec batch + single
- [ ] Endpoint API `/lands/{id}/reprocess-sentiment`
- [ ] Script CLI `reprocess_sentiment.py`
- [ ] Throttling + retry logic
- [ ] Tests batch (2+ cas)

---

## üìä √âTAPE 8 ‚Äì Monitoring & D√©ploiement (Dur√©e: 2-3h)

### Objectif
Instrumenter le syst√®me, d√©ployer progressivement, surveiller.

### Actions

#### 8.1 Ajouter M√©triques (Prometheus/StatsD)

**Fichier**: `MyWebIntelligenceAPI/app/services/sentiment_service.py`

```python
from app.utils.metrics import metrics  # Assuming prometheus_client

class SentimentService:
    # Metrics
    sentiment_counter = metrics.Counter(
        'sentiment_analysis_total',
        'Total sentiment analyses',
        ['status', 'language']
    )

    sentiment_duration = metrics.Histogram(
        'sentiment_analysis_duration_seconds',
        'Sentiment analysis duration'
    )

    sentiment_score_gauge = metrics.Gauge(
        'sentiment_score_distribution',
        'Distribution of sentiment scores',
        ['label']
    )

    def enrich_expression_sentiment(...) -> Dict[str, Any]:
        with self.sentiment_duration.time():
            # ... processing ...

            result = self.provider.analyze_sentiment(...)

            # Increment counter
            self.sentiment_counter.labels(
                status=result["status"],
                language=language
            ).inc()

            # Record score distribution
            if result["status"] == "computed":
                self.sentiment_score_gauge.labels(
                    label=result["label"]
                ).set(result["score"])

            return {...}
```

#### 8.2 Dashboard Monitoring

**Grafana Dashboard** (optionnel, requiert Prometheus):

```yaml
# dashboard.json (exemple simplifi√©)
{
  "panels": [
    {
      "title": "Sentiment Analysis Rate",
      "targets": [
        {
          "expr": "rate(sentiment_analysis_total[5m])"
        }
      ]
    },
    {
      "title": "Sentiment Score Distribution",
      "targets": [
        {
          "expr": "sentiment_score_distribution"
        }
      ]
    },
    {
      "title": "Analysis Duration (p95)",
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sentiment_analysis_duration_seconds)"
        }
      ]
    }
  ]
}
```

#### 8.3 Logging D√©taill√©

Assurer logs structur√©s pour debug:

```python
# Dans sentiment_service.py
logger.info(
    "Sentiment computed",
    extra={
        "expression_id": expr_id,
        "sentiment_label": result["label"],
        "sentiment_score": result["score"],
        "confidence": result["confidence"],
        "language": language,
        "duration_ms": duration * 1000
    }
)
```

#### 8.4 Guide Exploitation

**Nouveau fichier**: `MyWebIntelligenceAPI/docs/SENTIMENT_OPERATIONS.md`

```markdown
# Sentiment Analysis - Guide d'Exploitation

## Activation/D√©sactivation

### D√©sactiver temporairement
```bash
# Dans .env
ENABLE_SENTIMENT_ANALYSIS=false

# Red√©marrer services
docker compose restart api celery_worker
```

### R√©activer
```bash
ENABLE_SENTIMENT_ANALYSIS=true
docker compose restart api celery_worker
```

## Monitoring

### M√©triques cl√©s
- `sentiment_analysis_total`: Nombre d'analyses
- `sentiment_analysis_duration_seconds`: Latence
- Distribution `null` vs `computed`: Taux de succ√®s

### Logs √† surveiller
```bash
# Logs sentiment
docker logs mywebclient-api-1 | grep "Sentiment"

# Erreurs
docker logs mywebclient-api-1 | grep "sentiment.*ERROR"
```

## Troubleshooting

### Sympt√¥me: Beaucoup de `unsupported_lang`
**Cause**: Expressions dans langues non support√©es (DE, ES, etc.)
**Action**: Ajouter langues dans `SENTIMENT_SUPPORTED_LANGUAGES`

### Sympt√¥me: Latence > 2s
**Cause**: Mod√®le lourd ou CPU surcharg√©
**Actions**:
1. V√©rifier charge CPU: `docker stats`
2. Passer √† GPU si disponible
3. R√©duire `max_length` du mod√®le (512 ‚Üí 256 tokens)

### Sympt√¥me: `sentiment_score` toujours NULL
**Cause**: Feature flag d√©sactiv√© ou erreur mod√®le
**Actions**:
1. V√©rifier `ENABLE_SENTIMENT_ANALYSIS=true`
2. V√©rifier logs chargement mod√®le
3. Test manuel: `python -c "from app.services.sentiment_service import SentimentService; print(SentimentService().enrich_expression_sentiment('test'))"`

## Rollback

### Proc√©dure d'urgence
```bash
# 1. D√©sactiver sentiment
ENABLE_SENTIMENT_ANALYSIS=false

# 2. Red√©marrer
docker compose restart api celery_worker

# 3. V√©rifier crawl fonctionne sans sentiment
curl -X POST http://localhost:8000/api/v2/lands/TEST/crawl
```
```

#### 8.5 D√©ploiement Progressif

**Plan de d√©ploiement**:

1. **Phase 1: Staging (1 semaine)**
   ```bash
   # D√©ployer sur environnement staging
   ENABLE_SENTIMENT_ANALYSIS=true

   # Tester sur petit √©chantillon (10 lands)
   # Surveiller m√©triques, logs, latence
   ```

2. **Phase 2: Production Canary (1 semaine)**
   ```bash
   # Activer seulement pour 10% des lands
   # Modifier code pour sampling:

   if land_id % 10 == 0:  # 10% canary
       enable_sentiment = True
   ```

3. **Phase 3: Production Compl√®te**
   ```bash
   # Activer pour tous apr√®s validation
   ENABLE_SENTIMENT_ANALYSIS=true
   ```

### Tests √âtape 8

#### Smoke Test Post-D√©ploiement

```bash
# MyWebIntelligenceAPI/tests/smoke/test_sentiment_smoke.sh

#!/bin/bash
# Smoke test for sentiment feature after deployment

echo "üß™ Sentiment Feature Smoke Test"

# 1. Verify API is up
HTTP_CODE=$(curl -s -w "%{http_code}" "http://localhost:8000/health" -o /dev/null)
if [ "$HTTP_CODE" != "200" ]; then
    echo "‚ùå API not responding"
    exit 1
fi
echo "‚úÖ API responding"

# 2. Crawl test URL
# ... (create land, crawl)

# 3. Verify sentiment in DB
SENTIMENT_COUNT=$(docker exec mywebclient-db-1 psql -U mwi_user -d mwi_db -t -c \
  "SELECT COUNT(*) FROM expressions WHERE sentiment_score IS NOT NULL;")

echo "Expressions with sentiment: $SENTIMENT_COUNT"

if [ "$SENTIMENT_COUNT" -gt 0 ]; then
    echo "‚úÖ Sentiment feature working"
else
    echo "‚ö†Ô∏è No sentiment computed (check feature flag)"
fi

# 4. Check metrics endpoint
curl -s http://localhost:8000/metrics | grep sentiment_analysis_total

echo "‚úÖ Smoke test completed"
```

### Livrables √âtape 8
- [ ] M√©triques Prometheus instrument√©es
- [ ] Guide exploitation (`SENTIMENT_OPERATIONS.md`)
- [ ] Plan de d√©ploiement progressif
- [ ] Smoke test post-d√©ploiement
- [ ] Dashboard monitoring (optionnel)

---

## üìù Documentation Finale

### Fichiers √† Cr√©er/Mettre √† Jour

1. **`.claude/docs/sentiment_dev.md`** (ce fichier)
   - Plan d√©taill√© complet

2. **`.claude/docs/sentiment_validation_strategy.md`** ‚úÖ CR√â√â
   - 5 niveaux de validation (Provider ‚Üí M√©tier)
   - Corpus annot√© (60 expressions FR/EN)
   - Comparaison TextBlob vs LLM
   - Crit√®res de succ√®s et m√©triques

3. **`.claude/AGENTS.md`**
   - Ajouter section "Sentiment Analysis"
   - Checklist int√©gration double crawler

4. **`MyWebIntelligenceAPI/README.md`**
   - Ajouter feature sentiment dans liste
   - Variables d'environnement sentiment

5. **`MyWebIntelligenceAPI/docs/API.md`**
   - Documenter nouveaux champs API
   - Endpoint `/reprocess-sentiment`

6. **`MyWebIntelligenceAPI/docs/SENTIMENT_OPERATIONS.md`**
   - Guide op√©rationnel (cr√©√© √âtape 8)

---

## ‚úÖ Checklist Globale

### √âtape 1 - Cartographie ‚úÖ
- [x] V√©rifier mod√®le Expression
- [ ] Tracer flux crawlers
- [ ] Inventaire d√©pendances
- [ ] Tests audit complets

### √âtape 2 - Corpus & Exigences
- [ ] Constituer corpus 50-60 textes
- [ ] Annoter manuellement
- [ ] D√©finir seuils & format
- [ ] Documenter cas limites

### √âtape 3 - Mod√®le
- [ ] Choisir mod√®le (Option A recommand√©e)
- [ ] Impl√©menter `SentimentModelProvider`
- [ ] Pipeline normalisation texte
- [ ] Tests unitaires provider (10+ cas)

### √âtape 4 - Service
- [ ] Cr√©er `SentimentService`
- [ ] Orchestration langue + sentiment
- [ ] Seuils confiance
- [ ] Tests service (8+ cas)

### √âtape 5 - Int√©gration Crawlers ‚ö†Ô∏è CRITIQUE
- [ ] Modifier `crawler_engine.py` (async)
- [ ] Modifier `crawler_engine_sync.py` (sync)
- [ ] V√©rifier PARIT√â entre les deux
- [ ] Feature flag configuration
- [ ] Tests int√©gration (async + sync)

### √âtape 6 - API
- [ ] Sch√©mas Pydantic mis √† jour
- [ ] Endpoints exposent sentiment
- [ ] Exports CSV/JSON
- [ ] Tests API (3+ cas)

### √âtape 7 - Batch Reprocessing
- [ ] Task Celery batch
- [ ] Endpoint API reprocess
- [ ] Script CLI admin
- [ ] Tests batch (2+ cas)

### √âtape 8 - Monitoring & D√©ploiement
- [ ] M√©triques instrument√©es
- [ ] Guide exploitation
- [ ] Smoke tests
- [ ] D√©ploiement progressif

---

## üöÄ Prochaines Actions Imm√©diates

### Pour D√©marrer (aujourd'hui)
1. Valider plan avec √©quipe/product owner
2. Confirmer ressources hardware (CPU/RAM/GPU)
3. Installer d√©pendances de test:
   ```bash
   pip install transformers torch sentencepiece
   ```
4. Constituer corpus annot√© (√âtape 2)

### Sprint 1 (Semaine 1)
- √âtapes 1-3: Infrastructure mod√®le
- Tests unitaires provider

### Sprint 2 (Semaine 2)
- √âtapes 4-5: Service + int√©gration crawlers
- Tests int√©gration

### Sprint 3 (Semaine 3)
- √âtapes 6-7: API + batch
- Tests end-to-end

### Sprint 4 (Semaine 4)
- √âtape 8: Monitoring + d√©ploiement staging
- Validation m√©tier

---

## üìû Support & Questions

### Points de Blocage Potentiels

1. **Performance mod√®le trop lente**
   - Solution: GPU, mod√®le plus l√©ger (DistilBERT), ou API cloud

2. **M√©moire insuffisante**
   - Solution: Augmenter RAM container, ou mod√®le quantized

3. **Langues non support√©es majoritaires**
   - Solution: Ajouter mod√®les, ou accepter % null √©lev√©

4. **Double crawler oubli√©**
   - Solution: Checklist stricte, revue code obligatoire

### Contacts
- Architecture: Voir `AGENTS.md`, `Architecture.md`
- Op√©rations: Voir `GEMINI.md`
- Bugs crawlers: Voir `ERREUR_DOUBLE_CRAWLER.md`

---

**Derni√®re mise √† jour**: 18 octobre 2025
**Version**: 1.0
**Auteur**: Assistant AI (Claude)
**Statut**: Plan d√©taill√© pr√™t pour impl√©mentation
