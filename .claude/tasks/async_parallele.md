# Plan de D√©veloppement - Crawling Async Parall√®le

**Date**: 19 octobre 2025
**Statut**: ‚è∏Ô∏è EN PAUSE - Fonctionnel mais d√©sactiv√© temporairement
**Raison**: Erreurs `greenlet_spawn` dans l'extraction des liens (trop complexe √† r√©soudre)
**Objectif**: Impl√©menter le crawling HTTP parall√®le sans casser les op√©rations DB s√©quentielles

---

## ‚è∏Ô∏è D√âCISION: MISE EN PAUSE

**Date**: 19 octobre 2025

### Pourquoi la pause?

Le mode parall√®le **fonctionne** pour le crawl principal (4/5 URLs crawl√©es avec succ√®s, HTTP status sauv√©, metadata extraits, relevance calcul√©e), mais rencontre des **erreurs non-bloquantes** dans l'extraction des liens:

```
Error processing markdown link: greenlet_spawn has not been called;
can't call await_only() here. Was IO attempted in an unexpected place?
```

**Impact**:
- ‚úÖ Crawl principal: **FONCTIONNE**
- ‚úÖ M√©tadonn√©es: **SAUV√âES**
- ‚úÖ Performance: **3x plus rapide** (10s vs 30s)
- ‚ùå Liens entre expressions: **NON CR√â√âS** (erreur greenlet_spawn)
- ‚ùå Graphe de navigation: **INCOMPLET**

**D√©cision**: D√©sactiver temporairement le mode parall√®le (`parallel=False` par d√©faut) jusqu'√† r√©solution compl√®te du probl√®me d'extraction des liens.

### Ce qui a √©t√© corrig√©

Les 4 bugs critiques ont √©t√© r√©solus:
1. ‚úÖ `http_status` en INTEGER (au lieu de STRING)
2. ‚úÖ Headers HTTP en mode prefetched
3. ‚úÖ Logging des √©checs HTTP
4. ‚úÖ Tests avec le nouveau code

### Ce qui reste √† faire

- [ ] Refactoriser `_create_links_from_markdown()` pour √©viter erreurs greenlet_spawn
- [ ] Refactoriser `_extract_and_save_links()` pour √©viter erreurs greenlet_spawn
- [ ] Refactoriser `_save_media_from_list()` pour √©viter erreurs greenlet_spawn
- [ ] Tester extraction liens en mode parall√®le
- [ ] Valider graphe complet avec liens et media

**Complexit√© estim√©e**: 2-3 jours de d√©veloppement suppl√©mentaire

**Priorit√©**: BASSE (le crawl fonctionne sans parall√©lisme)

---

---

## üéØ Architecture Parall√®le

### Principe
L'impl√©mentation parall√®le divise le crawl en 2 phases:

**Phase 1 - HTTP PARALL√àLE** (I/O-bound, b√©n√©ficie de la concurrence)
```python
fetch_results = await asyncio.gather(
    *[fetch_with_semaphore(url) for url in urls],
    return_exceptions=True
)
```
- Toutes les requ√™tes HTTP sont lanc√©es en parall√®le
- Semaphore limite √† `max_concurrent` (default: 10)
- Pas d'acc√®s √† la DB ‚Üí Pas de conflit de session SQLAlchemy

**Phase 2 - TRAITEMENT S√âQUENTIEL** (DB-bound, √©vite les conflits)
```python
for fetch_result in fetch_results:
    status_code = await self.crawl_expression(expr, prefetched_content=fetch_result)
    await self.db.commit()  # Commit apr√®s CHAQUE expression
```
- Traitement un par un des r√©sultats HTTP
- Extraction de contenu, calcul de pertinence, sentiment
- Commit apr√®s chaque expression ‚Üí Session SQLAlchemy propre

### Avantages
- **Performance**: 5 URLs en ~10s au lieu de ~30s (3x plus rapide)
- **Fiabilit√©**: Pas d'erreurs `greenlet_spawn` ou `Session is already flushing`
- **Compatibilit√©**: R√©utilise `crawl_expression()` existant avec `prefetched_content`

### Limitations
- Commits fr√©quents (1 par expression) ‚Üí Overhead transactionnel
- Pas de rollback global si une expression √©choue
- N√©cessite plus de m√©moire (tous les r√©sultats HTTP en m√©moire)

---

## üêõ Bugs Identifi√©s

### BUG #1 CRITIQUE: `http_status` stock√© en STRING au lieu d'INTEGER

**Sympt√¥me**:
```
(sqlalchemy.dialects.postgresql.asyncpg.Error) <class 'asyncpg.exceptions.DataError'>:
invalid input for query argument $6: '200' ('str' object cannot be interpreted as an integer)
```

**Localisation**:
- `MyWebIntelligenceAPI/app/core/crawler_engine.py:322`
- `MyWebIntelligenceAPI/app/schemas/expression.py:33`

**Analyse**:
La DB PostgreSQL attend un INTEGER, mais le code convertit en STRING √† cause d'une incoh√©rence dans les sch√©mas Pydantic.

**Sch√©mas actuels**:
```python
# DB Model (models.py:207)
http_status = Column(Integer, nullable=True, index=True)  # ‚úÖ INTEGER

# Schema UPDATE (schemas/expression.py:33)
http_status: Optional[str] = None  # ‚ùå STRING (INCORRECT)

# Schema GET (schemas/expression.py:62)
http_status: Optional[int] = None  # ‚úÖ INTEGER

# Code (crawler_engine.py:322)
"http_status": str(http_status_code),  # ‚ùå Conversion en STRING (INCORRECT)
```

**Correction**:

1. **schemas/expression.py ligne 33**:
```python
# AVANT
http_status: Optional[str] = None  # Changed to string (legacy format)

# APR√àS
http_status: Optional[int] = None  # Must match DB type (INTEGER)
```

2. **crawler_engine.py ligne 322**:
```python
# AVANT
"http_status": str(http_status_code),  # Store as string (legacy format)

# APR√àS
"http_status": http_status_code,  # Store as int (DB expects INTEGER)
```

**Impact**: Sans ce fix, TOUTES les expressions √©chouent avec une erreur PostgreSQL.

---

### BUG #2 MAJEUR: Headers HTTP √©cras√©s en mode `prefetched`

**Sympt√¥me**:
Les champs `last_modified` et `etag` ne sont JAMAIS sauv√©s en mode parall√®le.

**Localisation**:
`MyWebIntelligenceAPI/app/core/crawler_engine.py:279-280, 312-319`

**Analyse**:
En mode `prefetched_content`, le code:
1. **Ligne 279-280**: Extrait correctement les headers du `prefetched_content` ‚úÖ
```python
last_modified_str = prefetched_content.get('last_modified')
etag_str = prefetched_content.get('etag')
```

2. **Ligne 312-313**: R√©initialise les variables √† `None` ‚ùå
```python
last_modified_str = None
etag_str = None
```

3. **Ligne 316-317**: Tente d'utiliser `response` qui n'existe PAS en mode prefetched ‚ùå
```python
last_modified_str = response.headers.get('last-modified', None)  # ‚ùå NameError!
etag_str = response.headers.get('etag', None)
```

**Code actuel probl√©matique**:
```python
# Ligne 272-280: Mode prefetched (‚úÖ extraction correcte)
if prefetched_content:
    html_content = prefetched_content['html_content']
    http_status_code = prefetched_content['status_code']
    content_type = prefetched_content.get('content_type')
    content_length = prefetched_content.get('content_length')
    last_modified_str = prefetched_content.get('last_modified')  # ‚úÖ
    etag_str = prefetched_content.get('etag')  # ‚úÖ
else:
    # Mode s√©quentiel...

# Ligne 312-319: PROBL√àME - √©crase les valeurs prefetched
last_modified_str = None  # ‚ùå √âcrase la valeur ligne 279!
etag_str = None  # ‚ùå √âcrase la valeur ligne 280!
if http_status_code and http_status_code < 400:
    try:
        last_modified_str = response.headers.get('last-modified', None)  # ‚ùå response n'existe pas!
        etag_str = response.headers.get('etag', None)
    except Exception:
        pass
```

**Correction**:
Conditionner le bloc √† `if not prefetched_content`:
```python
# Extract HTTP headers: Last-Modified and ETag
if not prefetched_content:  # ‚úÖ Seulement en mode s√©quentiel
    last_modified_str = None
    etag_str = None
    if http_status_code and http_status_code < 400:
        try:
            last_modified_str = response.headers.get('last-modified', None)
            etag_str = response.headers.get('etag', None)
        except Exception:
            pass
# Sinon, on garde les valeurs extraites du prefetched_content (lignes 279-280)
```

**Impact**: Sans ce fix, les headers HTTP ne sont jamais sauv√©s en mode parall√®le.

---

### BUG #3: Manque de logging pour les √©checs HTTP

**Sympt√¥me**:
Quand un fetch HTTP √©choue (4xx, 5xx, timeout), aucun log n'indique le probl√®me.

**Localisation**:
`MyWebIntelligenceAPI/app/core/crawler_engine.py:231-258`

**Analyse**:
Le code v√©rifie si `fetch_result` est une Exception, mais ne log pas si `fetch_result['success'] == False`.

**Code actuel**:
```python
for fetch_result in fetch_results:
    if isinstance(fetch_result, Exception):
        logger.error(f"HTTP fetch raised exception: {fetch_result}")
        error_count += 1
        http_stats['error'] += 1
        continue

    # ‚ùå Pas de v√©rification de fetch_result['success']
    url = fetch_result['url']
    expr = expr_map.get(url)
    if not expr:
        continue

    try:
        status_code = await self.crawl_expression(...)
```

**Correction**:
Ajouter un log apr√®s la v√©rification d'exception:
```python
for fetch_result in fetch_results:
    if isinstance(fetch_result, Exception):
        logger.error(f"HTTP fetch raised exception: {fetch_result}")
        error_count += 1
        http_stats['error'] += 1
        continue

    # ‚úÖ Log si le fetch a √©chou√© (mais pas lev√© d'exception)
    if not fetch_result.get('success', True):
        url = fetch_result.get('url', 'unknown')
        status_code = fetch_result.get('status_code', 0)
        error = fetch_result.get('error', 'Unknown error')
        logger.warning(
            f"HTTP fetch failed for {url}: "
            f"status={status_code}, error={error}"
        )

    url = fetch_result['url']
    # ...
```

**Note**: On continue quand m√™me le traitement car on veut enregistrer le `http_status` m√™me en cas d'erreur (4xx, 5xx).

**Impact**: Am√©liore le debugging, pas critique pour le fonctionnement.

---

### BUG #4: Tests ex√©cut√©s AVANT red√©marrage API

**Sympt√¥me**:
Les logs des tests montrent des erreurs d√©j√† corrig√©es:
- `'CRUDLand' object has no attribute 'get_land_dict'` (corrig√© ligne 397)
- `'SentimentService' object has no attribute 'analyze_sentiment'` (m√©thode dupliqu√©e supprim√©e)
- `greenlet_spawn has not been called` (corrig√© par l'architecture parall√®le)

**Analyse**:
Les tests ont √©t√© lanc√©s AVANT le red√©marrage de l'API, donc l'ancien code √©tait toujours en m√©moire.

**Correction**:
1. Red√©marrer l'API: `docker compose restart mywebintelligenceapi`
2. Attendre 5-10 secondes
3. Relancer les tests: `./MyWebIntelligenceAPI/tests/test-crawl-async.sh`

**Impact**: Les r√©sultats des tests ne refl√®tent pas le nouveau code.

---

## üìã Plan de Correction

### Phase 1: Corrections critiques

- [ ] **1.1** Corriger `schemas/expression.py` ligne 33
  - `http_status: Optional[str]` ‚Üí `http_status: Optional[int]`

- [ ] **1.2** Corriger `crawler_engine.py` ligne 322
  - `"http_status": str(http_status_code)` ‚Üí `"http_status": http_status_code`

- [ ] **1.3** Corriger `crawler_engine.py` lignes 312-319
  - Entourer avec `if not prefetched_content:`

### Phase 2: Am√©liorations

- [ ] **2.1** Ajouter logging dans `crawl_expressions_parallel` (ligne ~238)
  - Log si `fetch_result['success'] == False`

### Phase 3: Tests

- [ ] **3.1** Red√©marrer API
  - `docker compose restart mywebintelligenceapi`
  - Attendre 10 secondes

- [ ] **3.2** Lancer test-crawl-async.sh
  - V√©rifier: Pas d'erreur PostgreSQL
  - V√©rifier: Headers HTTP sauv√©s
  - V√©rifier: Metadata pr√©sents (title, lang, relevance)
  - V√©rifier: Content sauv√© (HTML, readable)

- [ ] **3.3** V√©rifier logs API
  - Chercher: "Starting PARALLEL HTTP fetch"
  - V√©rifier: "Parallel crawl completed: 5 processed, 0 errors"
  - Confirmer: Pas d'erreurs SQLAlchemy

---

## üîç Validation du Succ√®s

### Crit√®res de succ√®s
- ‚úÖ Aucune erreur PostgreSQL dans les logs
- ‚úÖ Test passe: 5/5 expressions crawl√©es
- ‚úÖ Headers HTTP: `last_modified` et `etag` pr√©sents (si serveur les envoie)
- ‚úÖ Metadata: `title`, `lang`, `description` pr√©sents
- ‚úÖ Relevance: > 0 pour au moins 3/5 expressions
- ‚úÖ Content: `content` (HTML) et `readable` non vides
- ‚úÖ Performance: Crawl < 15 secondes (vs ~30s en s√©quentiel)

### Requ√™te DB de validation
```sql
SELECT
    id,
    url,
    http_status,
    last_modified,
    etag,
    lang,
    relevance,
    LENGTH(content) as html_size,
    LENGTH(readable) as readable_size
FROM expressions
WHERE land_id = <LAND_ID>
ORDER BY id;
```

---

## üìä Comparaison S√©quentiel vs Parall√®le

| Aspect | S√©quentiel | Parall√®le |
|--------|-----------|-----------|
| **Dur√©e (5 URLs)** | ~30 secondes | ~10 secondes |
| **Concurrence HTTP** | 1 requ√™te √† la fois | 10 requ√™tes simultan√©es |
| **Commits DB** | 1 commit global | 1 commit par expression |
| **Risque session DB** | Faible | Nul (s√©quentiel) |
| **Utilisation m√©moire** | Faible | Moyenne |
| **Rollback** | Possible (global) | Impossible (commits fr√©quents) |
| **Code** | Simple | Mod√©r√© |

---

## üöÄ Prochaines √âtapes

Une fois les bugs corrig√©s:

1. **Performance tuning**
   - Mesurer temps r√©el de crawl
   - Ajuster `max_concurrent` (actuellement 10)
   - Profiler pour identifier autres goulots

2. **Robustesse**
   - Ajouter retry logic pour √©checs HTTP
   - Impl√©menter timeout par URL
   - G√©rer rate limiting

3. **Monitoring**
   - Ajouter m√©triques Prometheus
   - Logger temps par phase (HTTP vs DB)
   - Tracker taux de succ√®s/√©chec

4. **Documentation utilisateur**
   - Mettre √† jour API docs
   - Ajouter param√®tre `parallel=True/False` dans l'endpoint
   - Documenter diff√©rences de comportement

---

## üìö R√©f√©rences

- **Architecture**: Voir [align_sync_async.md](.claude/tasks/align_sync_async.md) pour alignement sync/async
- **Tests**: Voir [README_TEST_ASYNC.md](.claude/tasks/README_TEST_ASYNC.md) pour proc√©dure de test
- **Code source**: `MyWebIntelligenceAPI/app/core/crawler_engine.py` lignes 191-261 (parallel) et 263-532 (crawl_expression)
