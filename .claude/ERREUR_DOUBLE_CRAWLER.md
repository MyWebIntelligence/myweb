# üî¥ ERREUR FR√âQUENTE : Double Crawler Sync/Async

**Date**: 14 octobre 2025
**Gravit√©**: üî¥ **CRITIQUE** - Cause de bugs silencieux en production
**Fr√©quence**: ‚ö†Ô∏è **TR√àS FR√âQUENTE**

---

## ‚ùå Le Probl√®me

Le syst√®me MyWebIntelligence utilise **DEUX crawlers diff√©rents** pour des raisons techniques. Oublier de modifier l'un des deux cause des bugs qui passent inaper√ßus en d√©veloppement mais apparaissent en production.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                        ‚îÇ
‚îÇ  ‚ùå ERREUR TYPIQUE                                                     ‚îÇ
‚îÇ                                                                        ‚îÇ
‚îÇ  1. Vous modifiez crawler_engine.py (async)                          ‚îÇ
‚îÇ  2. Vous testez avec l'API directe ‚Üí ‚úÖ √áa marche !                  ‚îÇ
‚îÇ  3. Vous commitez                                                     ‚îÇ
‚îÇ  4. En production, Celery utilise crawler_engine_sync.py             ‚îÇ
‚îÇ  5. üí• Le bug n'appara√Æt qu'en production                            ‚îÇ
‚îÇ                                                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üèóÔ∏è Architecture : Pourquoi Deux Crawlers ?

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                      ‚îÇ
‚îÇ  üì± FastAPI (API endpoints)                                         ‚îÇ
‚îÇ      ‚îî‚îÄ AsyncCrawlerEngine (crawler_engine.py)                     ‚îÇ
‚îÇ          ‚îî‚îÄ Utilis√© pour : Tests, API directe                       ‚îÇ
‚îÇ          ‚îî‚îÄ Format : async/await natif                              ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  ‚öôÔ∏è  Celery Workers (tasks asynchrones)                            ‚îÇ
‚îÇ      ‚îî‚îÄ SyncCrawlerEngine (crawler_engine_sync.py)                 ‚îÇ
‚îÇ          ‚îî‚îÄ Utilis√© pour : Crawls en production                     ‚îÇ
‚îÇ          ‚îî‚îÄ Format : Synchrone avec asyncio.run()                   ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Pourquoi pas un seul crawler ?

1. **FastAPI** : Nativement async, pr√©f√®re `AsyncCrawlerEngine`
2. **Celery** : Workers synchrones, difficult√©s avec async/await
3. **Performances** : Les deux architectures sont optimis√©es diff√©remment

---

## üìã Checklist OBLIGATOIRE

**Avant CHAQUE modification de la logique de crawl :**

```bash
# ‚úÖ CHECKLIST
[ ] 1. Modifier crawler_engine.py (async)
[ ] 2. Modifier crawler_engine_sync.py (sync)
[ ] 3. V√©rifier que la logique est identique
[ ] 4. Tester avec l'API
[ ] 5. Tester avec Celery
[ ] 6. V√©rifier en base de donn√©es
```

---

## üêõ Exemples de Bugs R√©els

### Bug #1 : Champ `content` (HTML) non sauvegard√© (14 oct 2025)

**Sympt√¥mes** :
- Le champ `content` (HTML brut) est toujours NULL en DB
- Tests unitaires passent ‚úÖ
- Production √©choue ‚ùå

**Cause** :
```python
# ‚úÖ Ajout√© dans crawler_engine.py
if extraction_result.get('content'):
    update_data["content"] = extraction_result['content']

# ‚ùå OUBLI√â dans crawler_engine_sync.py
# ‚Üí Le HTML n'est jamais sauvegard√© en production
```

**Solution** :
```python
# Ajouter AUSSI dans crawler_engine_sync.py (lignes 192-197)
if extraction_result.get('content'):
    update_data["content"] = extraction_result['content']
    logger.debug(f"Storing HTML content: {len(extraction_result['content'])} chars")
```

---

### Bug #2 : M√©tadonn√©es manquantes

**Sympt√¥mes** :
- `title`, `description`, `keywords` extraits en dev
- Vides en production

**Cause** :
- Logique d'extraction ajout√©e dans `content_extractor.py`
- Crawler async l'utilise correctement
- Crawler sync ne r√©cup√®re pas les nouveaux champs

**Solution** :
- V√©rifier que les deux crawlers r√©cup√®rent les m√™mes champs depuis `extraction_result`

---

### Bug #3 : Nouveau calcul de relevance

**Sympt√¥mes** :
- Nouveau scoring fonctionne en tests
- Ancien scoring utilis√© en prod

**Cause** :
- Logique de relevance modifi√©e dans crawler async
- Crawler sync utilise toujours l'ancienne

---

## üîç Comment D√©tecter Ce Bug

### 1. Recherche dans le code

```bash
# Chercher votre modification dans LES DEUX fichiers
grep -n "votre_nouvelle_fonction" app/core/crawler_engine.py
grep -n "votre_nouvelle_fonction" app/core/crawler_engine_sync.py

# Si un seul r√©sultat ‚Üí üö® PROBL√àME !
```

### 2. V√©rifier les logs Celery

```bash
# Les logs Celery montrent quel crawler est utilis√©
docker logs mywebclient-celery_worker-1 --tail 100 | grep "Crawling"

# Chercher des erreurs silencieuses
docker logs mywebclient-celery_worker-1 --tail 100 | grep -E "(ERROR|WARNING)"
```

### 3. V√©rifier en base de donn√©es

```bash
# Apr√®s un crawl, v√©rifier que TOUTES les donn√©es sont sauvegard√©es
docker exec mywebclient-db-1 psql -U mwi_user -d mwi_db -c "
  SELECT
    url,
    title IS NOT NULL as has_title,
    description IS NOT NULL as has_desc,
    content IS NOT NULL as has_html,
    readable IS NOT NULL as has_readable
  FROM expressions
  WHERE created_at > NOW() - INTERVAL '5 minutes'
  LIMIT 3;
"
```

---

## üõ†Ô∏è Guide de Synchronisation

### Quand modifier les deux crawlers ?

**TOUJOURS** quand vous changez :

‚úÖ Extraction de donn√©es depuis les pages
‚úÖ Calcul de relevance
‚úÖ Sauvegarde de nouveaux champs en DB
‚úÖ Logique de filtrage des URLs
‚úÖ Traitement des erreurs HTTP
‚úÖ Gestion des m√©dias
‚úÖ Pipeline readable

‚ùå **PAS N√âCESSAIRE** pour :
- Routes API (endpoints FastAPI)
- Sch√©mas Pydantic
- Services externes (non li√©s au crawl)
- Configuration Docker

---

### Template de modification

```python
# ===== TOUJOURS SYNCHRONISER CES DEUX BLOCS =====

# üìÅ crawler_engine.py (ASYNC)
# Lignes ~165-170
if extraction_result.get('nouveau_champ'):
    update_data["nouveau_champ"] = extraction_result['nouveau_champ']
    logger.debug(f"Nouveau champ extrait")

# üìÅ crawler_engine_sync.py (SYNC)
# Lignes ~192-197
if extraction_result.get('nouveau_champ'):
    update_data["nouveau_champ"] = extraction_result['nouveau_champ']
    logger.debug(f"Nouveau champ extrait")
```

---

## üìä Diff√©rences entre les Deux Crawlers

| Aspect | AsyncCrawlerEngine | SyncCrawlerEngine |
|--------|-------------------|-------------------|
| **Fichier** | `crawler_engine.py` | `crawler_engine_sync.py` |
| **Utilis√© par** | FastAPI endpoints | Celery tasks |
| **Appels async** | `await func()` | `asyncio.run(func())` |
| **HTTP Client** | `httpx.AsyncClient` | `httpx.Client` |
| **DB Session** | `AsyncSession` | `Session` (sync) |
| **Logs** | Dans API logs | Dans Celery logs |

---

## üß™ Tests de Non-R√©gression

### Test apr√®s modification

```bash
# 1. Lancer un crawl via API (teste async)
curl -X POST "http://localhost:8000/api/v2/lands/1/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"max_pages": 1}'

# 2. Lancer un crawl via Celery (teste sync)
# (Le crawl normal utilise automatiquement Celery)

# 3. Comparer les r√©sultats
docker exec mywebclient-db-1 psql -U mwi_user -d mwi_db -c "
  SELECT
    id, url, title,
    LENGTH(content) as html_len,
    LENGTH(readable) as readable_len
  FROM expressions
  ORDER BY created_at DESC
  LIMIT 2;
"
```

### R√©sultat attendu

Les **deux derni√®res expressions** doivent avoir :
- ‚úÖ Title rempli
- ‚úÖ HTML (`content`) pr√©sent
- ‚úÖ Readable pr√©sent
- ‚úÖ M√™me structure de donn√©es

---

## üìö Ressources

- **Code source** :
  - `app/core/crawler_engine.py`
  - `app/core/crawler_engine_sync.py`

- **Tasks Celery** :
  - `app/tasks/crawling_task.py` (utilise SyncCrawlerEngine)

- **Documentation** :
  - `.claude/AGENTS.md` (section "Double Crawler")
  - `.claude/CORRECTIONS_EXTRACTION_METADATA.md`

---

## ‚úÖ R√©sum√© en 3 Points

1. **Deux crawlers existent** : async (API) et sync (Celery)
2. **Toujours modifier les deux** quand vous touchez √† la logique de crawl
3. **Tester avec Celery** avant de consid√©rer le bug r√©solu

---

**Derni√®re mise √† jour** : 14 octobre 2025
**Prochain audit** : √Ä chaque modification du pipeline de crawl
