# Strat√©gie de Validation - Sentiment Analysis

**Date**: 18 octobre 2025
**Projet**: MyWebIntelligence API - Sentiment Score
**Statut**: Plan de validation d√©taill√©

---

## üéØ Objectif

D√©finir une strat√©gie compl√®te de validation du sentiment analysis √† plusieurs niveaux : technique, qualit√©, et m√©tier.

---

## üìä Les 5 Niveaux de Validation

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   VALIDATION PYRAMID                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ  5. Validation M√©tier (Humain)                             ‚îÇ
‚îÇ     ‚îî‚îÄ Annotation manuelle corpus                           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  4. Validation A/B Testing                                  ‚îÇ
‚îÇ     ‚îî‚îÄ Comparaison TextBlob vs LLM                          ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  3. Validation End-to-End                                   ‚îÇ
‚îÇ     ‚îî‚îÄ Tests int√©gration crawlers                           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  2. Validation Service                                      ‚îÇ
‚îÇ     ‚îî‚îÄ Tests unitaires SentimentService                     ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  1. Validation Provider                                     ‚îÇ
‚îÇ     ‚îî‚îÄ Tests unitaires SentimentModelProvider               ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 1Ô∏è‚É£ Validation Provider (Technique)

### 1.1 Tests Unitaires TextBlob

**Fichier**: `tests/unit/test_sentiment_provider.py`

```python
def test_textblob_positive_sentiment_en():
    """Valider d√©tection sentiment positif EN."""
    provider = SentimentModelProvider()
    result = provider._analyze_textblob(
        "This is absolutely amazing! I love it.",
        language="en"
    )

    assert result["status"] == "computed"
    assert result["label"] == "positive"
    assert result["score"] > 0.3
    assert result["model"] == "textblob"

def test_textblob_negative_sentiment_fr():
    """Valider d√©tection sentiment n√©gatif FR."""
    result = provider._analyze_textblob(
        "C'est vraiment horrible et d√©cevant.",
        language="fr"
    )

    assert result["label"] == "negative"
    assert result["score"] < -0.3

def test_textblob_neutral_sentiment():
    """Valider d√©tection sentiment neutre."""
    result = provider._analyze_textblob(
        "The meeting is scheduled for 3pm.",
        language="en"
    )

    # Neutre = score proche de 0
    assert abs(result["score"]) < 0.2
```

### 1.2 Tests Unitaires OpenRouter (LLM)

```python
@pytest.mark.asyncio
async def test_llm_sentiment_high_quality():
    """Valider sentiment LLM haute qualit√©."""
    provider = SentimentModelProvider()

    # Skip si pas de cl√© API
    if not settings.OPENROUTER_API_KEY:
        pytest.skip("No OpenRouter API key")

    result = await provider._analyze_openrouter(
        "I'm extremely disappointed with this product.",
        language="en"
    )

    assert result["status"] == "computed"
    assert result["label"] == "negative"
    assert result["confidence"] > 0.7  # LLM = haute confiance
    assert "llm/" in result["model"]
```

### 1.3 Tests Cas Limites

```python
def test_empty_text():
    """Valider gestion texte vide."""
    result = provider._analyze_textblob("", "en")
    assert result["status"] == "failed"

def test_very_short_text():
    """Valider texte trop court."""
    result = provider._analyze_textblob("OK", "en")
    # Doit fonctionner mais avec confiance faible
    assert result["status"] == "computed"

def test_unsupported_language():
    """Valider langue non support√©e."""
    result = await provider.analyze_sentiment(
        "Êó•Êú¨Ë™û„ÅÆ„ÉÜ„Ç≠„Çπ„Éà",
        language="ja"
    )
    assert result["status"] == "unsupported_lang"

def test_mixed_language():
    """Valider texte multilingue."""
    result = provider._analyze_textblob(
        "This is great! C'est g√©nial!",
        language="en"  # Langue dominante
    )
    assert result["status"] == "computed"
```

**R√©sultat attendu** : 15+ tests unitaires, couverture >90%

---

## 2Ô∏è‚É£ Validation Service (Orchestration)

### 2.1 Tests Orchestration Langue + Sentiment

**Fichier**: `tests/unit/test_sentiment_service.py`

```python
def test_service_auto_detect_language():
    """Valider d√©tection automatique de langue."""
    service = SentimentService()

    result = service.enrich_expression_sentiment(
        readable="Ceci est un texte en fran√ßais tr√®s positif.",
        language=None  # Pas de langue fournie
    )

    # Doit d√©tecter FR et analyser
    assert result["sentiment_status"] == "computed"
    assert result["sentiment_label"] == "positive"

def test_service_prefers_readable_over_content():
    """Valider priorit√© readable vs content."""
    service = SentimentService()

    result = service.enrich_expression_sentiment(
        content="<html><body>Neutral HTML</body></html>",
        readable="This is a very positive article!",  # Priorit√©
        language="en"
    )

    # Doit analyser le readable, pas le HTML
    assert result["sentiment_label"] == "positive"

def test_service_fallback_to_content():
    """Valider fallback sur content si pas de readable."""
    result = service.enrich_expression_sentiment(
        content="This is great content!",
        readable=None,
        language="en"
    )

    assert result["sentiment_status"] == "computed"
    assert result["sentiment_label"] == "positive"
```

### 2.2 Tests Seuils de Confiance

```python
def test_low_confidence_flagging(monkeypatch):
    """Valider flag low_confidence si score faible."""
    service = SentimentService()

    # Mock provider pour retourner confiance < 0.5
    def mock_analyze(text, lang):
        return {
            "score": 0.1,
            "label": "neutral",
            "confidence": 0.4,  # < seuil 0.5
            "status": "computed",
            "model": "textblob"
        }

    monkeypatch.setattr(
        service.provider,
        "_analyze_textblob",
        mock_analyze
    )

    result = service.enrich_expression_sentiment(
        readable="Some text",
        language="en"
    )

    assert result["sentiment_status"] == "low_confidence"
    assert result["sentiment_score"] == 0.1  # Score gard√©
```

**R√©sultat attendu** : 10+ tests service, logique orchestration valid√©e

---

## 3Ô∏è‚É£ Validation End-to-End (Int√©gration)

### 3.1 Tests Crawlers avec Sentiment

**Fichier**: `tests/integration/test_crawler_sentiment.py`

```python
@pytest.mark.asyncio
async def test_async_crawler_adds_sentiment(db_session, test_land):
    """Valider crawler async enrichit sentiment."""
    crawler = AsyncCrawlerEngine(db_session=db_session)

    # Crawl URL avec contenu positif connu
    test_url = "https://httpbin.org/html"  # Contenu neutre

    await crawler.crawl_url(
        test_url,
        land_id=test_land.id,
        llm_validation=False  # TextBlob
    )

    # V√©rifier expression en DB
    expr = db_session.query(Expression)\
        .filter_by(url=test_url)\
        .first()

    assert expr is not None
    assert expr.sentiment_score is not None
    assert expr.sentiment_label in ["positive", "neutral", "negative"]
    assert expr.sentiment_model == "textblob"
    assert expr.sentiment_computed_at is not None

@pytest.mark.asyncio
async def test_async_crawler_llm_sentiment(db_session, test_land):
    """Valider crawler async avec LLM sentiment."""
    if not settings.OPENROUTER_API_KEY:
        pytest.skip("No API key")

    crawler = AsyncCrawlerEngine(db_session=db_session)

    await crawler.crawl_url(
        "https://example.com",
        land_id=test_land.id,
        llm_validation=True  # LLM
    )

    expr = db_session.query(Expression).first()
    assert "llm/" in expr.sentiment_model
```

### 3.2 Tests Double Crawler (Sync vs Async)

```python
def test_sync_async_parity(db_session, test_land):
    """Valider parit√© sentiment sync vs async."""
    test_url = "https://httpbin.org/html"

    # Crawl sync
    sync_crawler = SyncCrawlerEngine(db_session=db_session)
    sync_crawler.crawl_url(test_url, test_land.id, llm_validation=False)
    sync_expr = db_session.query(Expression)\
        .filter_by(url=test_url)\
        .first()

    # Nettoyage
    db_session.delete(sync_expr)
    db_session.commit()

    # Crawl async
    async_crawler = AsyncCrawlerEngine(db_session=db_session)
    asyncio.run(async_crawler.crawl_url(
        test_url, test_land.id, llm_validation=False
    ))
    async_expr = db_session.query(Expression)\
        .filter_by(url=test_url)\
        .first()

    # Comparer r√©sultats (doivent √™tre tr√®s proches)
    assert sync_expr.sentiment_label == async_expr.sentiment_label
    assert abs(sync_expr.sentiment_score - async_expr.sentiment_score) < 0.1
    assert sync_expr.sentiment_model == async_expr.sentiment_model
```

**R√©sultat attendu** : 5+ tests int√©gration, parit√© crawlers valid√©e

---

## 4Ô∏è‚É£ Validation A/B Testing (Comparaison M√©thodes)

### 4.1 Comparaison TextBlob vs LLM sur Corpus

**Script**: `tests/validation/compare_sentiment_methods.py`

```python
import json
from app.core.sentiment_provider import SentimentModelProvider

async def compare_methods_on_corpus():
    """Compare TextBlob vs LLM sur corpus annot√©."""

    # Charger corpus
    with open("tests/data/sentiment_corpus.json") as f:
        corpus = json.load(f)

    provider = SentimentModelProvider()
    results = {
        "textblob": {"correct": 0, "total": 0},
        "llm": {"correct": 0, "total": 0}
    }

    for item in corpus:
        text = item["text_snippet"]
        expected_label = item["manual_label"]

        # TextBlob
        tb_result = provider._analyze_textblob(text, "en")
        if tb_result["label"] == expected_label:
            results["textblob"]["correct"] += 1
        results["textblob"]["total"] += 1

        # LLM
        llm_result = await provider._analyze_openrouter(text, "en")
        if llm_result["label"] == expected_label:
            results["llm"]["correct"] += 1
        results["llm"]["total"] += 1

    # Calculer pr√©cision
    tb_accuracy = results["textblob"]["correct"] / results["textblob"]["total"]
    llm_accuracy = results["llm"]["correct"] / results["llm"]["total"]

    print(f"TextBlob Accuracy: {tb_accuracy:.2%}")
    print(f"LLM Accuracy: {llm_accuracy:.2%}")

    # Assertions
    assert tb_accuracy > 0.65  # TextBlob doit √™tre >65%
    assert llm_accuracy > 0.85  # LLM doit √™tre >85%
    assert llm_accuracy > tb_accuracy  # LLM meilleur que TextBlob
```

### 4.2 Analyse des D√©saccords

```python
def analyze_disagreements():
    """Analyser cas o√π TextBlob et LLM divergent."""

    disagreements = []

    for item in corpus:
        tb_result = provider._analyze_textblob(item["text_snippet"], "en")
        llm_result = await provider._analyze_openrouter(item["text_snippet"], "en")

        if tb_result["label"] != llm_result["label"]:
            disagreements.append({
                "text": item["text_snippet"][:100],
                "textblob": tb_result["label"],
                "llm": llm_result["label"],
                "manual": item["manual_label"]
            })

    # Afficher d√©saccords pour analyse humaine
    print(f"\n{len(disagreements)} d√©saccords trouv√©s:")
    for d in disagreements[:10]:  # Top 10
        print(f"\nText: {d['text']}")
        print(f"  TextBlob: {d['textblob']} | LLM: {d['llm']} | Human: {d['manual']}")
```

**R√©sultat attendu** :
- TextBlob pr√©cision : 65-75%
- LLM pr√©cision : 85-95%
- D√©saccords < 20% du corpus

---

## 5Ô∏è‚É£ Validation M√©tier (Annotation Humaine)

### 5.1 Constitution du Corpus Annot√©

**Processus** :

1. **S√©lectionner 60 expressions** (30 FR + 30 EN)
   ```sql
   -- √âchantillon stratifi√©
   SELECT id, url, title, readable, lang
   FROM expressions
   WHERE readable IS NOT NULL
     AND word_count BETWEEN 100 AND 500
     AND lang IN ('fr', 'en')
   ORDER BY RANDOM()
   LIMIT 60;
   ```

2. **Annoter manuellement**
   - 2 annotateurs ind√©pendants
   - √âtiquettes : positive / neutral / negative
   - Score cible : -1.0 √† +1.0
   - Niveau confiance : high / medium / low

3. **Format du corpus** (`sentiment_corpus.json`) :
   ```json
   [
     {
       "expression_id": 12345,
       "url": "https://example.com/article",
       "text_snippet": "First 200 chars of readable content...",
       "full_text_hash": "sha256...",
       "manual_label": "positive",
       "manual_score": 0.75,
       "annotator_1": "positive",
       "annotator_2": "positive",
       "agreement": true,
       "annotator_confidence": "high",
       "language": "fr",
       "word_count": 250,
       "notes": "Article √©logieux sur politique climat"
     }
   ]
   ```

### 5.2 M√©triques de Qualit√©

**Inter-annotator Agreement** :
```python
def calculate_agreement(corpus):
    """Calculer accord inter-annotateurs (Cohen's Kappa)."""
    from sklearn.metrics import cohen_kappa_score

    labels_1 = [item["annotator_1"] for item in corpus]
    labels_2 = [item["annotator_2"] for item in corpus]

    kappa = cohen_kappa_score(labels_1, labels_2)

    # Kappa > 0.6 = accord substantiel
    assert kappa > 0.6, f"Faible accord: {kappa:.2f}"

    return kappa
```

**Validation contre Corpus** :
```python
def validate_against_gold_standard():
    """Valider syst√®me contre gold standard."""
    from sklearn.metrics import classification_report, confusion_matrix

    # Pr√©dictions syst√®me
    y_true = [item["manual_label"] for item in corpus]
    y_pred_tb = [textblob_analyze(item["full_text"])["label"] for item in corpus]
    y_pred_llm = [await llm_analyze(item["full_text"])["label"] for item in corpus]

    # Rapports d√©taill√©s
    print("TextBlob Performance:")
    print(classification_report(y_true, y_pred_tb))

    print("\nLLM Performance:")
    print(classification_report(y_true, y_pred_llm))

    # Confusion matrix
    print("\nTextBlob Confusion Matrix:")
    print(confusion_matrix(y_true, y_pred_tb))
```

**M√©triques attendues** :
- **Precision** (positive) : >70% (TextBlob), >90% (LLM)
- **Recall** (positive) : >65% (TextBlob), >85% (LLM)
- **F1-score** : >68% (TextBlob), >87% (LLM)

---

## üìã Checklist Validation Compl√®te

### Phase 1 : Tests Automatis√©s ‚úÖ

- [ ] Tests unitaires `SentimentModelProvider` (15+ tests)
- [ ] Tests unitaires `SentimentService` (10+ tests)
- [ ] Tests int√©gration crawler async (3+ tests)
- [ ] Tests int√©gration crawler sync (3+ tests)
- [ ] Tests parit√© sync/async (1 test)
- [ ] Couverture code >85%

**Commande** :
```bash
pytest tests/unit/test_sentiment_*.py -v --cov=app.core.sentiment_provider --cov=app.services.sentiment_service
```

### Phase 2 : Corpus Annot√© üìù

- [ ] S√©lectionner 60 expressions (30 FR + 30 EN)
- [ ] Annotation annotateur 1
- [ ] Annotation annotateur 2
- [ ] Calcul inter-rater agreement (Kappa >0.6)
- [ ] R√©solution d√©saccords
- [ ] Export `sentiment_corpus.json`

### Phase 3 : Validation M√©tier üéØ

- [ ] A/B test TextBlob vs LLM sur corpus
- [ ] Analyse d√©saccords (manuel)
- [ ] Validation m√©triques (precision, recall, F1)
- [ ] Rapport comparatif TextBlob vs LLM
- [ ] D√©cision finale : TextBlob acceptable ? (si >65% pr√©cision)

### Phase 4 : Tests Production üöÄ

- [ ] Crawl test sur 10 lands √©chantillon
- [ ] V√©rification sentiment en DB (non-null)
- [ ] V√©rification distribution (30% pos, 50% neut, 20% neg attendu)
- [ ] V√©rification latence (<100ms TextBlob, <2s LLM)
- [ ] Smoke test post-d√©ploiement

---

## üéØ Crit√®res de Succ√®s

### Crit√®res Techniques

| Crit√®re | TextBlob | LLM |
|---------|----------|-----|
| **Latence moyenne** | <100ms | <2s |
| **Taux erreur** | <5% | <2% |
| **Taux null** | <10% | <5% |
| **Couverture tests** | >85% | >85% |

### Crit√®res Qualit√©

| M√©trique | TextBlob (Min) | LLM (Min) |
|----------|----------------|-----------|
| **Pr√©cision globale** | 65% | 85% |
| **Pr√©cision positive** | 70% | 90% |
| **Pr√©cision n√©gative** | 70% | 90% |
| **Recall positive** | 65% | 85% |
| **F1-score** | 68% | 87% |

### Crit√®res M√©tier

- **Coh√©rence** : Sentiment similaire pour contenus similaires
- **Stabilit√©** : M√™me texte ‚Üí m√™me sentiment (reproductibilit√©)
- **Explicabilit√©** : Pouvoir expliquer pourquoi un sentiment a √©t√© attribu√©
- **Utilit√©** : Les utilisateurs trouvent le sentiment pertinent

---

## üìä Rapports de Validation

### Template Rapport Final

```markdown
# Rapport de Validation Sentiment Analysis

## R√©sum√© Ex√©cutif
- Date validation : [DATE]
- Corpus : 60 expressions (30 FR, 30 EN)
- M√©thodes test√©es : TextBlob, OpenRouter LLM

## R√©sultats TextBlob
- Pr√©cision : XX%
- Recall : XX%
- F1-score : XX%
- Latence moyenne : XXms

## R√©sultats LLM
- Pr√©cision : XX%
- Recall : XX%
- F1-score : XX%
- Latence moyenne : XXms

## Comparaison
- Gain qualit√© LLM vs TextBlob : +XX%
- Surco√ªt latence : +XXms
- Co√ªt suppl√©mentaire : XX‚Ç¨/1000 analyses

## Recommandation
[TextBlob acceptable / LLM requis / Hybride optimal]

## Annexes
- Corpus annot√©
- Matrice de confusion
- Exemples d√©saccords
```

---

## üîÑ Processus de Validation Continue

### Monitoring Post-D√©ploiement

```python
# M√©triques √† tracker (Prometheus)
sentiment_distribution = Histogram(
    'sentiment_score_distribution',
    'Distribution des scores sentiment'
)

sentiment_null_rate = Gauge(
    'sentiment_null_rate',
    'Taux de sentiment NULL'
)

sentiment_method_usage = Counter(
    'sentiment_method_total',
    'Usage TextBlob vs LLM',
    ['method']  # textblob ou llm
)
```

### Validation Hebdomadaire

1. **√âchantillonner 10 expressions au hasard**
2. **Review manuelle** : sentiment correct ?
3. **Tracer d√©rive** : pr√©cision baisse ?
4. **Ajuster seuils** si n√©cessaire

---

**Derni√®re mise √† jour** : 18 octobre 2025
**Version** : 1.0
**Auteur** : Assistant AI (Claude)
