# Plan de DÃ©veloppement Quality Score - MyWebIntelligence API

**Date de crÃ©ation**: 18 octobre 2025
**Version**: 1.0
**Statut**: Plan dÃ©taillÃ© pour implÃ©mentation

---

## ğŸ¯ Objectif

ImplÃ©menter un systÃ¨me de scoring automatique de la qualitÃ© des expressions crawlÃ©es basÃ© sur des heuristiques et mÃ©tadonnÃ©es existantes (HTTP status, richesse textuelle, cohÃ©rence, pertinence), en respectant l'architecture du double crawler et les principes de paritÃ© legacy.

---

## âš¡ DÃ©cisions ClÃ©s (TL;DR)

### 1. Approche Technique : **Heuristique PondÃ©rÃ©e**
- **MÃ©thode** : Score composite basÃ© sur mÃ©tadonnÃ©es existantes
- **Pas de ML/API** : 100% dÃ©terministe et gratuit
- **Format** : Float 0.0 Ã  1.0 (ou 0-100 selon besoin mÃ©tier)

### 2. Installation
```bash
# AUCUNE dÃ©pendance supplÃ©mentaire !
# Utilise uniquement les donnÃ©es dÃ©jÃ  crawlÃ©es
```

### 3. Composants du Score (5 blocs)

| Bloc | Poids | CritÃ¨res |
|------|-------|----------|
| **AccÃ¨s** | 30% | HTTP 200, content-type HTML, pas de redirect |
| **Structure** | 15% | Title, description, keywords prÃ©sents |
| **Richesse** | 25% | Word count optimal (150-5000), ratio texte/HTML |
| **CohÃ©rence** | 20% | Reading time, langue vs land, relevance |
| **IntÃ©gritÃ©** | 10% | LLM validation, fraÃ®cheur, mÃ©tadonnÃ©es complÃ¨tes |

### 4. IntÃ©gration Simple
- **0 nouveau champ DB** : `quality_score` existe dÃ©jÃ  !
- **Nouveaux champs optionnels** : `quality_flags`, `quality_reason`
- **Double crawler** : MÃªme logique async + sync

### 5. Usage API
```bash
# Score calculÃ© automatiquement lors du crawl
POST /api/v2/lands/36/crawl

# Filtrer par qualitÃ©
GET /api/v2/lands/36/expressions?min_quality=0.7
```

### 6. Exemple de Scores

| Type d'Expression | Score | Raison |
|-------------------|-------|--------|
| Article complet 2000 mots | 0.95 | âœ… Tous critÃ¨res |
| Page courte 50 mots | 0.35 | âš ï¸ Pauvre en contenu |
| Erreur 404 | 0.0 | âŒ HTTP error |
| PDF (non-HTML) | 0.0 | âŒ Non parsable |
| Langue incorrecte | 0.45 | âš ï¸ Hors cible |

---

## ğŸ“Š Ã‰tat des Lieux

### âœ… Existant
- **Champ `quality_score`** : PrÃ©sent dans le modÃ¨le `Expression` (ligne 234, `models.py`)
  ```python
  quality_score = Column(Float, nullable=True)  # Score de qualitÃ© du contenu
  ```
- **Valeur actuelle** : Toujours `null` dans toutes les expressions existantes
- **Type** : `Float` (0.0 Ã  1.0 attendu)

### âœ… MÃ©tadonnÃ©es Disponibles (DÃ©jÃ  CrawlÃ©es)

Le quality_score peut Ãªtre calculÃ© avec les donnÃ©es **dÃ©jÃ  prÃ©sentes** :

```python
# Bloc AccÃ¨s
http_status: int              # 200, 404, 500, etc.
content_type: str             # "text/html", "application/pdf", etc.
crawled_at: datetime          # Timestamp du crawl

# Bloc Structure
title: str                    # Titre de la page
description: str              # Meta description
keywords: str                 # Meta keywords
canonical_url: str            # URL canonique

# Bloc Richesse
content: str                  # HTML complet
readable: str                 # Contenu lisible (markdown)
word_count: int               # Nombre de mots
content_length: int           # Taille HTML en bytes
reading_time: int             # Temps de lecture (minutes)

# Bloc CohÃ©rence
lang: str                     # Langue dÃ©tectÃ©e ("fr", "en")
relevance: float              # Score de pertinence vs mots-clÃ©s
depth: int                    # Profondeur de crawl
published_at: datetime        # Date de publication (si disponible)

# Bloc IntÃ©gritÃ©
valid_llm: str                # "oui"/"non" (validation LLM)
valid_model: str              # ModÃ¨le utilisÃ© pour validation
readable_at: datetime         # Timestamp extraction readable
approved_at: datetime         # Timestamp traitement complet
```

### âŒ Manquant
- Service de calcul `QualityScorer`
- IntÃ©gration dans crawlers (sync + async)
- **2 nouveaux champs optionnels** : `quality_flags`, `quality_reason`
- Tests unitaires et d'intÃ©gration
- Documentation mÃ©tier (grille de pondÃ©ration)

### ğŸ“‹ Nouveaux Champs Optionnels (TraÃ§abilitÃ©)

Ajouter Ã  la table `expressions` (optionnel mais recommandÃ©) :

```python
# Dans models.py (Ã  ajouter aprÃ¨s quality_score ligne 234)
quality_flags = Column(JSON, nullable=True)     # ["http_error", "short_content", "wrong_lang"]
quality_reason = Column(Text, nullable=True)    # Explication textuelle du score
quality_computed_at = Column(DateTime, nullable=True)  # Timestamp du calcul
```

**Avantage** : Permet de **dÃ©boguer** pourquoi un score est faible.

**Migration requise** : Alembic pour ajouter les 3 nouveaux champs (si dÃ©cision de les inclure).

---

## ğŸ—ï¸ Architecture Cible

### Composants Principaux

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    QUALITY PIPELINE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. QualityScorer (services/)                               â”‚
â”‚     â””â”€ Calculs heuristiques purs (dÃ©terministes)           â”‚
â”‚                                                              â”‚
â”‚  2. IntÃ©gration Crawlers                                    â”‚
â”‚     â”œâ”€ crawler_engine.py (async)                           â”‚
â”‚     â””â”€ crawler_engine_sync.py (sync) âš ï¸ DOUBLE CRAWLER     â”‚
â”‚                                                              â”‚
â”‚  3. API Endpoints                                           â”‚
â”‚     â”œâ”€ GET /expressions (avec quality_score filtrable)     â”‚
â”‚     â””â”€ POST /lands/{id}/reprocess-quality (batch)          â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Note clÃ©** : Pas de dÃ©pendances externes, tout est calculÃ© Ã  partir des mÃ©tadonnÃ©es existantes.

---

## ğŸ“‹ Plan de DÃ©veloppement DÃ©taillÃ©

---

## ğŸ” Ã‰TAPE 1 â€“ Audit de l'Existant (DurÃ©e: 1h)

### Objectif
Cartographier toutes les mÃ©tadonnÃ©es disponibles et identifier les points d'injection.

### Actions

#### 1.1 Cartographie Champs Disponibles

- [x] **VÃ©rifier champ `quality_score` dans `Expression`**
  - Fichier: `MyWebIntelligenceAPI/app/db/models.py:234`
  - Type: `Float`
  - Nullable: `True`

- [ ] **Inventorier mÃ©tadonnÃ©es exploitables**
  ```python
  # Script de vÃ©rification
  docker exec mywebclient-db-1 psql -U mwi_user -d mwi_db -c "
  SELECT
    COUNT(*) as total,
    COUNT(http_status) as has_http,
    COUNT(word_count) as has_words,
    COUNT(readable) as has_readable,
    COUNT(lang) as has_lang,
    COUNT(relevance) as has_relevance,
    AVG(word_count) as avg_words,
    AVG(relevance) as avg_relevance
  FROM expressions
  WHERE land_id = 36;  -- Land de test
  "
  ```

- [ ] **Analyser distribution des valeurs**
  ```sql
  -- HTTP status distribution
  SELECT http_status, COUNT(*)
  FROM expressions
  GROUP BY http_status
  ORDER BY COUNT(*) DESC;

  -- Word count ranges
  SELECT
    CASE
      WHEN word_count < 100 THEN 'very_short'
      WHEN word_count < 500 THEN 'short'
      WHEN word_count < 2000 THEN 'medium'
      ELSE 'long'
    END as length_category,
    COUNT(*)
  FROM expressions
  GROUP BY length_category;

  -- Language distribution
  SELECT lang, COUNT(*)
  FROM expressions
  GROUP BY lang;
  ```

#### 1.2 Identifier Points d'Injection

- [ ] **Localiser dans crawlers**
  - `crawler_engine.py` : AprÃ¨s extraction contenu + mÃ©tadonnÃ©es (ligne ~300)
  - `crawler_engine_sync.py` : MÃªme emplacement (ligne ~250)

- [ ] **VÃ©rifier schÃ©mas Pydantic**
  - Fichier: `MyWebIntelligenceAPI/app/schemas/expression.py`
  - S'assurer que `ExpressionOut` expose `quality_score`

- [ ] **VÃ©rifier exports**
  - `export_service.py` : Inclut-il dÃ©jÃ  `quality_score` dans CSV/JSON ?

#### 1.3 Ã‰tat Initial des Tests

- [ ] **ExÃ©cuter tests crawl existants**
  ```bash
  ./MyWebIntelligenceAPI/tests/test-crawl-simple.sh

  # VÃ©rifier aucune rÃ©gression
  # Confirmer quality_score = NULL actuellement
  ```

### Tests Ã‰tape 1

```bash
# 1. Rechercher "quality" dans codebase
rg "quality_score" MyWebIntelligenceAPI/ --type py

# 2. VÃ©rifier modÃ¨le Expression
grep -A 5 "quality_score" MyWebIntelligenceAPI/app/db/models.py

# 3. Analyse DB
docker exec mywebclient-db-1 psql -U mwi_user -d mwi_db -c \
  "SELECT id, url, quality_score, http_status, word_count, relevance
   FROM expressions
   LIMIT 10;"
```

### Livrables Ã‰tape 1
- [x] Liste des champs disponibles avec taux de remplissage
- [ ] Distribution statistique des valeurs clÃ©s (word_count, http_status, etc.)
- [ ] Identification des points d'injection dans crawlers (numÃ©ros de lignes)
- [ ] Rapport d'Ã©tat initial (aucune expression avec quality_score)

---

## ğŸ“ Ã‰TAPE 2 â€“ Cadrage MÃ©tier (DurÃ©e: 2h)

### Objectif
DÃ©finir les objectifs mÃ©tier, l'Ã©chelle du score, et les seuils de dÃ©cision.

### Actions

#### 2.1 DÃ©finir Objectifs MÃ©tier

**Cas d'usage** :

1. **Filtrage automatique**
   - Exclure pages de mauvaise qualitÃ© (`quality_score < 0.3`)
   - Ne garder que contenu riche pour analyse (`quality_score > 0.7`)

2. **Priorisation pour analystes**
   - Trier expressions par qualitÃ© dÃ©croissante
   - Identifier contenu premium pour annotation manuelle

3. **PondÃ©ration analyses**
   - Sentiment + Quality â†’ confiance composite
   - Relevance Ã— Quality â†’ score de pertinence ajustÃ©

4. **Monitoring santÃ© du crawl**
   - Taux de pages de qualitÃ© par land
   - DÃ©tecter problÃ¨mes de crawl (trop de 404, contenu vide)

#### 2.2 Valider Ã‰chelle et Seuils

**Ã‰chelle retenue** : `0.0` Ã  `1.0` (float)

**Seuils mÃ©tier** :

| Score | CatÃ©gorie | Description | Action |
|-------|-----------|-------------|--------|
| `0.0 - 0.2` | TrÃ¨s faible | Erreur HTTP, contenu vide, non-HTML | âŒ Exclure automatiquement |
| `0.2 - 0.4` | Faible | Page courte, pauvre en structure, hors cible | âš ï¸ Marquer pour review |
| `0.4 - 0.6` | Moyen | Contenu acceptable mais incomplet | âš ï¸ Utiliser avec prudence |
| `0.6 - 0.8` | Bon | Article standard avec mÃ©tadonnÃ©es correctes | âœ… QualitÃ© acceptable |
| `0.8 - 1.0` | Excellent | Contenu riche, complet, bien structurÃ© | âœ… QualitÃ© premium |

**RÃ¨gles de transparence** :

- **Flag explicatif** : Chaque pÃ©nalitÃ© doit Ãªtre tracÃ©e
  ```python
  quality_flags = [
      "http_error",           # HTTP != 200
      "short_content",        # word_count < 80
      "wrong_language",       # lang âˆ‰ land.lang
      "missing_structure",    # pas de title/description
      "poor_ratio"            # word_count / content_length < 0.1
  ]
  ```

- **Raison textuelle** : Explication en clair
  ```python
  quality_reason = "Score faible (0.25): HTTP 404, contenu vide"
  ```

#### 2.3 Ã‰tablir Truth Table (20 Cas ExtrÃªmes)

**Fichier** : `tests/data/quality_truth_table.json`

```json
[
  {
    "case_id": 1,
    "description": "Article complet FR 2000 mots",
    "http_status": 200,
    "word_count": 2000,
    "has_title": true,
    "has_description": true,
    "lang": "fr",
    "land_lang": ["fr"],
    "relevance": 3.5,
    "expected_score_min": 0.85,
    "expected_score_max": 1.0,
    "expected_category": "Excellent"
  },
  {
    "case_id": 2,
    "description": "Erreur 404",
    "http_status": 404,
    "word_count": 0,
    "has_title": false,
    "expected_score_min": 0.0,
    "expected_score_max": 0.0,
    "expected_flags": ["http_error", "no_content"],
    "expected_category": "TrÃ¨s faible"
  },
  {
    "case_id": 3,
    "description": "Page courte 50 mots",
    "http_status": 200,
    "word_count": 50,
    "has_title": true,
    "has_description": false,
    "lang": "en",
    "land_lang": ["en"],
    "relevance": 1.0,
    "expected_score_min": 0.25,
    "expected_score_max": 0.40,
    "expected_flags": ["short_content", "missing_structure"],
    "expected_category": "Faible"
  },
  {
    "case_id": 4,
    "description": "Langue incorrecte (DE au lieu de FR)",
    "http_status": 200,
    "word_count": 1500,
    "has_title": true,
    "lang": "de",
    "land_lang": ["fr"],
    "expected_score_min": 0.40,
    "expected_score_max": 0.55,
    "expected_flags": ["wrong_language"],
    "expected_category": "Moyen"
  },
  {
    "case_id": 5,
    "description": "PDF (non-HTML)",
    "http_status": 200,
    "content_type": "application/pdf",
    "expected_score_min": 0.0,
    "expected_score_max": 0.0,
    "expected_flags": ["non_html"],
    "expected_category": "TrÃ¨s faible"
  }
  // ... 15 autres cas
]
```

**Cas Ã  couvrir** :

1. âœ… Article complet optimal
2. âœ… Erreur HTTP (404, 500, 503)
3. âœ… Redirect (301, 302)
4. âœ… Page courte (<80 mots)
5. âœ… PDF ou non-HTML
6. âœ… Langue incorrecte
7. âœ… Pas de title
8. âœ… Ratio texte/HTML faible (<0.1)
9. âœ… Reading time incohÃ©rent (<15s ou >20min)
10. âœ… Contenu futur (published_at dans le futur)
11. âœ… Contenu trÃ¨s ancien (>5 ans)
12. âœ… Relevance = 0 (hors sujet)
13. âœ… Contenu validÃ© LLM (valid_llm = "oui")
14. âœ… Contenu sans readable (extraction Ã©chouÃ©e)
15. âœ… Page moyenne (word_count ~500)
16. âœ… Contenu trÃ¨s long (>10000 mots)
17. âœ… MÃ©tadonnÃ©es partielles (title mais pas description)
18. âœ… Land multilingue (FR+EN) avec contenu EN
19. âœ… Contenu avec relevance Ã©levÃ©e (>5.0)
20. âœ… Contenu crawlÃ© mais non approuvÃ© (approved_at = NULL)

### Tests Ã‰tape 2

```python
# tests/unit/test_quality_truth_table.py

def test_truth_table_consistency():
    """Valider que la truth table est cohÃ©rente."""
    with open("tests/data/quality_truth_table.json") as f:
        cases = json.load(f)

    for case in cases:
        # VÃ©rifier champs requis
        assert "case_id" in case
        assert "description" in case
        assert "expected_score_min" in case
        assert "expected_score_max" in case

        # CohÃ©rence min/max
        assert case["expected_score_min"] <= case["expected_score_max"]
        assert 0.0 <= case["expected_score_min"] <= 1.0
        assert 0.0 <= case["expected_score_max"] <= 1.0
```

### Livrables Ã‰tape 2
- [ ] Document objectifs mÃ©tier (filtrage, priorisation, pondÃ©ration)
- [ ] Ã‰chelle et seuils validÃ©s (0-1, 5 catÃ©gories)
- [ ] Truth table 20 cas (`quality_truth_table.json`)
- [ ] Validation PO/data analyst sur pondÃ©rations

---

## ğŸ§® Ã‰TAPE 3 â€“ Conception du Score (DurÃ©e: 3-4h)

### Objectif
DÃ©finir la formule heuristique complÃ¨te avec pondÃ©rations et transformations.

### Actions

#### 3.1 ModÃ¨le Heuristique PondÃ©rÃ© (5 Blocs)

```python
"""
Quality Score = Î£ (Bloc_i Ã— Poids_i)

Total poids = 1.0 (100%)
Score final clampÃ© entre 0.0 et 1.0
"""

# Configuration des poids
WEIGHTS = {
    "access": 0.30,      # 30% - AccessibilitÃ© de la page
    "structure": 0.15,   # 15% - Structure HTML/mÃ©tadonnÃ©es
    "richness": 0.25,    # 25% - Richesse du contenu
    "coherence": 0.20,   # 20% - CohÃ©rence et pertinence
    "integrity": 0.10    # 10% - IntÃ©gritÃ© pipeline
}
```

---

### 3.2 Bloc 1 : AccÃ¨s (30%)

**CritÃ¨res** : HTTP status, content-type, redirections

```python
def score_access(expression: Expression) -> tuple[float, list[str]]:
    """
    Score d'accessibilitÃ© (0.0 Ã  1.0).

    Returns:
        (score, flags)
    """
    score = 0.0
    flags = []

    # HTTP Status (critÃ¨re bloquant)
    if expression.http_status is None:
        flags.append("no_http_status")
        return 0.0, flags

    if 200 <= expression.http_status < 300:
        score += 1.0  # Full score si 2xx
    elif 300 <= expression.http_status < 400:
        score += 0.5  # MoitiÃ© si redirect
        flags.append("redirect")
    else:
        score = 0.0  # Zero si erreur
        flags.append("http_error")
        return score, flags  # Bloquant

    # Content-Type (critÃ¨re bloquant)
    if expression.content_type:
        if "text/html" in expression.content_type.lower():
            pass  # OK, pas de pÃ©nalitÃ©
        elif "application/pdf" in expression.content_type.lower():
            flags.append("non_html_pdf")
            return 0.0, flags  # Bloquant
        else:
            flags.append("non_html")
            score *= 0.3  # Grosse pÃ©nalitÃ© mais pas bloquant

    # Contenu crawlÃ© (vÃ©rifie que crawled_at existe)
    if expression.crawled_at is None:
        flags.append("not_crawled")
        return 0.0, flags

    return score, flags
```

**Exemples** :
- HTTP 200 + text/html â†’ `1.0` (âœ… parfait)
- HTTP 302 + text/html â†’ `0.5` (âš ï¸ redirect)
- HTTP 404 â†’ `0.0` (âŒ erreur)
- HTTP 200 + application/pdf â†’ `0.0` (âŒ non parsable)

---

### 3.3 Bloc 2 : Structure (15%)

**CritÃ¨res** : Title, description, keywords, canonical

```python
def score_structure(expression: Expression) -> tuple[float, list[str]]:
    """
    Score de structure HTML/mÃ©tadonnÃ©es (0.0 Ã  1.0).
    """
    score = 0.0
    flags = []
    max_points = 4  # 4 critÃ¨res

    # Title prÃ©sent et non vide
    if expression.title and len(expression.title.strip()) > 0:
        score += 0.4  # 40% du score structure
    else:
        flags.append("no_title")

    # Description prÃ©sente et suffisamment longue
    if expression.description and len(expression.description.strip()) > 20:
        score += 0.3  # 30%
    else:
        flags.append("no_description")

    # Keywords prÃ©sents
    if expression.keywords and len(expression.keywords.strip()) > 0:
        score += 0.15  # 15%
    else:
        flags.append("no_keywords")

    # Canonical URL (bonne pratique SEO)
    if expression.canonical_url:
        score += 0.15  # 15%
    else:
        flags.append("no_canonical")

    return score, flags
```

**Exemples** :
- Tous prÃ©sents â†’ `1.0` (âœ… structure complÃ¨te)
- Title seulement â†’ `0.4` (âš ï¸ structure partielle)
- Aucun â†’ `0.0` (âŒ structure absente)

---

### 3.4 Bloc 3 : Richesse (25%)

**CritÃ¨res** : Word count optimal, ratio texte/HTML, reading time

```python
def score_richness(expression: Expression) -> tuple[float, list[str]]:
    """
    Score de richesse textuelle (0.0 Ã  1.0).

    Courbe gaussienne centrÃ©e sur 1500 mots.
    """
    score = 0.0
    flags = []

    # Word count (50% du score richesse)
    if expression.word_count is None or expression.word_count == 0:
        flags.append("no_content")
        return 0.0, flags

    wc = expression.word_count

    if wc < 80:
        # Trop court, quasi-null
        score_wc = 0.1
        flags.append("very_short_content")
    elif wc < 150:
        # Court mais acceptable
        score_wc = 0.3
        flags.append("short_content")
    elif 150 <= wc <= 5000:
        # Zone optimale : courbe gaussienne centrÃ©e sur 1500
        optimal = 1500
        sigma = 1500  # Ã‰cart-type large pour tolÃ©rance
        score_wc = math.exp(-((wc - optimal) ** 2) / (2 * sigma ** 2))
    else:
        # TrÃ¨s long : dÃ©croissance douce
        score_wc = 0.8 - (wc - 5000) / 50000  # DÃ©croit doucement
        score_wc = max(0.5, score_wc)  # Plancher Ã  0.5
        if wc > 10000:
            flags.append("very_long_content")

    score += score_wc * 0.5

    # Ratio word_count / content_length (30% du score richesse)
    if expression.content_length and expression.content_length > 0:
        ratio = expression.word_count / expression.content_length

        if ratio < 0.05:
            # HTML trÃ¨s lourd, peu de texte (boilerplate, scripts)
            score_ratio = 0.2
            flags.append("poor_text_ratio")
        elif ratio < 0.1:
            score_ratio = 0.5
            flags.append("low_text_ratio")
        elif 0.1 <= ratio <= 0.3:
            # Zone optimale
            score_ratio = 1.0
        else:
            # Trop de texte vs HTML (inhabituel mais OK)
            score_ratio = 0.9

        score += score_ratio * 0.3
    else:
        # Pas de content_length â†’ neutre
        score += 0.3 * 0.5  # Score moyen

    # Reading time cohÃ©rent (20% du score richesse)
    if expression.reading_time:
        rt = expression.reading_time  # En minutes

        if rt < 0.25:  # <15 secondes
            score_rt = 0.2
            flags.append("very_short_reading")
        elif rt < 0.5:  # 15-30 secondes
            score_rt = 0.5
            flags.append("short_reading")
        elif 0.5 <= rt <= 15:  # 30s Ã  15min (zone normale)
            score_rt = 1.0
        elif 15 < rt <= 25:  # 15-25min (long mais OK)
            score_rt = 0.8
        else:  # >25min (suspicieux)
            score_rt = 0.3
            flags.append("very_long_reading")

        score += score_rt * 0.2
    else:
        # Pas de reading_time â†’ neutre
        score += 0.2 * 0.5

    return score, flags
```

**Exemples** :
- 1500 mots, ratio 0.15, reading 5min â†’ `1.0` (âœ… optimal)
- 50 mots, ratio 0.02, reading 0.1min â†’ `0.15` (âŒ trÃ¨s pauvre)
- 8000 mots, ratio 0.25, reading 30min â†’ `0.75` (âš ï¸ long mais acceptable)

---

### 3.5 Bloc 4 : CohÃ©rence (20%)

**CritÃ¨res** : Langue vs land, relevance, fraÃ®cheur

```python
def score_coherence(
    expression: Expression,
    land: Land
) -> tuple[float, list[str]]:
    """
    Score de cohÃ©rence avec le land et logique mÃ©tier (0.0 Ã  1.0).
    """
    score = 0.0
    flags = []

    # Langue alignÃ©e avec land (40% du score cohÃ©rence)
    if expression.lang and land.lang:
        land_languages = land.lang if isinstance(land.lang, list) else [land.lang]

        if expression.lang in land_languages:
            score_lang = 1.0
        else:
            score_lang = 0.0
            flags.append("wrong_language")

        score += score_lang * 0.4
    else:
        # Pas de langue dÃ©tectÃ©e â†’ neutre
        score += 0.4 * 0.5
        if not expression.lang:
            flags.append("no_language")

    # Relevance (pertinence mot-clÃ©s) (40% du score cohÃ©rence)
    if expression.relevance is not None:
        # Normaliser relevance (supposÃ© 0-10 ou plus)
        # Mapper vers 0-1 avec saturation Ã  5.0
        norm_relevance = min(expression.relevance / 5.0, 1.0)
        score += norm_relevance * 0.4

        if expression.relevance < 0.5:
            flags.append("low_relevance")
    else:
        # Pas de relevance â†’ neutre
        score += 0.4 * 0.5

    # FraÃ®cheur contenu (20% du score cohÃ©rence)
    if expression.published_at:
        from datetime import datetime, timezone
        now = datetime.now(timezone.utc)
        age_days = (now - expression.published_at).days

        if age_days < 0:
            # PubliÃ© dans le futur (erreur)
            score_fresh = 0.0
            flags.append("future_date")
        elif age_days < 365:  # <1 an
            score_fresh = 1.0
        elif age_days < 730:  # 1-2 ans
            score_fresh = 0.9
        elif age_days < 1825:  # 2-5 ans
            score_fresh = 0.7
        else:  # >5 ans
            score_fresh = 0.5
            flags.append("old_content")

        score += score_fresh * 0.2
    else:
        # Pas de date de publication â†’ neutre
        score += 0.2 * 0.5

    return score, flags
```

**Exemples** :
- Langue OK, relevance 4.0, publiÃ© il y a 6 mois â†’ `0.95` (âœ… trÃ¨s cohÃ©rent)
- Langue incorrecte, relevance 0.2, publiÃ© il y a 8 ans â†’ `0.25` (âŒ incohÃ©rent)

---

### 3.6 Bloc 5 : IntÃ©gritÃ© (10%)

**CritÃ¨res** : Validation LLM, extraction readable, pipeline complet

```python
def score_integrity(expression: Expression) -> tuple[float, list[str]]:
    """
    Score d'intÃ©gritÃ© du pipeline (0.0 Ã  1.0).
    """
    score = 0.0
    flags = []

    # Validation LLM rÃ©ussie (40% du score intÃ©gritÃ©)
    if expression.valid_llm == "oui":
        score += 0.4
    elif expression.valid_llm == "non":
        score += 0.0
        flags.append("llm_rejected")
    else:
        # Pas de validation LLM â†’ neutre
        score += 0.4 * 0.5

    # Extraction readable rÃ©ussie (40% du score intÃ©gritÃ©)
    if expression.readable_at and expression.readable:
        if len(expression.readable.strip()) > 100:
            score += 0.4
        else:
            score += 0.2
            flags.append("short_readable")
    else:
        score += 0.0
        flags.append("no_readable")

    # Pipeline complet (approved_at prÃ©sent) (20%)
    if expression.approved_at:
        score += 0.2
    else:
        score += 0.0
        flags.append("not_approved")

    return score, flags
```

**Exemples** :
- LLM validÃ©, readable OK, approved â†’ `1.0` (âœ… pipeline complet)
- LLM rejetÃ©, pas de readable â†’ `0.1` (âŒ pipeline incomplet)

---

### 3.7 Score Final (AgrÃ©gation)

```python
from typing import TypedDict

class QualityResult(TypedDict):
    """RÃ©sultat du calcul de qualitÃ©."""
    score: float                    # 0.0 Ã  1.0
    category: str                   # "Excellent", "Bon", "Moyen", etc.
    flags: list[str]                # ["short_content", "wrong_language"]
    reason: str                     # Explication textuelle
    details: dict[str, float]       # Scores par bloc pour debug

def compute_quality_score(
    expression: Expression,
    land: Land
) -> QualityResult:
    """
    Calcule le quality_score complet.

    Returns:
        QualityResult avec score, catÃ©gorie, flags, raison
    """
    all_flags = []
    details = {}

    # Bloc 1: AccÃ¨s (30%)
    access_score, access_flags = score_access(expression)
    all_flags.extend(access_flags)
    details["access"] = access_score

    # Si accÃ¨s Ã©choue (HTTP erreur), score = 0
    if access_score == 0.0:
        return {
            "score": 0.0,
            "category": "TrÃ¨s faible",
            "flags": all_flags,
            "reason": f"AccÃ¨s impossible: {', '.join(all_flags)}",
            "details": details
        }

    # Bloc 2: Structure (15%)
    struct_score, struct_flags = score_structure(expression)
    all_flags.extend(struct_flags)
    details["structure"] = struct_score

    # Bloc 3: Richesse (25%)
    rich_score, rich_flags = score_richness(expression)
    all_flags.extend(rich_flags)
    details["richness"] = rich_score

    # Bloc 4: CohÃ©rence (20%)
    coher_score, coher_flags = score_coherence(expression, land)
    all_flags.extend(coher_flags)
    details["coherence"] = coher_score

    # Bloc 5: IntÃ©gritÃ© (10%)
    integ_score, integ_flags = score_integrity(expression)
    all_flags.extend(integ_flags)
    details["integrity"] = integ_score

    # AgrÃ©gation pondÃ©rÃ©e
    final_score = (
        access_score * WEIGHTS["access"] +
        struct_score * WEIGHTS["structure"] +
        rich_score * WEIGHTS["richness"] +
        coher_score * WEIGHTS["coherence"] +
        integ_score * WEIGHTS["integrity"]
    )

    # Clamp 0-1 (sÃ©curitÃ©)
    final_score = max(0.0, min(1.0, final_score))

    # DÃ©terminer catÃ©gorie
    if final_score >= 0.8:
        category = "Excellent"
    elif final_score >= 0.6:
        category = "Bon"
    elif final_score >= 0.4:
        category = "Moyen"
    elif final_score >= 0.2:
        category = "Faible"
    else:
        category = "TrÃ¨s faible"

    # GÃ©nÃ©rer raison textuelle
    if final_score >= 0.8:
        reason = f"Haute qualitÃ© ({final_score:.2f}): contenu riche et complet"
    elif final_score >= 0.6:
        reason = f"QualitÃ© acceptable ({final_score:.2f}): contenu standard"
    else:
        # Identifier principale pÃ©nalitÃ©
        main_issues = []
        if "http_error" in all_flags:
            main_issues.append("erreur HTTP")
        if "short_content" in all_flags or "very_short_content" in all_flags:
            main_issues.append("contenu trop court")
        if "wrong_language" in all_flags:
            main_issues.append("langue incorrecte")
        if "low_relevance" in all_flags:
            main_issues.append("faible pertinence")

        reason = f"QualitÃ© {category.lower()} ({final_score:.2f}): {', '.join(main_issues or all_flags[:2])}"

    return {
        "score": round(final_score, 3),
        "category": category,
        "flags": all_flags,
        "reason": reason,
        "details": details
    }
```

---

### 3.8 Tests de Conception

**Notebook de prototypage** : `tests/notebooks/quality_prototype.ipynb`

```python
# Test sur truth table
import json
from app.services.quality import QualityScorer

with open("tests/data/quality_truth_table.json") as f:
    cases = json.load(f)

scorer = QualityScorer()

for case in cases:
    # Mock expression object
    expr_mock = MockExpression(**case)
    land_mock = MockLand(lang=case.get("land_lang", ["fr"]))

    result = scorer.compute_quality_score(expr_mock, land_mock)

    # Valider score dans fourchette attendue
    assert case["expected_score_min"] <= result["score"] <= case["expected_score_max"], \
        f"Case {case['case_id']}: score {result['score']} hors limites"

    print(f"âœ… Case {case['case_id']}: {result['score']:.2f} ({result['category']})")
```

### Livrables Ã‰tape 3
- [ ] Formule complÃ¨te des 5 blocs (code Python)
- [ ] Configuration pondÃ©rations (`WEIGHTS` dict)
- [ ] Structure `QualityResult` (TypedDict)
- [ ] Notebook de prototypage validÃ© sur truth table
- [ ] Documentation formule dans docstrings

---

## ğŸ”§ Ã‰TAPE 4 â€“ ImplÃ©mentation du Service (DurÃ©e: 3h)

### Objectif
CrÃ©er le service `QualityScorer` pur et testable.

### Actions

#### 4.1 CrÃ©er Service `QualityScorer`

**Nouveau fichier** : `MyWebIntelligenceAPI/app/services/quality_scorer.py`

```python
"""
Quality Scoring Service for MyWebIntelligence API

Computes quality_score based on heuristics and existing metadata.
Pure function service (no external dependencies, deterministic).
"""

import logging
import math
from datetime import datetime, timezone
from typing import TypedDict

from app.db.models import Expression, Land

logger = logging.getLogger(__name__)

# Configuration des poids (modifiable via settings)
WEIGHTS = {
    "access": 0.30,
    "structure": 0.15,
    "richness": 0.25,
    "coherence": 0.20,
    "integrity": 0.10
}

class QualityResult(TypedDict):
    """RÃ©sultat du calcul de qualitÃ©."""
    score: float
    category: str
    flags: list[str]
    reason: str
    details: dict[str, float]

class QualityScorer:
    """
    Service de calcul du quality_score.

    100% dÃ©terministe, basÃ© sur mÃ©tadonnÃ©es existantes.
    Pas de dÃ©pendances externes.
    """

    def __init__(self, custom_weights: dict = None):
        """
        Initialize scorer avec pondÃ©rations optionnelles.

        Args:
            custom_weights: Remplace WEIGHTS par dÃ©faut (pour tests/tuning)
        """
        self.weights = custom_weights if custom_weights else WEIGHTS
        logger.info(f"QualityScorer initialized with weights: {self.weights}")

    def compute_quality_score(
        self,
        expression: Expression,
        land: Land
    ) -> QualityResult:
        """
        Calcule quality_score complet pour une expression.

        Args:
            expression: Expression ORM object
            land: Land parent ORM object

        Returns:
            QualityResult avec score 0-1, catÃ©gorie, flags, raison
        """
        # ... (code complet des sections 3.2 Ã  3.7)
        pass  # Voir sections prÃ©cÃ©dentes

    # Fonctions internes des 5 blocs
    def _score_access(self, expression: Expression) -> tuple[float, list[str]]:
        """Score bloc AccÃ¨s (30%)."""
        # Code section 3.2
        pass

    def _score_structure(self, expression: Expression) -> tuple[float, list[str]]:
        """Score bloc Structure (15%)."""
        # Code section 3.3
        pass

    def _score_richness(self, expression: Expression) -> tuple[float, list[str]]:
        """Score bloc Richesse (25%)."""
        # Code section 3.4
        pass

    def _score_coherence(
        self,
        expression: Expression,
        land: Land
    ) -> tuple[float, list[str]]:
        """Score bloc CohÃ©rence (20%)."""
        # Code section 3.5
        pass

    def _score_integrity(self, expression: Expression) -> tuple[float, list[str]]:
        """Score bloc IntÃ©gritÃ© (10%)."""
        # Code section 3.6
        pass
```

#### 4.2 Rendre Configurable via Settings

**Fichier** : `MyWebIntelligenceAPI/app/config.py`

```python
class Settings(BaseSettings):
    # ... existing settings ...

    # Quality Scoring
    ENABLE_QUALITY_SCORING: bool = True
    QUALITY_WEIGHT_ACCESS: float = 0.30
    QUALITY_WEIGHT_STRUCTURE: float = 0.15
    QUALITY_WEIGHT_RICHNESS: float = 0.25
    QUALITY_WEIGHT_COHERENCE: float = 0.20
    QUALITY_WEIGHT_INTEGRITY: float = 0.10
```

**Usage dans scorer** :
```python
from app.config import settings

# Dans __init__
self.weights = {
    "access": settings.QUALITY_WEIGHT_ACCESS,
    "structure": settings.QUALITY_WEIGHT_STRUCTURE,
    "richness": settings.QUALITY_WEIGHT_RICHNESS,
    "coherence": settings.QUALITY_WEIGHT_COHERENCE,
    "integrity": settings.QUALITY_WEIGHT_INTEGRITY
}
```

### Tests Ã‰tape 4

**Fichier** : `tests/unit/test_quality_scorer.py`

```python
import pytest
from app.services.quality_scorer import QualityScorer, QualityResult
from app.db.models import Expression, Land

@pytest.fixture
def scorer():
    return QualityScorer()

@pytest.fixture
def perfect_expression(db_session):
    """Expression parfaite (score ~1.0)."""
    return Expression(
        http_status=200,
        content_type="text/html",
        title="Great Article Title",
        description="Comprehensive description with sufficient length",
        keywords="machine learning, AI, deep learning",
        canonical_url="https://example.com/article",
        word_count=1500,
        content_length=50000,
        reading_time=6,  # 6 minutes
        lang="fr",
        relevance=4.5,
        published_at=datetime.now(timezone.utc) - timedelta(days=30),
        valid_llm="oui",
        readable="Long readable content...",
        readable_at=datetime.now(timezone.utc),
        approved_at=datetime.now(timezone.utc)
    )

@pytest.fixture
def test_land():
    return Land(lang=["fr", "en"])

def test_perfect_expression_score(scorer, perfect_expression, test_land):
    """Score parfait pour expression optimale."""
    result = scorer.compute_quality_score(perfect_expression, test_land)

    assert result["score"] >= 0.85
    assert result["category"] == "Excellent"
    assert len(result["flags"]) == 0  # Aucun flag de pÃ©nalitÃ©

def test_http_error_expression(scorer, test_land):
    """Score 0 pour erreur HTTP."""
    expr = Expression(http_status=404, content_type="text/html")
    result = scorer.compute_quality_score(expr, test_land)

    assert result["score"] == 0.0
    assert "http_error" in result["flags"]
    assert result["category"] == "TrÃ¨s faible"

def test_short_content_penalty(scorer, test_land):
    """PÃ©nalitÃ© pour contenu court."""
    expr = Expression(
        http_status=200,
        content_type="text/html",
        word_count=50,  # TrÃ¨s court
        content_length=5000,
        lang="fr"
    )
    result = scorer.compute_quality_score(expr, test_land)

    assert result["score"] < 0.5
    assert "short_content" in result["flags"] or "very_short_content" in result["flags"]

def test_wrong_language_penalty(scorer):
    """PÃ©nalitÃ© pour langue incorrecte."""
    expr = Expression(
        http_status=200,
        content_type="text/html",
        word_count=1000,
        lang="de"  # Allemand
    )
    land_fr = Land(lang=["fr"])  # Land FR seulement

    result = scorer.compute_quality_score(expr, land_fr)

    assert "wrong_language" in result["flags"]
    assert result["score"] < 0.7  # PÃ©nalitÃ© significative

def test_low_relevance_penalty(scorer, test_land):
    """PÃ©nalitÃ© pour faible pertinence."""
    expr = Expression(
        http_status=200,
        word_count=1000,
        relevance=0.2,  # TrÃ¨s faible
        lang="fr"
    )
    result = scorer.compute_quality_score(expr, test_land)

    assert "low_relevance" in result["flags"]

def test_pdf_content_blocked(scorer, test_land):
    """PDF doit Ãªtre bloquÃ© (score 0)."""
    expr = Expression(
        http_status=200,
        content_type="application/pdf"
    )
    result = scorer.compute_quality_score(expr, test_land)

    assert result["score"] == 0.0
    assert "non_html_pdf" in result["flags"]

def test_custom_weights(test_land):
    """Test avec pondÃ©rations personnalisÃ©es."""
    custom_weights = {
        "access": 0.5,  # Augmenter importance accÃ¨s
        "structure": 0.1,
        "richness": 0.2,
        "coherence": 0.1,
        "integrity": 0.1
    }
    scorer = QualityScorer(custom_weights=custom_weights)

    expr = Expression(http_status=200, content_type="text/html")
    result = scorer.compute_quality_score(expr, test_land)

    # Access parfait devrait donner score plus Ã©levÃ©
    assert result["score"] >= 0.4

def test_quality_result_structure(scorer, perfect_expression, test_land):
    """VÃ©rifier structure QualityResult."""
    result = scorer.compute_quality_score(perfect_expression, test_land)

    assert "score" in result
    assert "category" in result
    assert "flags" in result
    assert "reason" in result
    assert "details" in result

    assert isinstance(result["score"], float)
    assert isinstance(result["category"], str)
    assert isinstance(result["flags"], list)
    assert isinstance(result["details"], dict)

    # Details contient les 5 blocs
    assert "access" in result["details"]
    assert "structure" in result["details"]
    assert "richness" in result["details"]
    assert "coherence" in result["details"]
    assert "integrity" in result["details"]

def test_truth_table_validation(scorer):
    """Valider contre truth table."""
    with open("tests/data/quality_truth_table.json") as f:
        cases = json.load(f)

    for case in cases[:5]:  # Test premiers cas
        expr = create_expression_from_case(case)
        land = create_land_from_case(case)

        result = scorer.compute_quality_score(expr, land)

        assert case["expected_score_min"] <= result["score"] <= case["expected_score_max"], \
            f"Case {case['case_id']}: score hors limites"
```

**ExÃ©cution** :
```bash
pytest tests/unit/test_quality_scorer.py -v --cov=app.services.quality_scorer
```

**Attendu** : 15+ tests, couverture >90%

### Livrables Ã‰tape 4
- [ ] Fichier `quality_scorer.py` complet (300+ lignes)
- [ ] Configuration via settings
- [ ] Tests unitaires (15+ tests)
- [ ] Validation truth table passÃ©e
- [ ] Documentation docstrings

---

## ğŸ•·ï¸ Ã‰TAPE 5 â€“ IntÃ©gration Pipeline Crawler (DurÃ©e: 2-3h)

### Objectif
âš ï¸ **CRITIQUE** : IntÃ©grer quality dans LES DEUX crawlers (async + sync).

### Actions

#### 5.1 IntÃ©gration dans `crawler_engine.py` (ASYNC)

**Fichier** : `MyWebIntelligenceAPI/app/core/crawler_engine.py`

**Localisation** : AprÃ¨s extraction contenu + mÃ©tadonnÃ©es (ligne ~300)

```python
from app.services.quality_scorer import QualityScorer

class AsyncCrawlerEngine:
    def __init__(self, ...):
        # ... existing code ...
        self.quality_scorer = QualityScorer()  # âœ… NOUVEAU

    async def _process_expression(
        self,
        expr_url: str,
        expression: Expression,
        depth: int,
        land: Land,
        # ...
    ):
        # ... existing extraction logic ...

        # After all metadata extracted
        expression.title = extraction_result.get('title')
        expression.word_count = len(readable.split()) if readable else 0
        expression.relevance = await self.calculate_relevance(...)
        # ... other fields ...

        # âœ… NOUVEAU: Compute quality score
        if settings.ENABLE_QUALITY_SCORING:
            try:
                quality_result = self.quality_scorer.compute_quality_score(
                    expression=expression,
                    land=land
                )

                expression.quality_score = quality_result["score"]
                expression.quality_flags = quality_result["flags"]  # Si champ existe
                expression.quality_reason = quality_result["reason"]  # Si champ existe

                logger.debug(
                    f"Quality computed for {expr_url}: "
                    f"{quality_result['score']:.2f} ({quality_result['category']})"
                )
            except Exception as e:
                logger.error(f"Quality scoring failed for {expr_url}: {e}")
                # Continue without quality (non-blocking)

        # ... rest of processing ...
```

#### 5.2 IntÃ©gration dans `crawler_engine_sync.py` (SYNC)

**âš ï¸ DOUBLE CRAWLER - NE PAS OUBLIER !**

**Fichier** : `MyWebIntelligenceAPI/app/core/crawler_engine_sync.py`

**MÃŠME LOGIQUE** que async :

```python
from app.services.quality_scorer import QualityScorer

class SyncCrawlerEngine:
    def __init__(self, ...):
        # ... existing code ...
        self.quality_scorer = QualityScorer()  # âœ… NOUVEAU

    def _process_expression(
        self,
        expr_url: str,
        expression: Expression,
        depth: int,
        land: Land,
        # ...
    ):
        # ... existing extraction logic ...

        # âœ… NOUVEAU: Compute quality score (IDENTIQUE Ã  async)
        if settings.ENABLE_QUALITY_SCORING:
            try:
                quality_result = self.quality_scorer.compute_quality_score(
                    expression=expression,
                    land=land
                )

                expression.quality_score = quality_result["score"]
                expression.quality_flags = quality_result["flags"]
                expression.quality_reason = quality_result["reason"]

                logger.debug(
                    f"[SYNC] Quality computed for {expr_url}: "
                    f"{quality_result['score']:.2f} ({quality_result['category']})"
                )
            except Exception as e:
                logger.error(f"[SYNC] Quality scoring failed: {e}")

        # ... rest ...
```

#### 5.3 Configuration

**Fichier** : `MyWebIntelligenceAPI/.env`

```bash
# Quality Scoring
ENABLE_QUALITY_SCORING=true

# PondÃ©rations (optionnel, valeurs par dÃ©faut OK)
QUALITY_WEIGHT_ACCESS=0.30
QUALITY_WEIGHT_STRUCTURE=0.15
QUALITY_WEIGHT_RICHNESS=0.25
QUALITY_WEIGHT_COHERENCE=0.20
QUALITY_WEIGHT_INTEGRITY=0.10
```

#### 5.4 VÃ©rification RÃ¨gle Double Crawler

**Checklist OBLIGATOIRE** (voir `ERREUR_DOUBLE_CRAWLER.md`) :

- [ ] âœ… Modifier `crawler_engine.py` (async)
- [ ] âœ… Modifier `crawler_engine_sync.py` (sync)
- [ ] âœ… VÃ©rifier que les deux ont la MÃŠME logique quality
- [ ] âœ… Tester avec Celery (pas seulement l'API directe)

**VÃ©rification** :
```bash
# Chercher "quality_scorer" dans les DEUX crawlers
grep -n "quality_scorer" MyWebIntelligenceAPI/app/core/crawler_engine.py
grep -n "quality_scorer" MyWebIntelligenceAPI/app/core/crawler_engine_sync.py

# Les deux doivent avoir des lignes similaires !
```

### Tests Ã‰tape 5

#### Test d'IntÃ©gration Crawler Async

```python
# tests/integration/test_crawler_quality.py

import pytest
from app.core.crawler_engine import AsyncCrawlerEngine
from app.db.models import Expression, Land

@pytest.mark.asyncio
async def test_crawler_computes_quality(db_session, test_land):
    """VÃ©rifier que crawler async calcule quality_score."""

    crawler = AsyncCrawlerEngine(db_session=db_session)

    # Crawl test URL
    test_url = "https://httpbin.org/html"

    await crawler.crawl_url(test_url, land_id=test_land.id, depth=0)

    # RÃ©cupÃ©rer expression
    expression = db_session.query(Expression)\
        .filter_by(url=test_url)\
        .first()

    assert expression is not None
    assert expression.quality_score is not None
    assert 0.0 <= expression.quality_score <= 1.0

    # VÃ©rifier champs optionnels si prÃ©sents
    if hasattr(expression, 'quality_flags'):
        assert isinstance(expression.quality_flags, (list, type(None)))

@pytest.mark.asyncio
async def test_quality_score_realistic_range(db_session, test_land):
    """VÃ©rifier que scores sont dans plages rÃ©alistes."""

    crawler = AsyncCrawlerEngine(db_session=db_session)

    # Crawl plusieurs URLs
    urls = [
        "https://httpbin.org/html",
        "https://example.com"
    ]

    for url in urls:
        await crawler.crawl_url(url, land_id=test_land.id, depth=0)

    expressions = db_session.query(Expression)\
        .filter_by(land_id=test_land.id)\
        .all()

    scores = [e.quality_score for e in expressions if e.quality_score is not None]

    assert len(scores) > 0
    # Tous les scores entre 0 et 1
    assert all(0.0 <= s <= 1.0 for s in scores)
    # Au moins une variation (pas tous identiques)
    assert len(set(scores)) > 1 or len(scores) == 1
```

#### Test Crawler Sync (Celery)

```bash
# tests/integration/test-crawl-quality.sh

#!/bin/bash
# Test quality scoring integration in sync crawler (Celery)

# ... (authentication, create land)

# Crawl with quality enabled
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "limit": 5
  }'

# Wait for completion
sleep 30

# Verify quality in DB
docker exec mywebclient-db-1 psql -U mwi_user -d mwi_db -c "
  SELECT
    id,
    url,
    quality_score,
    http_status,
    word_count,
    relevance
  FROM expressions
  WHERE land_id = ${LAND_ID}
  ORDER BY quality_score DESC
  LIMIT 10;
"

# Check distribution
docker exec mywebclient-db-1 psql -U mwi_user -d mwi_db -c "
  SELECT
    CASE
      WHEN quality_score >= 0.8 THEN 'Excellent'
      WHEN quality_score >= 0.6 THEN 'Bon'
      WHEN quality_score >= 0.4 THEN 'Moyen'
      WHEN quality_score >= 0.2 THEN 'Faible'
      ELSE 'TrÃ¨s faible'
    END as category,
    COUNT(*)
  FROM expressions
  WHERE land_id = ${LAND_ID}
  GROUP BY category;
"
```

### Livrables Ã‰tape 5
- [ ] `crawler_engine.py` modifiÃ© avec quality
- [ ] `crawler_engine_sync.py` modifiÃ© (IDENTIQUE)
- [ ] Feature flag dans `.env`
- [ ] Tests intÃ©gration (2+ tests)
- [ ] Documentation dans `AGENTS.md` (section quality)

---

## ğŸ“¡ Ã‰TAPE 6 â€“ API & Exports (DurÃ©e: 1-2h)

### Objectif
Exposer `quality_score` dans les schÃ©mas API et exports.

### Actions

#### 6.1 VÃ©rifier SchÃ©mas Pydantic

**Fichier** : `MyWebIntelligenceAPI/app/schemas/expression.py`

```python
class ExpressionBase(BaseModel):
    # ... existing fields ...

    # Quality Score (devrait dÃ©jÃ  exister si champ en DB)
    quality_score: Optional[float] = Field(
        None,
        description="Quality score from 0.0 (very poor) to 1.0 (excellent)"
    )

    # âœ… NOUVEAU (si champs ajoutÃ©s)
    quality_flags: Optional[list[str]] = Field(
        None,
        description="Quality penalty flags (e.g., ['short_content', 'wrong_language'])"
    )
    quality_reason: Optional[str] = Field(
        None,
        description="Human-readable explanation of quality score"
    )

class ExpressionOut(ExpressionBase):
    """Schema for Expression API output."""
    id: int
    url: str
    # ... quality fields inherited from Base

    class Config:
        from_attributes = True
```

#### 6.2 Endpoints avec Filtrage Quality

**Fichier** : `MyWebIntelligenceAPI/app/api/v2/endpoints/lands_v2.py`

```python
@router.get("/{land_id}/expressions", response_model=List[ExpressionOut])
async def get_land_expressions(
    land_id: int,
    min_quality: float = Query(None, ge=0.0, le=1.0, description="Minimum quality score"),
    max_quality: float = Query(None, ge=0.0, le=1.0, description="Maximum quality score"),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user),
    # ... other filters ...
):
    """
    Get expressions for a land with optional quality filtering.

    Example:
        GET /api/v2/lands/36/expressions?min_quality=0.7
        â†’ Only high-quality expressions
    """
    query = select(Expression).filter(Expression.land_id == land_id)

    # âœ… NOUVEAU: Filtre quality
    if min_quality is not None:
        query = query.filter(Expression.quality_score >= min_quality)
    if max_quality is not None:
        query = query.filter(Expression.quality_score <= max_quality)

    # ... existing filters ...

    result = await db.execute(query)
    expressions = result.scalars().all()

    return expressions
```

#### 6.3 Stats Quality dans Land

**Ajout endpoint statistiques** :

```python
@router.get("/{land_id}/quality-stats")
async def get_land_quality_stats(
    land_id: int,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Get quality score distribution for a land.

    Returns:
        {
            "total_expressions": 1500,
            "with_quality": 1450,
            "avg_quality": 0.67,
            "distribution": {
                "Excellent": 250,
                "Bon": 600,
                "Moyen": 400,
                "Faible": 150,
                "TrÃ¨s faible": 50
            }
        }
    """
    from sqlalchemy import func, case

    # Total et moyenne
    stats_query = select(
        func.count(Expression.id).label("total"),
        func.count(Expression.quality_score).label("with_quality"),
        func.avg(Expression.quality_score).label("avg_quality")
    ).filter(Expression.land_id == land_id)

    stats = await db.execute(stats_query)
    row = stats.first()

    # Distribution par catÃ©gorie
    dist_query = select(
        case(
            (Expression.quality_score >= 0.8, "Excellent"),
            (Expression.quality_score >= 0.6, "Bon"),
            (Expression.quality_score >= 0.4, "Moyen"),
            (Expression.quality_score >= 0.2, "Faible"),
            else_="TrÃ¨s faible"
        ).label("category"),
        func.count().label("count")
    ).filter(
        Expression.land_id == land_id,
        Expression.quality_score.isnot(None)
    ).group_by("category")

    dist_result = await db.execute(dist_query)
    distribution = {cat: count for cat, count in dist_result}

    return {
        "total_expressions": row.total,
        "with_quality": row.with_quality,
        "avg_quality": round(row.avg_quality, 3) if row.avg_quality else None,
        "distribution": distribution
    }
```

#### 6.4 Exports CSV/JSON

**Fichier** : `MyWebIntelligenceAPI/app/services/export_service.py`

```python
def export_expressions_csv(self, land_id: int, ...) -> str:
    """
    Export expressions to CSV with quality fields.
    """
    # CSV headers
    headers = [
        'id', 'url', 'title', 'relevance', 'depth',
        'http_status', 'word_count', 'language',
        # âœ… NOUVEAU
        'quality_score', 'quality_category'
    ]

    # ... write CSV ...

    for expr in expressions:
        # DÃ©terminer catÃ©gorie
        if expr.quality_score is not None:
            if expr.quality_score >= 0.8:
                quality_cat = "Excellent"
            elif expr.quality_score >= 0.6:
                quality_cat = "Bon"
            elif expr.quality_score >= 0.4:
                quality_cat = "Moyen"
            elif expr.quality_score >= 0.2:
                quality_cat = "Faible"
            else:
                quality_cat = "TrÃ¨s faible"
        else:
            quality_cat = "N/A"

        writer.writerow([
            expr.id,
            expr.url,
            expr.title,
            expr.relevance,
            expr.depth,
            expr.http_status,
            expr.word_count,
            expr.lang,
            expr.quality_score,
            quality_cat
        ])
```

### Tests Ã‰tape 6

```python
# tests/api/test_expression_quality_api.py

import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
async def test_get_expression_includes_quality(
    client: AsyncClient,
    test_expression_with_quality
):
    """VÃ©rifier GET /expressions/{id} retourne quality_score."""

    response = await client.get(
        f"/api/v2/expressions/{test_expression_with_quality.id}"
    )

    assert response.status_code == 200
    data = response.json()

    assert "quality_score" in data
    assert isinstance(data["quality_score"], (float, type(None)))

    if data["quality_score"] is not None:
        assert 0.0 <= data["quality_score"] <= 1.0

@pytest.mark.asyncio
async def test_filter_by_quality(client: AsyncClient, test_land):
    """VÃ©rifier filtrage par quality_score."""

    # CrÃ©er expressions avec diffÃ©rents scores
    # ... (setup)

    # Filtrer >= 0.7
    response = await client.get(
        f"/api/v2/lands/{test_land.id}/expressions",
        params={"min_quality": 0.7}
    )

    assert response.status_code == 200
    data = response.json()

    # Toutes les expressions retournÃ©es doivent avoir score >= 0.7
    for expr in data:
        if expr["quality_score"] is not None:
            assert expr["quality_score"] >= 0.7

@pytest.mark.asyncio
async def test_quality_stats_endpoint(client: AsyncClient, test_land):
    """VÃ©rifier endpoint statistiques quality."""

    response = await client.get(
        f"/api/v2/lands/{test_land.id}/quality-stats"
    )

    assert response.status_code == 200
    data = response.json()

    assert "total_expressions" in data
    assert "avg_quality" in data
    assert "distribution" in data

    # VÃ©rifier cohÃ©rence
    total_in_dist = sum(data["distribution"].values())
    assert total_in_dist <= data["total_expressions"]

@pytest.mark.asyncio
async def test_csv_export_includes_quality(client: AsyncClient, test_land):
    """VÃ©rifier export CSV contient quality_score."""

    response = await client.post(
        "/api/v1/export/direct",
        json={
            "land_id": test_land.id,
            "export_type": "pagecsv"
        }
    )

    assert response.status_code == 200
    csv_content = response.text

    # VÃ©rifier header
    assert "quality_score" in csv_content
    assert "quality_category" in csv_content or "quality_cat" in csv_content
```

### Livrables Ã‰tape 6
- [ ] SchÃ©mas Pydantic vÃ©rifiÃ©s/mis Ã  jour
- [ ] Endpoint filtrage quality (`min_quality`, `max_quality`)
- [ ] Endpoint stats quality (`/quality-stats`)
- [ ] Export CSV/JSON avec quality
- [ ] Tests API (4+ tests)

---

## ğŸ”„ Ã‰TAPE 7 â€“ Reprocessing Historique (DurÃ©e: 2-3h)

### Objectif
Recalculer quality_score sur expressions existantes (batch).

### Actions

#### 7.1 Script CLI Reprocessing

**Nouveau fichier** : `MyWebIntelligenceAPI/scripts/admin/reprocess_quality.py`

```python
#!/usr/bin/env python3
"""
CLI tool to reprocess quality_score for existing expressions.

Usage:
    python scripts/admin/reprocess_quality.py --land-id 36 --limit 100
    python scripts/admin/reprocess_quality.py --all --dry-run
"""

import argparse
import asyncio
import logging
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select

from app.db.session import AsyncSessionLocal
from app.db.models import Expression, Land
from app.services.quality_scorer import QualityScorer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def reprocess_quality(
    land_id: int = None,
    limit: int = None,
    dry_run: bool = False,
    force: bool = False
):
    """
    Reprocess quality_score for expressions.

    Args:
        land_id: If provided, only process this land
        limit: Max expressions to process
        dry_run: Don't commit changes
        force: Recompute even if quality_score exists
    """
    async with AsyncSessionLocal() as db:
        scorer = QualityScorer()

        # Build query
        query = select(Expression, Land).join(Land)

        if land_id:
            query = query.filter(Expression.land_id == land_id)

        if not force:
            # Only expressions without quality_score
            query = query.filter(Expression.quality_score.is_(None))

        if limit:
            query = query.limit(limit)

        result = await db.execute(query)
        pairs = result.all()

        logger.info(f"Processing {len(pairs)} expressions...")

        stats = {
            "processed": 0,
            "excellent": 0,
            "good": 0,
            "medium": 0,
            "poor": 0,
            "very_poor": 0,
            "errors": 0
        }

        for expr, land in pairs:
            try:
                # Compute quality
                quality_result = scorer.compute_quality_score(expr, land)

                if not dry_run:
                    expr.quality_score = quality_result["score"]
                    if hasattr(expr, 'quality_flags'):
                        expr.quality_flags = quality_result["flags"]
                    if hasattr(expr, 'quality_reason'):
                        expr.quality_reason = quality_result["reason"]

                # Update stats
                stats["processed"] += 1

                if quality_result["score"] >= 0.8:
                    stats["excellent"] += 1
                elif quality_result["score"] >= 0.6:
                    stats["good"] += 1
                elif quality_result["score"] >= 0.4:
                    stats["medium"] += 1
                elif quality_result["score"] >= 0.2:
                    stats["poor"] += 1
                else:
                    stats["very_poor"] += 1

                if stats["processed"] % 100 == 0:
                    logger.info(f"  Processed {stats['processed']}/{len(pairs)}...")

            except Exception as e:
                logger.error(f"Error processing expression {expr.id}: {e}")
                stats["errors"] += 1

        if not dry_run:
            await db.commit()
            logger.info("âœ… Changes committed to database")
        else:
            logger.info("ğŸ” Dry run - no changes committed")

        # Print stats
        logger.info("\nğŸ“Š STATISTICS:")
        logger.info(f"  Total processed: {stats['processed']}")
        logger.info(f"  Excellent (>=0.8): {stats['excellent']} ({stats['excellent']/stats['processed']*100:.1f}%)")
        logger.info(f"  Good (0.6-0.8): {stats['good']} ({stats['good']/stats['processed']*100:.1f}%)")
        logger.info(f"  Medium (0.4-0.6): {stats['medium']} ({stats['medium']/stats['processed']*100:.1f}%)")
        logger.info(f"  Poor (0.2-0.4): {stats['poor']} ({stats['poor']/stats['processed']*100:.1f}%)")
        logger.info(f"  Very Poor (<0.2): {stats['very_poor']} ({stats['very_poor']/stats['processed']*100:.1f}%)")
        logger.info(f"  Errors: {stats['errors']}")

        return stats

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--land-id", type=int, help="Land ID to process")
    parser.add_argument("--limit", type=int, help="Max expressions to process")
    parser.add_argument("--dry-run", action="store_true", help="Don't commit changes")
    parser.add_argument("--force", action="store_true", help="Recompute existing scores")
    parser.add_argument("--all", action="store_true", help="Process all lands")

    args = parser.parse_args()

    if not args.land_id and not args.all:
        parser.error("Must specify --land-id or --all")

    asyncio.run(reprocess_quality(
        land_id=args.land_id,
        limit=args.limit,
        dry_run=args.dry_run,
        force=args.force
    ))
```

**Usage** :
```bash
# Dry-run sur land 36 (100 expressions)
python scripts/admin/reprocess_quality.py --land-id 36 --limit 100 --dry-run

# Reprocess pour de vrai
python scripts/admin/reprocess_quality.py --land-id 36 --limit 1000

# Reprocess tout (avec prudence !)
python scripts/admin/reprocess_quality.py --all --force
```

#### 7.2 Task Celery (Optionnel)

**Fichier** : `MyWebIntelligenceAPI/app/tasks/quality_task.py`

```python
"""
Celery tasks for quality score batch processing.
"""

from app.core.celery_app import celery_app

@celery_app.task(bind=True, name="quality.batch_reprocess")
def batch_reprocess_quality(self, land_id: int = None, limit: int = 100):
    """
    Batch reprocess quality for expressions (Celery task).
    """
    import asyncio
    from scripts.admin.reprocess_quality import reprocess_quality

    # Run async function in Celery context
    stats = asyncio.run(reprocess_quality(
        land_id=land_id,
        limit=limit,
        dry_run=False,
        force=False
    ))

    return stats
```

**Endpoint API** :
```python
@router.post("/{land_id}/reprocess-quality")
async def reprocess_land_quality(
    land_id: int,
    limit: int = Query(100, description="Max expressions to reprocess"),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Launch quality reprocessing for a land (Celery task).
    """
    from app.tasks.quality_task import batch_reprocess_quality

    # Verify land exists
    land = await db.get(Land, land_id)
    if not land:
        raise HTTPException(404, "Land not found")

    # Launch task
    task = batch_reprocess_quality.delay(
        land_id=land_id,
        limit=limit
    )

    return {
        "task_id": task.id,
        "status": "processing",
        "land_id": land_id,
        "limit": limit
    }
```

### Tests Ã‰tape 7

```bash
# Test dry-run
python MyWebIntelligenceAPI/scripts/admin/reprocess_quality.py \
  --land-id 36 --limit 10 --dry-run

# VÃ©rifier stats affichÃ©es
# VÃ©rifier aucune modification en DB

# Test reprocess rÃ©el (petit Ã©chantillon)
python MyWebIntelligenceAPI/scripts/admin/reprocess_quality.py \
  --land-id 36 --limit 50

# VÃ©rifier DB
docker exec mywebclient-db-1 psql -U mwi_user -d mwi_db -c \
  "SELECT COUNT(*), AVG(quality_score)
   FROM expressions
   WHERE land_id = 36 AND quality_score IS NOT NULL;"
```

### Livrables Ã‰tape 7
- [ ] Script CLI `reprocess_quality.py`
- [ ] Task Celery (optionnel)
- [ ] Endpoint API `/reprocess-quality` (optionnel)
- [ ] Tests dry-run + reprocess rÃ©el
- [ ] Documentation usage dans README

---

## ğŸ“Š Ã‰TAPE 8 â€“ Monitoring & Exploitation (DurÃ©e: 2h)

### Objectif
Instrumenter le systÃ¨me, crÃ©er dashboards, documenter exploitation.

### Actions

#### 8.1 MÃ©triques Prometheus

**Fichier** : `MyWebIntelligenceAPI/app/services/quality_scorer.py`

```python
from prometheus_client import Histogram, Counter, Gauge

# MÃ©triques quality
quality_score_histogram = Histogram(
    'quality_score_distribution',
    'Distribution of quality scores',
    buckets=[0, 0.2, 0.4, 0.6, 0.8, 1.0]
)

quality_computation_duration = Histogram(
    'quality_computation_duration_seconds',
    'Time to compute quality score'
)

quality_flags_counter = Counter(
    'quality_flags_total',
    'Total quality flags by type',
    ['flag_type']
)

class QualityScorer:
    def compute_quality_score(self, expression, land):
        import time
        start = time.time()

        # ... computation ...

        result = {...}

        # Record metrics
        duration = time.time() - start
        quality_computation_duration.observe(duration)
        quality_score_histogram.observe(result["score"])

        for flag in result["flags"]:
            quality_flags_counter.labels(flag_type=flag).inc()

        return result
```

#### 8.2 Dashboard Grafana (Exemple)

**Panels recommandÃ©s** :

1. **Quality Score Distribution**
   ```promql
   histogram_quantile(0.5, quality_score_distribution)
   histogram_quantile(0.9, quality_score_distribution)
   ```

2. **Average Quality by Land**
   ```sql
   SELECT land_id, AVG(quality_score)
   FROM expressions
   GROUP BY land_id
   ```

3. **Top Quality Flags**
   ```promql
   topk(10, rate(quality_flags_total[5m]))
   ```

4. **Quality Computation Latency (p95)**
   ```promql
   histogram_quantile(0.95, quality_computation_duration_seconds)
   ```

#### 8.3 Guide d'Exploitation

**Nouveau fichier** : `MyWebIntelligenceAPI/docs/QUALITY_OPERATIONS.md`

```markdown
# Quality Score - Guide d'Exploitation

## Activation/DÃ©sactivation

### DÃ©sactiver temporairement
\`\`\`bash
# Dans .env
ENABLE_QUALITY_SCORING=false

# RedÃ©marrer
docker compose restart api celery_worker
\`\`\`

## Monitoring

### MÃ©triques ClÃ©s
- `quality_score_distribution` : Distribution des scores (histogramme)
- `quality_flags_total` : Flags de pÃ©nalitÃ© par type
- `quality_computation_duration` : Latence de calcul

### RequÃªtes SQL Utiles

\`\`\`sql
-- Distribution quality par land
SELECT
  land_id,
  AVG(quality_score) as avg_quality,
  COUNT(*) as total
FROM expressions
WHERE quality_score IS NOT NULL
GROUP BY land_id
ORDER BY avg_quality DESC;

-- Top 10 meilleurs contenus
SELECT url, quality_score, word_count, relevance
FROM expressions
ORDER BY quality_score DESC
LIMIT 10;

-- Flags les plus frÃ©quents
SELECT unnest(quality_flags) as flag, COUNT(*)
FROM expressions
WHERE quality_flags IS NOT NULL
GROUP BY flag
ORDER BY COUNT(*) DESC;
\`\`\`

## Troubleshooting

### SymptÃ´me : Tous les scores = 0.0
**Cause** : HTTP errors (404, 500) ou contenu non-HTML
**Action** : VÃ©rifier `http_status` et `content_type` dans expressions

### SymptÃ´me : Scores trop faibles (<0.3) pour bon contenu
**Cause** : PondÃ©rations inadaptÃ©es
**Action** : Ajuster weights dans `.env` :
\`\`\`bash
QUALITY_WEIGHT_ACCESS=0.20  # RÃ©duire importance HTTP
QUALITY_WEIGHT_RICHNESS=0.35  # Augmenter importance contenu
\`\`\`

### SymptÃ´me : Scores NULL aprÃ¨s crawl
**Cause** : Feature flag dÃ©sactivÃ© ou erreur dans scorer
**Action** :
1. VÃ©rifier `ENABLE_QUALITY_SCORING=true`
2. Consulter logs : `docker logs mywebclient-api-1 | grep quality`
3. Test manuel : `python -c "from app.services.quality_scorer import QualityScorer; ..."`

## Ajustement des Seuils

Si la distribution actuelle ne convient pas :

\`\`\`python
# Analyser distribution actuelle
import pandas as pd
scores = pd.read_sql("SELECT quality_score FROM expressions", con)
print(scores['quality_score'].describe())

# Ajuster WEIGHTS en consÃ©quence
\`\`\`

## Rollback

\`\`\`bash
# 1. DÃ©sactiver quality
ENABLE_QUALITY_SCORING=false

# 2. (Optionnel) RÃ©initialiser scores en DB
docker exec mywebclient-db-1 psql -U mwi_user -d mwi_db -c \
  "UPDATE expressions SET quality_score = NULL;"

# 3. RedÃ©marrer
docker compose restart api celery_worker
\`\`\`
\`\`\`

#### 8.4 Alertes RecommandÃ©es

```yaml
# alerts.yml (Prometheus)
groups:
  - name: quality_score
    rules:
      - alert: QualityScoreVeryLow
        expr: avg(quality_score_distribution) < 0.3
        for: 10m
        annotations:
          summary: "Average quality score trÃ¨s faible"
          description: "Score moyen < 0.3 depuis 10min"

      - alert: HighErrorRate
        expr: rate(quality_flags_total{flag_type="http_error"}[5m]) > 0.5
        for: 5m
        annotations:
          summary: "Taux Ã©levÃ© d'erreurs HTTP"
          description: ">50% des pages en erreur HTTP"
```

### Tests Ã‰tape 8

```bash
# Smoke test post-dÃ©ploiement
./MyWebIntelligenceAPI/tests/smoke/test_quality_smoke.sh

# VÃ©rifier mÃ©triques Prometheus
curl http://localhost:8000/metrics | grep quality_score

# VÃ©rifier dashboard Grafana
open http://localhost:3001
# â†’ Naviguer vers dashboard "Quality Score"

# Test alertes (optionnel)
# Simuler crawl avec beaucoup d'erreurs HTTP
# VÃ©rifier alerte dÃ©clenchÃ©e
```

### Livrables Ã‰tape 8
- [ ] MÃ©triques Prometheus instrumentÃ©es
- [ ] Dashboard Grafana (optionnel)
- [ ] Guide exploitation (`QUALITY_OPERATIONS.md`)
- [ ] Alertes configurÃ©es (optionnel)
- [ ] Smoke test post-dÃ©ploiement

---

## ğŸ“ Documentation Finale

### Fichiers CrÃ©Ã©s/Mis Ã  Jour

1. **`.claude/docs/quality_dev.md`** (ce fichier) âœ…
   - Plan dÃ©taillÃ© complet

2. **`MyWebIntelligenceAPI/app/services/quality_scorer.py`** (NOUVEAU)
   - Service de calcul quality_score (300+ lignes)

3. **`MyWebIntelligenceAPI/scripts/admin/reprocess_quality.py`** (NOUVEAU)
   - Script CLI reprocessing (150+ lignes)

4. **`MyWebIntelligenceAPI/tests/unit/test_quality_scorer.py`** (NOUVEAU)
   - Tests unitaires scorer (15+ tests)

5. **`MyWebIntelligenceAPI/tests/integration/test_crawler_quality.py`** (NOUVEAU)
   - Tests intÃ©gration crawlers (5+ tests)

6. **`MyWebIntelligenceAPI/tests/data/quality_truth_table.json`** (NOUVEAU)
   - Corpus de validation 20 cas

7. **`MyWebIntelligenceAPI/docs/QUALITY_OPERATIONS.md`** (NOUVEAU)
   - Guide opÃ©rationnel

8. **`.claude/AGENTS.md`** (MIS Ã€ JOUR)
   - Ajouter section "Quality Scoring"
   - Checklist double crawler

9. **`MyWebIntelligenceAPI/README.md`** (MIS Ã€ JOUR)
   - Feature quality_score
   - Variables d'environnement

---

## âœ… Checklist Globale

### Ã‰tape 1 - Audit Existant âœ…
- [x] VÃ©rifier champ `quality_score` en DB
- [ ] Inventorier mÃ©tadonnÃ©es disponibles
- [ ] Analyser distribution valeurs
- [ ] Tests audit SQL

### Ã‰tape 2 - Cadrage MÃ©tier
- [ ] DÃ©finir objectifs (filtrage, priorisation, etc.)
- [ ] Valider Ã©chelle (0-1) et seuils (5 catÃ©gories)
- [ ] CrÃ©er truth table (20 cas)
- [ ] Validation PO/data analyst

### Ã‰tape 3 - Conception Score
- [ ] ModÃ¨le 5 blocs avec pondÃ©rations
- [ ] Fonctions score_access, score_structure, etc.
- [ ] Structure QualityResult
- [ ] Notebook prototypage validÃ©

### Ã‰tape 4 - Service
- [ ] CrÃ©er `QualityScorer` classe
- [ ] Configuration via settings
- [ ] Tests unitaires (15+ tests)
- [ ] Couverture >90%

### Ã‰tape 5 - IntÃ©gration Crawlers âš ï¸ CRITIQUE
- [ ] Modifier `crawler_engine.py` (async)
- [ ] Modifier `crawler_engine_sync.py` (sync)
- [ ] VÃ©rifier PARITÃ‰
- [ ] Tests intÃ©gration (async + sync)

### Ã‰tape 6 - API
- [ ] SchÃ©mas Pydantic vÃ©rifiÃ©s
- [ ] Endpoint filtrage (`min_quality`)
- [ ] Endpoint stats (`/quality-stats`)
- [ ] Export CSV/JSON
- [ ] Tests API (4+ tests)

### Ã‰tape 7 - Reprocessing
- [ ] Script CLI `reprocess_quality.py`
- [ ] Task Celery (optionnel)
- [ ] Tests dry-run + rÃ©el
- [ ] Documentation usage

### Ã‰tape 8 - Monitoring
- [ ] MÃ©triques Prometheus
- [ ] Dashboard Grafana (optionnel)
- [ ] Guide exploitation
- [ ] Alertes (optionnel)
- [ ] Smoke test

---

## ğŸš€ Prochaines Actions ImmÃ©diates

### Pour DÃ©marrer (Aujourd'hui)
1. Valider plan avec Ã©quipe/PO
2. Confirmer pondÃ©rations mÃ©tier (WEIGHTS)
3. CrÃ©er truth table (20 cas) avec data analyst
4. Analyser distribution mÃ©tadonnÃ©es actuelles (SQL)

### Sprint 1 (Semaine 1)
- Ã‰tapes 1-3 : Audit + cadrage + conception
- Prototype notebook

### Sprint 2 (Semaine 2)
- Ã‰tapes 4-5 : Service + intÃ©gration crawlers
- Tests unitaires + intÃ©gration

### Sprint 3 (Semaine 3)
- Ã‰tapes 6-7 : API + reprocessing
- Tests end-to-end

### Sprint 4 (Semaine 4)
- Ã‰tape 8 : Monitoring + dÃ©ploiement
- Validation mÃ©tier

---

## ğŸ“ Support & Questions

### Points de Blocage Potentiels

1. **PondÃ©rations inadaptÃ©es**
   - Solution : ItÃ©rer avec PO sur truth table, ajuster WEIGHTS

2. **Distribution biaisÃ©e (tous hauts ou bas)**
   - Solution : Recalibrer seuils, vÃ©rifier donnÃ©es d'entrÃ©e

3. **Champs manquants (word_count, etc.)**
   - Solution : VÃ©rifier extraction contenu, ajouter fallbacks

4. **Double crawler oubliÃ©**
   - Solution : Checklist stricte, revue code obligatoire

### Contacts
- Architecture : Voir `AGENTS.md`, `Architecture.md`
- MÃ©tier : Valider pondÃ©rations avec PO/data analyst
- Bugs crawlers : Voir `ERREUR_DOUBLE_CRAWLER.md`

---

## ğŸ“Š Comparaison Sentiment vs Quality

| Aspect | Sentiment Score | Quality Score |
|--------|----------------|---------------|
| **MÃ©thode** | ML (TextBlob) ou LLM | Heuristique pure |
| **DÃ©pendances** | `textblob` (15 MB) | Aucune (0 MB) âœ… |
| **Latence** | 50ms (TextBlob) | <10ms âœ… |
| **CoÃ»t** | Gratuit (TextBlob) | Gratuit âœ… |
| **ReproductibilitÃ©** | Stochastique (LLM) | 100% dÃ©terministe âœ… |
| **ComplexitÃ©** | ModÃ©rÃ©e | Faible âœ… |
| **Validation** | Corpus annotÃ© humain | Truth table mÃ©tier |
| **Tuning** | Difficile (modÃ¨le figÃ©) | Facile (ajuster weights) âœ… |

**Conclusion** : Quality score est **plus simple, plus rapide, et plus flexible** que sentiment !

---

**DerniÃ¨re mise Ã  jour** : 18 octobre 2025
**Version** : 1.0
**Auteur** : Assistant AI (Claude)
**Statut** : Plan dÃ©taillÃ© prÃªt pour implÃ©mentation
