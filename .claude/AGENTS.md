# MyWebIntelligence API - Guide Complet pour Agents

MyWebIntelligence est une API FastAPI encapsulant les fonctionnalit√©s du crawler MyWebIntelligencePython. Elle permet l'int√©gration avec MyWebClient et ouvre la voie √† un d√©ploiement SaaS scalable.

## üéØ Concepts Cl√©s

### Qu'est-ce qu'un "Land" ?
Un **Land** est un projet de crawling/recherche qui contient :
- **URLs de d√©part** (`start_urls`) : Points d'entr√©e du crawl
- **Mots-cl√©s** (`words`) : Termes √† rechercher avec leurs lemmes
- **Configuration** : Langues support√©es, limites, etc.
- **R√©sultats** : Expressions (pages) et m√©dias d√©couverts

### Qu'est-ce qu'une "Expression" ?
Une **Expression** est une page web crawl√©e qui contient :
- **URL** et **contenu** de la page
- **Profondeur** (`depth`) : Distance depuis les URLs de d√©part (0 = URL initiale, 1 = lien direct, etc.)
- **Pertinence** (`relevance`) : Score de correspondance avec les mots-cl√©s
- **M√©dias** associ√©s (images, vid√©os, audio)

### Workflow Typique
1. **Cr√©er un Land** avec URLs de d√©part et mots-cl√©s
2. **Lancer le crawl** ‚Üí D√©couverte d'expressions et m√©dias
3. **Pipeline Readable** ‚Üí Extraction de contenu lisible (Mercury-like)
4. **Analyser les m√©dias** ‚Üí Extraction de m√©tadonn√©es (couleurs, dimensions, EXIF)
5. **Exporter les donn√©es** ‚Üí CSV, GEXF, JSON

## üöÄ D√©marrage Rapide

### Authentification
```bash
# Option 1‚ÄØ: depuis l‚Äôh√¥te
TOKEN=$(curl -s -X POST "http://localhost:8000/api/v1/auth/login" -H "Content-Type: application/x-www-form-urlencoded" -d "username=admin@example.com&password=changeme" | jq -r .access_token)

# Option 2‚ÄØ: via Docker Compose (container ¬´‚ÄØapi‚ÄØ¬ª)
TOKEN=$(docker compose exec api curl -s -X POST "http://localhost:8000/api/v1/auth/login" -H "Content-Type: application/x-www-form-urlencoded" -d "username=${MYWI_USERNAME:-admin@example.com}&password=${MYWI_PASSWORD:-changeme}" | jq -r .access_token)

echo "Token JWT: $TOKEN"
```

### Serveur Docker
```bash
# (Re)d√©ployer avec migrations fra√Æchement appliqu√©es
docker compose down -v
docker compose up --build -d

# V√©rifier les containers
docker ps | grep mywebintelligenceapi

# Red√©marrer si n√©cessaire
docker restart mywebintelligenceapi

# Tester que le serveur r√©pond
curl -s -w "%{http_code}" "http://localhost:8000/" -o /dev/null
```

## üèóÔ∏è Architecture de l'API

### Structure des Endpoints

#### API v1 (Legacy)
- `/api/v1/auth/` - Authentification
- `/api/v1/export/` - Export de donn√©es (CSV, GEXF)

#### API v2 (Moderne)
- `/api/v2/lands/` - Gestion des projets (lands)
- `/api/v2/lands/{id}/crawl` - Lancement de crawls
- `/api/v2/lands/{id}/readable` - Pipeline readable (extraction contenu)
- `/api/v2/lands/{id}/media-analysis-async` - Analyse des m√©dias (asynchrone)
- `/api/v2/lands/{id}/stats` - Statistiques

### Mod√®les de Donn√©es

#### Land (Projet)
```python
id: int                    # ID unique
name: str                  # Nom du projet
description: str           # Description
owner_id: int              # Propri√©taire (FK users.id)
lang: List[str]            # Langues ["fr", "en"]
start_urls: List[str]      # URLs de d√©part
crawl_status: str          # "pending", "running", "completed"
total_expressions: int     # Nombre d'expressions
total_domains: int         # Nombre de domaines
words: List[dict]          # Mots-cl√©s avec lemmes
```

#### Expression (Page crawl√©e)
```python
id: int                    # ID unique
land_id: int               # Projet parent
domain_id: int             # Domaine parent
url: str                   # URL de la page
title: str                 # Titre
content: str               # Contenu HTML
readable: str              # Contenu lisible (markdown)
depth: int                 # Profondeur de crawl
relevance: float           # Score de pertinence
language: str              # Langue d√©tect√©e
word_count: int            # Nombre de mots
```

#### Media (Fichiers m√©dia)
```python
id: int                    # ID unique
expression_id: int         # Expression parent
url: str                   # URL du m√©dia
type: str                  # "img", "video", "audio"
is_processed: bool         # Analys√© ou non
width: int                 # Largeur (images)
height: int                # Hauteur (images)
file_size: int             # Taille en bytes
metadata: dict             # M√©tadonn√©es EXIF, etc.
dominant_colors: List[str] # Couleurs dominantes
```

## üÜï Mises √† jour structurelles (juillet 2025)

- **Sch√©ma SQLAlchemy r√©align√©**  
  - `domains` conserve `http_status` et `fetched_at`.  
  - `expressions` stocke d√©sormais `published_at`, `approved_at`, `validllm`, `validmodel`, `seorank`, et la langue d√©tect√©e.  
  - `words` embarque `language` et `frequency`. Une migration Alembic `006_add_legacy_crawl_columns.py` doit √™tre appliqu√©e.

- **Dictionnaire de land**  
  - `DictionaryService.populate_land_dictionary` accepte les seeds (`words`) fournis √† la cr√©ation et cr√©e automatiquement des entr√©es `Word` multilingues.  
  - Les variations g√©n√©r√©es via `get_lemma` remplissent la table `land_dictionaries` sans manipuler directement la relation ORM.

- **Service de crawl**  
  - `start_crawl_for_land` renvoie un `CrawlJobResponse` typ√© (avec `ws_channel`) et convertit les filtres `http_status` en entiers avant insertion.  
  - `CrawlerEngine` persiste la langue, approuve les expressions pertinentes et peuple les nouveaux champs de m√©tadonn√©es.
- **Migrations automatiques**  
  - Les services `api` et `celery_worker` ex√©cutent `alembic upgrade head` √† chaque d√©marrage du conteneur. Reconstruis la stack (`docker compose down -v && docker compose up --build`) apr√®s un pull pour appliquer les derniers sch√©mas.

- **Tests & environnement**  
  - Les tests de crawling n√©cessitent `pytest`, `sqlalchemy`, `aiosqlite` dans le venv.  
  - Sous Python 3.13, certaines wheels (`pydantic-core`, `asyncpg`, `pillow`) √©chouent √† la compilation ; privil√©gier Python 3.11/3.12 ou installer Rust + toolchain compatible.

## üîÑ Pipelines de Traitement

### 1. Pipeline de Crawl
```
Start URLs ‚Üí Crawler ‚Üí Pages ‚Üí Content Extraction ‚Üí Expressions
                          ‚Üì
                     Media Detection ‚Üí Media Records
```

**Endpoint:** `POST /api/v2/lands/{id}/crawl`
```json
{
  "limit": 10,              // Nombre max de pages
  "depth": 2,               // Profondeur max
  "analyze_media": true     // Analyser les m√©dias
}
```

### 2. Pipeline d'Analyse Media
```
Expressions ‚Üí Media URLs ‚Üí Download ‚Üí Analysis ‚Üí Metadata Storage
                               ‚Üì
                         PIL/OpenCV ‚Üí Colors, Dimensions, EXIF
```

**Endpoint:** `POST /api/v2/lands/{id}/media-analysis-async`
```json
{
  "depth": 1,               // Profondeur max des expressions √† analyser (0=URLs initiales, 1=liens directs, etc.)
  "minrel": 0.5             // Score de pertinence minimum des expressions
}
```

**IMPORTANT:** `depth` ne limite PAS le nombre d'unit√©s/m√©dias √† analyser, mais la profondeur des expressions source !
- `depth: 0` = Analyser uniquement les m√©dias des URLs de d√©part
- `depth: 1` = Inclure aussi les m√©dias des pages li√©es directement 
- `depth: 999` = Analyser tous les m√©dias sans limite de profondeur

**STRAT√âGIE de LIMITATION:** Pour limiter le nombre de m√©dias analys√©s, utiliser `minrel` (pertinence) :
- `minrel: 0.0` = Toutes les expressions
- `minrel: 1.0` = Expressions moyennement pertinentes  
- `minrel: 3.0` = Expressions tr√®s pertinentes seulement (recommand√© pour tests)
- `minrel: 5.0` = Expressions extr√™mement pertinentes

### 3. Pipeline Readable (Nouveau)
```
Expressions ‚Üí Content Extraction ‚Üí Readable Content ‚Üí Text Processing ‚Üí Paragraphs
                     ‚Üì
            Mercury/Trafilatura ‚Üí Clean Text ‚Üí LLM Validation ‚Üí Storage
```

**Endpoint:** `POST /api/v2/lands/{id}/readable`
```json
{
  "limit": 50,              // Nombre max d'expressions √† traiter
  "depth": 1,               // Profondeur max des expressions
  "merge_strategy": "smart_merge",  // "mercury_priority", "preserve_existing"
  "enable_llm": false,      // Validation LLM (optionnel)
  "batch_size": 10,         // Expressions par batch
  "max_concurrent": 5       // Batches concurrents
}
```

**IMPORTANT:** Le pipeline readable transforme le contenu HTML brut des expressions en contenu lisible et structur√© :
- **Extraction intelligente** : Utilise Trafilatura puis Mercury fallback
- **Nettoyage** : Supprime le markup, garde le contenu principal
- **Validation LLM** : Optionnelle, am√©liore la qualit√© du contenu
- **Segmentation** : D√©coupe en paragraphes pour l'embedding

### 4. Pipeline LLM Validation (Int√©gr√©) ‚úÖ
```
Expressions ‚Üí OpenRouter API ‚Üí Relevance Check ‚Üí Database Update
                     ‚Üì
              "oui"/"non" ‚Üí valid_llm, valid_model ‚Üí Relevance=0 si non pertinent
```

**Int√©gration dans les pipelines existants :**
- **Crawl avec LLM** : `POST /api/v2/lands/{id}/crawl` avec `"enable_llm": true`
- **Readable avec LLM** : `POST /api/v2/lands/{id}/readable` avec `"enable_llm": true`

**Configuration OpenRouter requise :**
```bash
export OPENROUTER_ENABLED=True
export OPENROUTER_API_KEY=sk-or-v1-your-key-here
export OPENROUTER_MODEL=anthropic/claude-3.5-sonnet
```

**Exemples d'usage :**
```bash
# Crawl avec validation LLM
curl -X POST "http://localhost:8000/api/v2/lands/36/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"limit": 5, "enable_llm": true}'

# Readable avec validation LLM  
curl -X POST "http://localhost:8000/api/v2/lands/36/readable" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"limit": 3, "enable_llm": true}'
```

**R√©sultats stock√©s :**
- `valid_llm` : "oui" (pertinent) ou "non" (non pertinent)
- `valid_model` : Mod√®le utilis√© (ex: "anthropic/claude-3.5-sonnet")
- `relevance` : Mis √† 0 si expression jug√©e non pertinente

### 5. Pipeline d'Export
```
Data Selection ‚Üí Format Conversion ‚Üí Response
     ‚Üì
CSV/GEXF/JSON ‚Üí Compressed ‚Üí Download
```

**Endpoint:** `POST /api/v1/export/direct`
```json
{
  "land_id": 36,
  "export_type": "mediacsv", // "pagecsv", "nodecsv", etc.
  "limit": 100
}
```

## üóÑÔ∏è Base de Donn√©es

### Tables Principales
- `users` - Utilisateurs
- `lands` - Projets de crawl
- `domains` - Domaines web
- `expressions` - Pages crawl√©es
- `media` - Fichiers m√©dia
- `paragraphs` - Contenu segment√©
- `crawl_jobs` - Jobs Celery

### Relations Cl√©s
```
users (1) ‚Üí (n) lands
lands (1) ‚Üí (n) domains
lands (1) ‚Üí (n) expressions
expressions (1) ‚Üí (n) media
expressions (1) ‚Üí (n) paragraphs
```

## üß™ Tests Rapides


### Authentification
```bash

TOKEN=$(docker compose exec mywebintelligenceapi curl -s -X POST "http://localhost:8000/api/v1/auth/login" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=admin@example.com&password=changethispassword" \
  | jq -r '.access_token')

echo "Token JWT : $TOKEN"
export TOKEN


```

### Pr√©parer un projet de test "giletsjaunes"
```bash
# 1. Cr√©er le land (le slash final √©vite la redirection 307)
LAND_ID=$(
  curl -s -X POST "http://localhost:8000/api/v2/lands/" \
    -H "Authorization: Bearer $TOKEN" \
    -H "Content-Type: application/json" \
    -d '{"name":"Lecornu202531","description":"testprojet gilets jaunes","words":["lecornu"]}' \
  | jq -r '.id // empty'
)

if [ -z "$LAND_ID" ]; then
  echo "‚ùå Impossible de cr√©er le land. V√©rifiez les logs ou les permissions."
else
  echo "‚úÖ Land giletsjaunes cr√©√© : LAND_ID=${LAND_ID}"

  # 2. Ajouter la liste d‚ÄôURLs de test (conversion du fichier texte en JSON)
  curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/urls" \
    -H "Authorization: Bearer $TOKEN" \
    -H "Content-Type: application/json" \
    --data "$(jq -Rs '{urls: split("\n") | map(select(length>0))}' MyWebIntelligenceAPI/scripts/data/lecornu.txt)"
fi



```

### 1. Lister les Lands
```bash
curl -X GET "http://localhost:8000/api/v2/lands/?page=1&page_size=20" \
  -H "Authorization: Bearer $TOKEN"
```

### 2. D√©tails d'un Land
```bash
curl -X GET "http://localhost:8000/api/v2/lands/${LAND_ID}" \
  -H "Authorization: Bearer $TOKEN"
```

### 3. Statistiques d'un Land
```bash
curl -X GET "http://localhost:8000/api/v2/lands/${LAND_ID}/stats" \
  -H "Authorization: Bearer $TOKEN"
```

### 4. Lancer un Crawl avec Analyse Media
```bash
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"analyze_media": true, "limit": 25, "llm_validation": true}'
```

### 5. Analyser les M√©dias (ASYNC)
```bash
# Analyser TOUS les m√©dias (toutes profondeurs, toute pertinence)
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"depth": 0, "minrel": 3.0}'

# Analyser uniquement les m√©dias des URLs de d√©part (depth=0)
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"depth": 0, "minrel": 0.0}'

# Analyser avec filtre de pertinence (expressions tr√®s pertinentes seulement)
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"minrel": 3.0}'

# TEST RAPIDE - Analyser seulement les plus pertinents (recommand√©)
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"minrel": 3.0}'
```

### 6. Pipeline Readable - Extraction de Contenu (‚úÖ FONCTIONNEL)
```bash
# üéØ TEST DIRECT CELERY (recommand√© pour v√©rifier que √ßa marche)
docker-compose exec celery_worker python -c "
from app.tasks.readable_working_task import readable_working_task
result = readable_working_task.delay(land_id=8, job_id=999, limit=2)
print(f'Task ID: {result.id}')
import time; time.sleep(15)
print(f'Result: {result.get()}')"

# üìã PIPELINE COMPLET (via API - maintenant fonctionnel!)
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/readable" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"limit": 2}' \
  --max-time 60

# üîß LOGS EN TEMPS R√âEL
docker-compose logs celery_worker --tail=20 -f

# ‚úÖ R√âSULTATS ATTENDUS:
# - Content extraction avec Trafilatura + fallback Archive.org
# - Processing r√©ussi avec logs d√©taill√©s
# - Extraction de 3000+ caract√®res de contenu readable
# - Dur√©e: 15-20 secondes pour 2 URLs
# - Status: completed avec statistiques d√©taill√©es

# üìä STATISTIQUES TYPIQUES:
# - processed: 2 URLs
# - successful: 1-2 selon le contenu disponible  
# - errors: 0-1 (example.com peut √©chouer)
# - readable_length: 3000+ caract√®res extraits
# - source: "archive_org" (fallback utilis√©)
```

## üêõ D√©buggage Fr√©quent

### Erreurs SQL
- **Probl√®me:** `should be explicitly declared as text()`
- **Solution:** Ajouter `from sqlalchemy import text` et wrapper les requ√™tes avec `text()`

### Erreurs de Mod√®le
- **Probl√®me:** `'Land' object has no attribute 'user_id'`
- **Solution:** Utiliser `owner_id` au lieu de `user_id`

### Timeouts & Freeze
- **Probl√®me:** L'analyse media timeout ou freeze (pas de r√©ponse)
- **Cause:** Traitement lourd des images (PIL/sklearn) sur trop de m√©dias
- **Solutions √©prouv√©es:** 
  - **PRIORIT√â 1:** Utiliser `minrel: 3.0` ou plus pour r√©duire drastiquement le nombre d'expressions
  - Utiliser `depth: 0` pour analyser seulement les URLs initiales
  - Ajouter `--max-time 120` √† curl pour √©viter les timeouts infinis
  - V√©rifier les logs Celery: `docker logs mywebintelligenceapi_celery_1`
  - Red√©marrer le worker: `docker restart mywebintelligenceapi_celery_1`

### R√©sultats d'Analyse Typiques
- **Expressions totales:** ~1000+
- **Avec minrel=3.0:** ~2-5 expressions (filtrage tr√®s efficace)
- **M√©dias analys√©s:** 7-50 selon la pertinence
- **√âchecs fr√©quents:** Images 404, SVG non support√©s, URLs de tracking
- **Temps de traitement:** 60-120s avec filtrage, timeout sans filtrage

### üö® Probl√®me URLs de M√©dias Incorrectes
- **Probl√®me majeur:** URLs de m√©dias g√©n√©r√©es incorrectement durant le crawl
- **Causes principales:**
  - Proxies WordPress (i0.wp.com, i1.wp.com) qui ne fonctionnent plus
  - URLs relatives mal r√©solues
  - Manque de validation d'URLs
  - Attributs alternatifs (srcset, data-src) ignor√©s
- **Solution:** Nouveau syst√®me de nettoyage d'URLs dans MediaProcessor
- **Impact:** R√©duction drastique des erreurs 404 lors de l'analyse

### üö® PROBL√àME CRITIQUE : Dictionary Starvation - R√âSOLU ‚úÖ
- **Cause racine:** Lands cr√©√©s sans dictionnaires de mots-cl√©s peupl√©s
- **Sympt√¥mes:** 
  - Toutes les expressions ont pertinence = 0
  - Crawler suit tous les liens sans discrimination
  - Explosion du nombre d'expressions (1831 pour 10 URLs)
- **Impact:** Syst√®me de pertinence compl√®tement cass√©
- **Solution impl√©ment√©e:** 
  - Auto-population des dictionnaires lors de la cr√©ation des lands (`crud_land.py`)
  - Service `DictionaryService` pour g√©rer les dictionnaires
  - Endpoints `/populate-dictionary` et `/dictionary-stats` pour diagnostiquer
- **Code impact√©:** `text_processing.expression_relevance()` retourne 0 si dictionnaire vide

### üîß Pipeline Legacy - IMPL√âMENT√â ‚úÖ
**Logique des dates respect√©e:**
1. `created_at`: Date de cr√©ation de l'expression
2. `crawled_at`: Date de r√©cup√©ration du contenu (fetched_at)  
3. `approved_at`: Date d'approbation automatique si `relevance > 0`

**Endpoints de diagnostic du pipeline:**
- `GET /api/v2/lands/{id}/pipeline-stats` - Statistiques compl√®tes du pipeline
- `POST /api/v2/lands/{id}/fix-pipeline` - R√©pare les incoh√©rences de dates

### Container Docker
- **Probl√®me:** Changements de code non pris en compte
- **Solution:** `docker restart mywebintelligenceapi`

### Logs d'Analyse M√©dia
- **IMPORTANT:** L'analyse m√©dia est **ASYNCHRONE** (avec Celery)
- **Logs √† surveiller:** `docker logs mywebclient-celery_worker-1 -f`
- **Signaux d'activit√©:** 
  - `sklearn.base.py:1152: ConvergenceWarning` = Clustering couleurs dominantes
  - `PIL/Image.py:975: UserWarning` = Traitement d'images avec transparence
  - Task termin√© avec succ√®s dans les logs Celery

## üìä Donn√©es de Test

### Land 36 "giletsjaunes"
- **ID:** 36
- **URLs:** Blogs gilets jaunes (over-blog.com, etc.)
- **Contenu:** Articles sur le mouvement
- **M√©dias:** ~850 images selon les stats mock√©es

### Autres Lands
- **37, 38, 39:** Lands de test avec URLs example.com
- **40:** Land basique avec example.org
- **41:** Autre land gilets jaunes

## üîß Technologies

### Backend
- **FastAPI** - Framework web moderne avec validation automatique
- **SQLAlchemy 2.0** - ORM async avec mod√®les d√©claratifs
- **PostgreSQL 15+** - Base de donn√©es relationnelle
- **Celery** - T√¢ches asynchrones distribu√©es 
- **Redis** - Broker Celery et cache

### Analyse Media
- **PIL/Pillow** - Traitement d'images (dimensions, format, EXIF)
- **OpenCV** - Vision par ordinateur avanc√©e
- **scikit-learn** - Machine learning (clustering couleurs dominantes)
- **httpx** - Client HTTP asynchrone pour t√©l√©chargement

### Architecture Async
- **AsyncSession** - Connexions DB non-bloquantes
- **async/await** - Gestion asynchrone des t√¢ches lourdes
- **WebSocket** - Suivi temps r√©el des jobs

### Containerisation
- **Docker Compose** - Orchestration multi-services
- **Services:** API, Celery Worker, PostgreSQL, Redis, Flower (monitoring)
- **Volumes persistants** - Donn√©es et logs
- **Network isolation** - S√©curit√© inter-services

### Installation & Configuration
```bash
# Installation compl√®te avec Docker
git clone <repository-url>
cd MyWebIntelligenceAPI
cp .env.example .env           # Configurer DATABASE_URL, REDIS_URL, SECRET_KEY
docker-compose up -d           # Lancer tous les services
docker-compose exec api alembic upgrade head    # Migrations DB

# Acc√®s services
# API: http://localhost:8000
# Docs: http://localhost:8000/docs  
# Flower: http://localhost:5555
# Prometheus: http://localhost:9090
# Grafana: http://localhost:3001
```

### Variables d'Environnement Critiques
```bash
# Base de donn√©es
DATABASE_URL=postgresql://user:pass@postgres:5432/mywebintelligence

# Cache/Queue  
REDIS_URL=redis://redis:6379

# S√©curit√©
SECRET_KEY=<g√©n√©rer-cl√©-al√©atoire-64-chars>
FIRST_SUPERUSER_EMAIL=admin@example.com
FIRST_SUPERUSER_PASSWORD=changethispassword

# API Externe (optionnel)
OPENROUTER_API_KEY=<pour-analyse-s√©mantique>
```

---

## üöÄ TEST RAPIDE COMPLET - SCRIPT AUTOMATIS√â

### üî• Script de Test Robuste Anti-Erreurs (2 minutes)

```bash
#!/bin/bash
# Test complet crawl + analyse m√©dia ASYNC Celery - AGENTS.md
# Version robuste qui √©vite les pi√®ges courants

# Fonction pour renouveler le token (expire rapidement)
get_fresh_token() {
    TOKEN=$(curl -s -X POST "http://localhost:8000/api/v1/auth/login" \
      -H "Content-Type: application/x-www-form-urlencoded" \
      -d "username=admin@example.com&password=changeme" | jq -r .access_token)
    if [ "$TOKEN" = "null" ] || [ -z "$TOKEN" ]; then
        echo "‚ùå √âchec authentification"
        exit 1
    fi
}

echo "üîß 1/7 - V√©rification serveur..."
if ! curl -s -w "%{http_code}" "http://localhost:8000/" -o /dev/null | grep -q "200"; then
    echo "‚ùå Serveur API non accessible. Lancez: docker compose up -d"
    exit 1
fi

echo "üîë 2/7 - Authentification..."
get_fresh_token
echo "‚úÖ Token obtenu: ${TOKEN:0:20}..."

echo "üèóÔ∏è 3/7 - Cr√©ation land avec URLs int√©gr√©es..."
# ASTUCE: URLs directement dans start_urls (l'endpoint /urls est bugu√©)
LAND_ID=$(curl -s -X POST "http://localhost:8000/api/v2/lands/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"name":"test_complet_media","description":"Test crawl + analyse m√©dia","start_urls":["https://www.lemonde.fr/politique/article/2025/10/11/emmanuel-macron-maintient-sebastien-lecornu-a-matignon-malgre-l-hostilite-de-l-ensemble-de-la-classe-politique_6645724_823448.html"]}' | jq -r '.id')

if [ "$LAND_ID" = "null" ] || [ -z "$LAND_ID" ]; then
    echo "‚ùå √âchec cr√©ation land"
    exit 1
fi
echo "‚úÖ Land cr√©√©: LAND_ID=$LAND_ID"

echo "üìù 4/7 - Ajout mots-cl√©s (OBLIGATOIRE pour pertinence)..."
get_fresh_token  # Token peut expirer
curl -s -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/terms" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"terms": ["lecornu", "sebastien", "macron", "matignon"]}' > /dev/null

echo "üï∑Ô∏è 5/7 - Lancement crawl..."
get_fresh_token  # Renouveler avant chaque action importante
CRAWL_RESULT=$(curl -s -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"limit": 3}' --max-time 60)

JOB_ID=$(echo "$CRAWL_RESULT" | jq -r '.job_id')
if [ "$JOB_ID" = "null" ] || [ -z "$JOB_ID" ]; then
    echo "‚ùå √âchec crawl: $CRAWL_RESULT"
    exit 1
fi
echo "‚úÖ Crawl lanc√©: JOB_ID=$JOB_ID"

echo "‚è≥ 6/7 - Attente crawl (45s)..."
sleep 45

echo "üé® 7/8 - Test analyse m√©dia ASYNC avec Celery..."
get_fresh_token  # Token frais pour derni√®re √©tape
ASYNC_RESULT=$(curl -s -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"depth": 0, "minrel": 0.0}')

ASYNC_JOB_ID=$(echo "$ASYNC_RESULT" | jq -r '.job_id')
CELERY_TASK_ID=$(echo "$ASYNC_RESULT" | jq -r '.celery_task_id')

if [ "$ASYNC_JOB_ID" = "null" ]; then
    echo "‚ùå √âchec analyse async: $ASYNC_RESULT"
    exit 1
fi

echo "‚úÖ Analyse m√©dia ASYNC lanc√©e:"
echo "  - Job ID: $ASYNC_JOB_ID"
echo "  - Celery Task: $CELERY_TASK_ID"

echo "üìñ 8/8 - Test pipeline Readable (NOUVEAU)..."
get_fresh_token  # Token frais pour derni√®re √©tape
READABLE_RESULT=$(curl -s -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/readable" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"limit": 5, "depth": 1, "merge_strategy": "smart_merge"}' \
  --max-time 120)

READABLE_JOB_ID=$(echo "$READABLE_RESULT" | jq -r '.job_id')
READABLE_TASK_ID=$(echo "$READABLE_RESULT" | jq -r '.celery_task_id')

if [ "$READABLE_JOB_ID" = "null" ]; then
    echo "‚ùå √âchec pipeline readable: $READABLE_RESULT"
    # Ne pas exit ici, c'est un test de la nouvelle fonctionnalit√©
else
    echo "‚úÖ Pipeline Readable lanc√©:"
    echo "  - Job ID: $READABLE_JOB_ID"
    echo "  - Celery Task: $READABLE_TASK_ID"
fi

echo ""
echo "üìã SUIVI LOGS CELERY (20s):"
echo "docker logs mywebclient-celery_worker-1 --tail=10 -f"
echo ""
docker logs mywebclient-celery_worker-1 --tail=10 -f &
TAIL_PID=$!
sleep 20
kill $TAIL_PID 2>/dev/null

echo ""
echo "üéØ R√âSUM√â FINAL:"
echo "- Land ID: $LAND_ID"
echo "- Crawl Job: $JOB_ID"
echo "- Media Analysis Job: $ASYNC_JOB_ID" 
echo "- Readable Processing Job: $READABLE_JOB_ID"
echo "- Celery Tasks: $CELERY_TASK_ID, $READABLE_TASK_ID"
echo ""
echo "üîç Commandes utiles:"
echo "# Statut job: curl -H \"Authorization: Bearer \$TOKEN\" \"http://localhost:8000/api/v2/jobs/${ASYNC_JOB_ID}\""
echo "# Statut readable: curl -H \"Authorization: Bearer \$TOKEN\" \"http://localhost:8000/api/v2/jobs/${READABLE_JOB_ID}\""
echo "# Stats land: curl -H \"Authorization: Bearer \$TOKEN\" \"http://localhost:8000/api/v2/lands/${LAND_ID}/stats\""
echo "# Logs Celery: docker logs mywebclient-celery_worker-1 --tail=20 -f"
```

### Utilisation Rapide
```bash
# Copier-coller et ex√©cuter :
curl -s https://raw.githubusercontent.com/MyWebIntelligence/scripts/test-complet.sh | bash

# OU cr√©er le fichier localement :
# 1. Copier le script ci-dessus dans test-crawl.sh
# 2. chmod +x test-crawl.sh && ./test-crawl.sh
```

## üêõ CORRECTIONS CRITIQUES AGENTS - Le√ßons Apprises

### ‚ùå **Erreurs Fr√©quentes √† √âviter**

#### 1. **Bug job_id** (R√âSOLU)
- **Probl√®me** : `job_id should be a valid integer [input_value=None]`
- **Cause** : `/app/api/v2/endpoints/lands_v2.py:582` cherchait `"id"` au lieu de `"job_id"`
- **Fix** : `job_payload.get("id")` ‚Üí `job_payload.get("job_id")`

#### 2. **Tokens JWT Expirent Rapidement** ‚ö†Ô∏è
- **Probl√®me** : `Could not validate credentials` apr√®s quelques minutes
- **Solution** : Fonction `get_fresh_token()` avant chaque appel critique
- **Astuce** : Renouveler syst√©matiquement avant crawl/analyse

#### 3. **Endpoint `/urls` Bugu√©** ‚ö†Ô∏è
- **Probl√®me** : Impossible d'ajouter URLs apr√®s cr√©ation land
- **Solution** : URLs directement dans `start_urls` lors de cr√©ation
- **√âviter** : `POST /api/v2/lands/{id}/urls`

#### 4. **Analyse M√©dia : Utiliser UNIQUEMENT l'Endpoint ASYNC** ‚ö†Ô∏è
- **UTILISER** : `/media-analysis-async` (asynchrone, recommand√©, logs Celery visibles)
- **NE PAS UTILISER** : `/media-analysis` (synchrone, d√©pr√©ci√©, peut timeout)
- **Logs Celery** : `docker logs mywebclient-celery_worker-1 --tail=20 -f`

#### 5. **Mots-cl√©s Obligatoires** ‚ö†Ô∏è
- **Probl√®me** : Sans mots-cl√©s, `relevance=0` pour toutes expressions
- **Solution** : Toujours ajouter termes via `/terms` apr√®s cr√©ation land
- **Impact** : D√©termine filtrage pertinence dans analyse m√©dia

#### 6. **DEPTH = Niveau de Crawl** üî• **CRITIQUE**
- **`depth: 0`** = Analyser m√©dias des **start_urls** seulement
- **`depth: 1`** = Analyser m√©dias des **liens directs** depuis start_urls
- **`depth: 2`** = Analyser m√©dias des **liens de liens** (2e niveau)
- **`depth: 999`** = Analyser **TOUS** les m√©dias sans limite de profondeur
- **‚ö†Ô∏è BUG ENDPOINT** : L'endpoint `/media-analysis-async` ignore le param√®tre `depth` et force toujours `depth: 999`

### üéØ **Workflow Anti-Erreurs**

```bash
1. ‚úÖ Cr√©er land avec start_urls int√©gr√©es (pas d'endpoint /urls)
2. ‚úÖ Ajouter termes OBLIGATOIREMENT  
3. ‚úÖ Renouveler token avant chaque action
4. ‚úÖ Utiliser /media-analysis-async (pas /media-analysis)
5. ‚úÖ Suivre logs Celery pour v√©rifier traitement
6. ‚úÖ Attendre suffisamment (45s crawl, 20s+ analyse)
```

---

## üß™ Tests Manuels D√©taill√©s

### Test 1 : Authentification
```bash
TOKEN=$(curl -s -X POST "http://localhost:8000/api/v1/auth/login" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=admin@example.com&password=changeme" | jq -r .access_token)
echo "Token: $TOKEN"
```

### Test 2 : Cr√©ation Land Compl√®te
```bash
# Land avec URLs int√©gr√©es (√©vite l'endpoint /urls bugu√©)
LAND_ID=$(curl -s -X POST "http://localhost:8000/api/v2/lands/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "test_fonctionnel", 
    "description": "Test crawl fonctionnel",
    "start_urls": ["https://httpbin.org/html", "https://example.com"],
    "words": ["test", "example"]
  }' | jq -r '.id')
echo "Land cr√©√©: $LAND_ID"
```

### Test 3 : Ajout Mots-cl√©s (Obligatoire pour pertinence)
```bash
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/terms" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"terms": ["html", "test", "example", "title"]}'
```

### Test 4 : Crawl Simple
```bash
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"limit": 3}' --max-time 120
```

### Test 5 : Analyse M√©dia (ASYNC)
```bash
# Analyse rapide (expressions tr√®s pertinentes seulement)
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"depth": 0, "minrel": 3.0}'

# Analyse compl√®te (toutes expressions)  
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"minrel": 0.0}'
```

## üß≠ Sc√©nario d'Usage Complet

### Script de Test Automatis√©
Le script `scripts/land_scenario.py` reproduit l'ancien workflow CLI :

```bash
python scripts/land_scenario.py \
  --land-name "MyResearchTopic" \
  --terms "keyword1,keyword2" \
  --urls "https://example.org,https://example.com" \
  --crawl-limit 25
```

**Variables d'environnement:**
- `MYWI_BASE_URL` (d√©faut: `http://localhost:8000`)  
- `MYWI_USERNAME` / `MYWI_PASSWORD` (d√©faut: `admin@example.com` / `changeme`)

### Migration depuis SQLite
```bash
# Migration d'une base SQLite existante
python scripts/migrate_sqlite_to_postgres.py --source /path/to/mwi.db
```
