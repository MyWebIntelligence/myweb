# Corrections ParitÃ© Legacy â€“ MÃ©tadonnÃ©es & HTML

**DerniÃ¨re mise Ã  jour**: 14 octobre 2025 (revue 18 octobre 2025)  
**PÃ©rimÃ¨tre**: Garantir que la pipeline de crawl aligne l'API sur le comportement legacy en stockant systÃ©matiquement les mÃ©tadonnÃ©es (title, description, keywords, lang) et le HTML complet (`content`).

> Pour le rÃ©capitulatif produit, voir `RÃ‰SUMÃ‰_CORRECTIONS_17OCT2025.md`. Les dÃ©cisions d'architecture globales sont dÃ©crites dans `TRANSFERT_API_CRAWL.md` et `CHAÃNE_FALLBACKS.md`.

---

## ğŸ¯ ProblÃ¨mes IdentifiÃ©s

### 1. **MÃ©tadonnÃ©es manquantes**
Les mÃ©tadonnÃ©es (title, description, keywords) n'Ã©taient pas systÃ©matiquement extraites depuis le HTML lors du crawl, mÃªme si Trafilatura les extrait.

### 2. **Champ `content` (HTML brut) non sauvegardÃ©**
Le champ `content` destinÃ© Ã  contenir le HTML complet de la page Ã©tait toujours `NULL` en base de donnÃ©es, alors que le code prÃ©voyait de le sauvegarder.

---

## âœ… Corrections ApportÃ©es

### 1. Extraction SystÃ©matique des MÃ©tadonnÃ©es

**Fichier modifiÃ©**: `MyWebIntelligenceAPI/app/core/content_extractor.py`

Ajout de l'extraction des mÃ©tadonnÃ©es Ã  **tous les niveaux** de la chaÃ®ne de fallback:

#### a) Trafilatura Direct (lignes 272-287)
```python
# Extract metadata robustly from soup
metadata = get_metadata(soup, url)

return {
    'readable': enriched_content,
    'content': raw_html,
    'soup': soup,
    'readable_html': readable_html,
    'extraction_source': 'trafilatura_direct',
    'media_list': media_list,
    'links': links,
    'title': metadata.get('title'),           # âœ… NOUVEAU
    'description': metadata.get('description'), # âœ… NOUVEAU
    'keywords': metadata.get('keywords'),       # âœ… NOUVEAU
    'language': metadata.get('lang')            # âœ… NOUVEAU
}
```

#### b) Archive.org Fallback (lignes 406-421)
```python
soup = BeautifulSoup(archived_html, 'html.parser')
metadata = get_metadata(soup, url)  # âœ… NOUVEAU

return {
    'readable': enriched_content,
    'content': archived_html,
    # ... autres champs ...
    'title': metadata.get('title'),
    'description': metadata.get('description'),
    'keywords': metadata.get('keywords'),
    'language': metadata.get('lang')
}
```

#### c) BeautifulSoup Smart Extraction (lignes 299-317)
```python
# Extract metadata before any modifications
metadata = get_metadata(soup, url)  # âœ… NOUVEAU

# Try smart extraction first
smart_content = _smart_content_extraction(soup)
if smart_content and len(smart_content) > 100:
    return {
        # ... autres champs ...
        'title': metadata.get('title'),
        'description': metadata.get('description'),
        'keywords': metadata.get('keywords'),
        'language': metadata.get('lang')
    }
```

#### d) BeautifulSoup Basic Fallback (lignes 319-335)
MÃªme principe que smart extraction.

#### e) All Methods Failed (lignes 337-351)
MÃªme en cas d'Ã©chec total, on tente d'extraire les mÃ©tadonnÃ©es si le soup est disponible.

---

### 2. Sauvegarde du HTML Complet dans le Champ `content`

**ProblÃ¨me**: Le crawler synchrone (`crawler_engine_sync.py`) utilisÃ© par Celery n'extrayait PAS le champ `content` depuis `extraction_result`.

**Fichier modifiÃ©**: `MyWebIntelligenceAPI/app/core/crawler_engine_sync.py`

**Ajout lignes 192-197**:
```python
# Store raw HTML content (legacy field) - ALIGNED WITH ASYNC CRAWLER
if extraction_result.get('content'):
    update_data["content"] = extraction_result['content']
    logger.debug(f"Storing HTML content for {expr_url}: {len(extraction_result['content'])} chars")
else:
    logger.warning(f"No HTML content in extraction_result for {expr_url}")
```

**MÃªme modification appliquÃ©e** dans `crawler_engine.py` (lignes 165-170) pour cohÃ©rence.

---

## ğŸ§ª Validation

### Test EffectuÃ©

```bash
./test_content_field.sh
```

### RÃ©sultats

**Expression 11543** (https://www.francebleu.fr/...):
```
âœ… Title: "Crise politique : 'Si les conditions n'Ã©taient plus remplies...'"
âœ… Description: "Tard vendredi soir, SÃ©bastien Lecornu a Ã©tÃ©..."
âœ… Keywords: "AssemblÃ©e nationale,Emmanuel Macron,Gouvernement,SÃ©bastien Lecornu"
âœ… Content (HTML): 194,976 caractÃ¨res
âœ… Readable: 19,982 caractÃ¨res
```

**Expression 11539** (https://www.info.gouv.fr/...):
```
âœ… Content (HTML): 7,210 caractÃ¨res
âš ï¸  MÃ©tadonnÃ©es: Absentes (le site ne les fournit pas)
```

---

## ğŸ“Š Fonction `get_metadata()`

Cette fonction utilise une **chaÃ®ne de fallbacks robuste** pour extraire les mÃ©tadonnÃ©es:

```python
def get_metadata(soup: BeautifulSoup, url: str) -> Dict[str, Any]:
    title = get_title(soup) or url
    description = get_description(soup)
    keywords = get_keywords(soup)
    lang = soup.html.get('lang', '') if soup.html else ''

    return {
        'title': title,
        'description': description,
        'keywords': keywords,
        'lang': lang
    }
```

### Ordre de PrioritÃ© pour `title`:
1. OpenGraph: `<meta property="og:title">`
2. Twitter Card: `<meta name="twitter:title">`
3. Tag HTML: `<title>`
4. Fallback: URL

### Ordre de PrioritÃ© pour `description`:
1. OpenGraph: `<meta property="og:description">`
2. Twitter Card: `<meta name="twitter:description">`
3. Meta standard: `<meta name="description">`

### Keywords:
- Extrait depuis: `<meta name="keywords">`

---

## ğŸ“ Fichiers ModifiÃ©s

```
MyWebIntelligenceAPI/
â”œâ”€â”€ app/core/
â”‚   â”œâ”€â”€ content_extractor.py      (+52 lignes, extraction metadata Ã  tous niveaux)
â”‚   â”œâ”€â”€ crawler_engine.py          (+6 lignes, logs pour debugging)
â”‚   â””â”€â”€ crawler_engine_sync.py     (+6 lignes, sauvegarde du champ content)
â””â”€â”€ test_content_field.sh          (nouveau script de validation)
```

---

## ğŸ¯ Impact

### Avant
- MÃ©tadonnÃ©es: **Souvent manquantes** ou incomplÃ¨tes
- Champ `content`: **Toujours NULL**
- Impossible de rÃ©analyser le HTML brut aprÃ¨s crawl

### AprÃ¨s
- MÃ©tadonnÃ©es: **SystÃ©matiquement extraites** (si disponibles sur la page)
- Champ `content`: **Toujours rempli** avec le HTML complet de la page
- PossibilitÃ© de retraiter le HTML sans re-crawler

---

## ğŸš€ DÃ©ploiement

1. âœ… Code modifiÃ© et testÃ©
2. âœ… Validation sur expressions rÃ©elles (land 21)
3. â³ Ã€ tester: Impact sur performance avec gros volumes
4. â³ Ã€ vÃ©rifier: Taille du champ `content` en base de donnÃ©es (peut Ãªtre volumineux)

---

## ğŸ“ Notes Techniques

### Stockage du HTML Complet

Le champ `content` est de type `Text` dans PostgreSQL, ce qui peut stocker jusqu'Ã  ~1 GB par entrÃ©e. Pour une page web moyenne de 100-500 KB:

- **Avantages**:
  - RÃ©analyse possible sans re-crawl
  - TraÃ§abilitÃ© complÃ¨te du contenu original
  - PossibilitÃ© d'extraction diffÃ©rÃ©e de nouvelles donnÃ©es

- **InconvÃ©nients**:
  - Augmentation significative de la taille de la base de donnÃ©es
  - Backup plus longs

**Recommandation**: Monitorer la croissance de la base et envisager une compression ou un archivage pÃ©riodique si nÃ©cessaire.

---

**ValidÃ© par**: Tests automatiques et vÃ©rification manuelle en base de donnÃ©es
**Status**: âœ… **PRODUCTION READY**

---

## âš ï¸ LeÃ§on Apprise : Double Crawler

**Ce bug a rÃ©vÃ©lÃ© un problÃ¨me architectural fondamental** : le systÃ¨me utilise deux crawlers diffÃ©rents (async et sync) qui doivent Ãªtre synchronisÃ©s manuellement.

**ğŸ“– Documentation dÃ©taillÃ©e** : [ERREUR_DOUBLE_CRAWLER.md](ERREUR_DOUBLE_CRAWLER.md)

**Checklist pour Ã©viter ce bug Ã  l'avenir** :

- [ ] Modifier `crawler_engine.py` (async)
- [ ] Modifier `crawler_engine_sync.py` (sync)
- [ ] Tester avec Celery (pas seulement l'API)
- [ ] VÃ©rifier en base de donnÃ©es
