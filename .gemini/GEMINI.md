# MyWebIntelligence API - Guide Complet pour Agents

MyWebIntelligence est une API FastAPI encapsulant les fonctionnalit√©s du crawler MyWebIntelligencePython. Elle permet l'int√©gration avec MyWebClient et ouvre la voie √† un d√©ploiement SaaS scalable.

## üìö Documentation active

- [INDEX_DOCUMENTATION.md](INDEX_DOCUMENTATION.md) ‚Äî plan de lecture consolid√© & statuts
- [R√âSUM√â_CORRECTIONS_17OCT2025.md](R√âSUM√â_CORRECTIONS_17OCT2025.md) ‚Äî synth√®se d√©cisionnelle
- [TRANSFERT_API_CRAWL.md](TRANSFERT_API_CRAWL.md) ‚Äî audit Legacy ‚Üí API
- [CORRECTIONS_PARIT√â_LEGACY.md](CORRECTIONS_PARIT√â_LEGACY.md) ‚Äî d√©tails techniques (m√©tadonn√©es & HTML)
- [Transfert_readable.md](Transfert_readable.md) ‚Äî suivi du pipeline readable
- [CHA√éNE_FALLBACKS.md](CHA√éNE_FALLBACKS.md) ‚Äî sch√©ma d'extraction
- [METADATA_FIXES.md](METADATA_FIXES.md) ‚Äî journal d√©taill√© des corrections m√©tadonn√©es
- [CORRECTIONS_FINALES.md](CORRECTIONS_FINALES.md) ‚Äî synth√®se + checklist de validation
- [compare_addterms_analysis.md](compare_addterms_analysis.md) ‚Äî analyse AddTerms & pertinence
- [AGENTS.md](AGENTS.md) ‚Äî checklists incidents (double crawler, init DB, dictionnaire)

## üéØ Concepts Cl√©s

### Qu'est-ce qu'un "Land" ?
Un **Land** est un projet de crawling/recherche qui contient :
- **URLs de d√©part** (`start_urls`) : Points d'entr√©e du crawl
- **Mots-cl√©s** (`words`) : Termes √† rechercher avec leurs lemmes
- **Configuration** : Langues support√©es, limites, etc.
- **R√©sultats** : Expressions (pages) et m√©dias d√©couverts

### Qu'est-ce qu'une "Expression" ?
Une **Expression** est une page web crawl√©e qui contient :
- **URL** et **contenu** de la page
- **Profondeur** (`depth`) : Distance depuis les URLs de d√©part (0 = URL initiale, 1 = lien direct, etc.)
- **Pertinence** (`relevance`) : Score de correspondance avec les mots-cl√©s
- **M√©dias** associ√©s (images, vid√©os, audio)

### Workflow Typique
1. **Cr√©er un Land** avec URLs de d√©part et mots-cl√©s
2. **Lancer le crawl** ‚Üí D√©couverte d'expressions et m√©dias
3. **Pipeline Readable** ‚Üí Extraction de contenu lisible (Mercury-like)
4. **Analyser les m√©dias** ‚Üí Extraction de m√©tadonn√©es (couleurs, dimensions, EXIF)
5. **Exporter les donn√©es** ‚Üí CSV, GEXF, JSON

## üöÄ D√©marrage Rapide

### Authentification
```bash
# Option 1‚ÄØ: depuis l‚Äôh√¥te
TOKEN=$(curl -s -X POST "http://localhost:8000/api/v1/auth/login" -H "Content-Type: application/x-www-form-urlencoded" -d "username=admin@example.com&password=changeme" | jq -r .access_token)

# Option 2‚ÄØ: via Docker Compose (container ¬´‚ÄØapi‚ÄØ¬ª)
TOKEN=$(docker compose exec api curl -s -X POST "http://localhost:8000/api/v1/auth/login" -H "Content-Type: application/x-www-form-urlencoded" -d "username=${MYWI_USERNAME:-admin@example.com}&password=${MYWI_PASSWORD:-changeme}" | jq -r .access_token)

echo "Token JWT: $TOKEN"
```

### Serveur Docker
```bash
# (Re)d√©ployer avec migrations fra√Æchement appliqu√©es
docker compose down -v
docker compose up --build -d

# V√©rifier les containers
docker ps | grep mywebintelligenceapi

# Red√©marrer si n√©cessaire
docker restart mywebintelligenceapi

# Tester que le serveur r√©pond
curl -s -w "%{http_code}" "http://localhost:8000/" -o /dev/null
```

## üèóÔ∏è Architecture de l'API

### Structure des Endpoints

#### API v1 (Legacy)
- `/api/v1/auth/` - Authentification
- `/api/v1/export/` - Export de donn√©es (CSV, GEXF)

#### API v2 (Moderne)
- `/api/v2/lands/` - Gestion des projets (lands)
- `/api/v2/lands/{id}/crawl` - Lancement de crawls
- `/api/v2/lands/{id}/readable` - Pipeline readable (extraction contenu)
- `/api/v2/lands/{id}/media-analysis-async` - Analyse des m√©dias (asynchrone)
- `/api/v2/lands/{id}/stats` - Statistiques

### Mod√®les de Donn√©es

#### Land (Projet)
```python
id: int                    # ID unique
name: str                  # Nom du projet
description: str           # Description
owner_id: int              # Propri√©taire (FK users.id)
lang: List[str]            # Langues ["fr", "en"]
start_urls: List[str]      # URLs de d√©part
crawl_status: str          # "pending", "running", "completed"
total_expressions: int     # Nombre d'expressions
total_domains: int         # Nombre de domaines
words: List[dict]          # Mots-cl√©s avec lemmes
```

#### Expression (Page crawl√©e)
```python
id: int                    # ID unique
land_id: int               # Projet parent
domain_id: int             # Domaine parent
url: str                   # URL de la page
title: str                 # Titre
content: str               # Contenu HTML
readable: str              # Contenu lisible (markdown)
depth: int                 # Profondeur de crawl
relevance: float           # Score de pertinence
language: str              # Langue d√©tect√©e
word_count: int            # Nombre de mots
```

#### Media (Fichiers m√©dia)
```python
id: int                    # ID unique
expression_id: int         # Expression parent
url: str                   # URL du m√©dia
type: str                  # "img", "video", "audio"
is_processed: bool         # Analys√© ou non
width: int                 # Largeur (images)
height: int                # Hauteur (images)
file_size: int             # Taille en bytes
metadata: dict             # M√©tadonn√©es EXIF, etc.
dominant_colors: List[str] # Couleurs dominantes
```

## üÜï Mises √† jour structurelles (juillet 2025)

- **Sch√©ma SQLAlchemy r√©align√©**  
  - `domains` conserve `http_status` et `fetched_at`.  
  - `expressions` stocke d√©sormais `published_at`, `approved_at`, `validllm`, `validmodel`, `seorank`, et la langue d√©tect√©e.  
  - `words` embarque `language` et `frequency`. Une migration Alembic `006_add_legacy_crawl_columns.py` doit √™tre appliqu√©e.

- **Dictionnaire de land**  
  - `DictionaryService.populate_land_dictionary` accepte les seeds (`words`) fournis √† la cr√©ation et cr√©e automatiquement des entr√©es `Word` multilingues.  
  - Les variations g√©n√©r√©es via `get_lemma` remplissent la table `land_dictionaries` sans manipuler directement la relation ORM.

- **Service de crawl**  
  - `start_crawl_for_land` renvoie un `CrawlJobResponse` typ√© (avec `ws_channel`) et convertit les filtres `http_status` en entiers avant insertion.  
  - `CrawlerEngine` persiste la langue, approuve les expressions pertinentes et peuple les nouveaux champs de m√©tadonn√©es.
- **Migrations automatiques**  
  - Les services `api` et `celery_worker` ex√©cutent `alembic upgrade head` √† chaque d√©marrage du conteneur. Reconstruis la stack (`docker compose down -v && docker compose up --build`) apr√®s un pull pour appliquer les derniers sch√©mas.

- **Tests & environnement**  
  - Les tests de crawling n√©cessitent `pytest`, `sqlalchemy`, `aiosqlite` dans le venv.  
  - Sous Python 3.13, certaines wheels (`pydantic-core`, `asyncpg`, `pillow`) √©chouent √† la compilation ; privil√©gier Python 3.11/3.12 ou installer Rust + toolchain compatible.

## üîÑ Pipelines de Traitement

### 1. Pipeline de Crawl
```
Start URLs ‚Üí Crawler ‚Üí Pages ‚Üí Content Extraction ‚Üí Expressions
                          ‚Üì
                     Media Detection ‚Üí Media Records
```

**Endpoint:** `POST /api/v2/lands/{id}/crawl`
```json
{
  "limit": 10,              // Nombre max de pages
  "depth": 2,               // Profondeur max
  "analyze_media": true     // Analyser les m√©dias
}
```

### 2. Pipeline d'Analyse Media
```
Expressions ‚Üí Media URLs ‚Üí Download ‚Üí Analysis ‚Üí Metadata Storage
                               ‚Üì
                         PIL/OpenCV ‚Üí Colors, Dimensions, EXIF
```

**Endpoint:** `POST /api/v2/lands/{id}/media-analysis-async`
```json
{
  "depth": 1,               // Profondeur max des expressions √† analyser (0=URLs initiales, 1=liens directs, etc.)
  "minrel": 0.5             // Score de pertinence minimum des expressions
}
```

**IMPORTANT:** `depth` ne limite PAS le nombre d'unit√©s/m√©dias √† analyser, mais la profondeur des expressions source !
- `depth: 0` = Analyser uniquement les m√©dias des URLs de d√©part
- `depth: 1` = Inclure aussi les m√©dias des pages li√©es directement 
- `depth: 999` = Analyser tous les m√©dias sans limite de profondeur

**STRAT√âGIE de LIMITATION:** Pour limiter le nombre de m√©dias analys√©s, utiliser `minrel` (pertinence) :
- `minrel: 0.0` = Toutes les expressions
- `minrel: 1.0` = Expressions moyennement pertinentes  
- `minrel: 3.0` = Expressions tr√®s pertinentes seulement (recommand√© pour tests)
- `minrel: 5.0` = Expressions extr√™mement pertinentes

### 3. Pipeline Readable (Nouveau)
```
Expressions ‚Üí Content Extraction ‚Üí Readable Content ‚Üí Text Processing ‚Üí Paragraphs
                     ‚Üì
            Mercury/Trafilatura ‚Üí Clean Text ‚Üí LLM Validation ‚Üí Storage
```

**Endpoint:** `POST /api/v2/lands/{id}/readable`
```json
{
  "limit": 50,              // Nombre max d'expressions √† traiter
  "depth": 1,               // Profondeur max des expressions
  "merge_strategy": "smart_merge",  // "mercury_priority", "preserve_existing"
  "enable_llm": false,      // Validation LLM (optionnel)
  "batch_size": 10,         // Expressions par batch
  "max_concurrent": 5       // Batches concurrents
}
```

**IMPORTANT:** Le pipeline readable transforme le contenu HTML brut des expressions en contenu lisible et structur√© :
- **Extraction intelligente** : Utilise Trafilatura puis Mercury fallback
- **Nettoyage** : Supprime le markup, garde le contenu principal
- **Validation LLM** : Optionnelle, am√©liore la qualit√© du contenu
- **Segmentation** : D√©coupe en paragraphes pour l'embedding

### 4. Pipeline LLM Validation (Int√©gr√©) ‚úÖ
```
Expressions ‚Üí OpenRouter API ‚Üí Relevance Check ‚Üí Database Update
                     ‚Üì
              "oui"/"non" ‚Üí valid_llm, valid_model ‚Üí Relevance=0 si non pertinent
```

**Int√©gration dans les pipelines existants :**
- **Crawl avec LLM** : `POST /api/v2/lands/{id}/crawl` avec `"enable_llm": true`
- **Readable avec LLM** : `POST /api/v2/lands/{id}/readable` avec `"enable_llm": true`

**Configuration OpenRouter requise :**
```bash
export OPENROUTER_ENABLED=True
export OPENROUTER_API_KEY=sk-or-v1-your-key-here
export OPENROUTER_MODEL=anthropic/claude-3.5-sonnet
```

**Exemples d'usage :**
```bash
# Crawl avec validation LLM
curl -X POST "http://localhost:8000/api/v2/lands/36/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"limit": 5, "enable_llm": true}'

# Readable avec validation LLM  
curl -X POST "http://localhost:8000/api/v2/lands/36/readable" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"limit": 3, "enable_llm": true}'
```

**R√©sultats stock√©s :**
- `valid_llm` : "oui" (pertinent) ou "non" (non pertinent)
- `valid_model` : Mod√®le utilis√© (ex: "anthropic/claude-3.5-sonnet")
- `relevance` : Mis √† 0 si expression jug√©e non pertinente

### 5. Pipeline d'Export
```
Data Selection ‚Üí Format Conversion ‚Üí Response
     ‚Üì
CSV/GEXF/JSON ‚Üí Compressed ‚Üí Download
```

**Endpoint:** `POST /api/v1/export/direct`
```json
{
  "land_id": 36,
  "export_type": "mediacsv", // "pagecsv", "nodecsv", etc.
  "limit": 100
}
```

## üóÑÔ∏è Base de Donn√©es

### Tables Principales
- `users` - Utilisateurs
- `lands` - Projets de crawl
- `domains` - Domaines web
- `expressions` - Pages crawl√©es
- `media` - Fichiers m√©dia
- `paragraphs` - Contenu segment√©
- `crawl_jobs` - Jobs Celery

### Relations Cl√©s
```
users (1) ‚Üí (n) lands
lands (1) ‚Üí (n) domains
lands (1) ‚Üí (n) expressions
expressions (1) ‚Üí (n) media
expressions (1) ‚Üí (n) paragraphs
```

## üß™ Tests Rapides

### ‚ö†Ô∏è PROBL√àME DE PERSISTENCE DU TOKEN

Le token JWT ne persiste pas car `export TOKEN` dans un **sous-shell Docker** ne remonte pas au shell parent. Voici **3 solutions** :

---

### ‚úÖ **Solution 1 : Script complet dans Docker** (recommand√© pour les tests)

Ex√©cuter tout le workflow dans un seul appel Docker pour que le token reste en m√©moire :

```bash
docker compose exec mywebintelligenceapi bash -c '
  # 1. Authentification
  TOKEN=$(curl -s -X POST "http://localhost:8000/api/v1/auth/login" \
    -H "Content-Type: application/x-www-form-urlencoded" \
    -d "username=admin@example.com&password=changethispassword" \
    | jq -r ".access_token")

  echo "‚úÖ Token obtenu : ${TOKEN:0:20}..."

  # 2. Cr√©er le land
  LAND_ID=$(curl -s -X POST "http://localhost:8000/api/v2/lands/" \
    -H "Authorization: Bearer $TOKEN" \
    -H "Content-Type: application/json" \
    -d "{\"name\":\"Lecornu$(date +%s)\",\"description\":\"Test gilets jaunes\",\"words\":[\"lecornu\"]}" \
    | jq -r ".id // empty")

  if [ -z "$LAND_ID" ]; then
    echo "‚ùå Impossible de cr√©er le land"
    exit 1
  fi

  echo "‚úÖ Land cr√©√© : LAND_ID=${LAND_ID}"

  # 3. Ajouter les URLs
  curl -s -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/urls" \
    -H "Authorization: Bearer $TOKEN" \
    -H "Content-Type: application/json" \
    --data "$(jq -Rs "{urls: split(\"\n\") | map(select(length>0))}" /app/scripts/data/lecornu.txt)" \
    | jq "."

  echo "‚úÖ URLs ajout√©es au land ${LAND_ID}"
'
```

---

### ‚úÖ **Solution 2 : Appels depuis l'h√¥te** (si API accessible sur localhost)

Stocker le token dans un fichier temporaire pour le r√©utiliser :

```bash
# 1. Authentification (sauvegarder le token dans un fichier)
curl -s -X POST "http://localhost:8000/api/v1/auth/login" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=admin@example.com&password=changethispassword" \
  | jq -r '.access_token' > /tmp/mywebintel_token.txt

export TOKEN=$(cat /tmp/mywebintel_token.txt)
echo "‚úÖ Token obtenu : ${TOKEN:0:20}..."

# 2. Cr√©er le land
LAND_ID=$(curl -s -X POST "http://localhost:8000/api/v2/lands/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"name":"Lecornu202535","description":"testprojet gilets jaunes","words":["lecornu"]}' \
  | jq -r '.id // empty')

if [ -z "$LAND_ID" ]; then
  echo "‚ùå Impossible de cr√©er le land. V√©rifiez les logs ou les permissions."
  exit 1
else
  echo "‚úÖ Land cr√©√© : LAND_ID=${LAND_ID}"

  # 3. Ajouter la liste d'URLs
  curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/urls" \
    -H "Authorization: Bearer $TOKEN" \
    -H "Content-Type: application/json" \
    --data "$(jq -Rs '{urls: split("\n") | map(select(length>0))}' MyWebIntelligenceAPI/scripts/data/lecornu.txt)"
fi
```

---

### ‚úÖ **Solution 3 : Session interactive dans le container** (pour tests manuels)

Entrer directement dans le container pour garder le token en m√©moire :

```bash
# 1. Entrer dans le container
docker compose exec mywebintelligenceapi bash

# 2. Dans le container, authentification + export
TOKEN=$(curl -s -X POST "http://localhost:8000/api/v1/auth/login" -H "Content-Type: application/x-www-form-urlencoded" -d "username=admin@example.com&password=changethispassword" | jq -r '.access_token')

export TOKEN
echo "‚úÖ Token : ${TOKEN:0:20}..."

# 3. Maintenant toutes vos commandes curl peuvent utiliser $TOKEN
# Cr√©er le land
LAND_ID=$(curl -s -X POST "http://localhost:8000/api/v2/lands/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"name":"Lecornu202540","description":"Test gilets jaunes","words":["lecornu"]}' \
  | jq -r '.id')

echo "‚úÖ Land cr√©√© : $LAND_ID"

# Ajouter les URLs
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/urls" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  --data "$(jq -Rs '{urls: split("\n") | map(select(length>0))}' /app/scripts/data/lecornu.txt)"

# Pour sortir du container :
# exit
```

### 1. Lister les Lands
```bash
curl -X GET "http://localhost:8000/api/v2/lands/?page=1&page_size=20" \
  -H "Authorization: Bearer $TOKEN"
```

### 2. D√©tails d'un Land
```bash
curl -X GET "http://localhost:8000/api/v2/lands/${LAND_ID}" \
  -H "Authorization: Bearer $TOKEN"
```

### 3. Statistiques d'un Land
```bash
curl -X GET "http://localhost:8000/api/v2/lands/${LAND_ID}/stats" \
  -H "Authorization: Bearer $TOKEN"
```

### 4. Lancer un Crawl avec Analyse Media
```bash
curl -X POST "http://localhost:8000/api/v2/lands/7/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"analyze_media": false, "limit": 25, "llm_validation": false}'
```

### 5. Analyser les M√©dias (ASYNC)
```bash
# Analyser TOUS les m√©dias (toutes profondeurs, toute pertinence)
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"depth": 0, "minrel": 3.0}'

# Analyser uniquement les m√©dias des URLs de d√©part (depth=0)
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"depth": 0, "minrel": 0.0}'

# Analyser avec filtre de pertinence (expressions tr√®s pertinentes seulement)
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"minrel": 3.0}'

# TEST RAPIDE - Analyser seulement les plus pertinents (recommand√©)
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"minrel": 3.0}'
```

### 6. Pipeline Readable - Extraction de Contenu (‚úÖ FONCTIONNEL)
```bash
# üéØ TEST DIRECT CELERY (recommand√© pour v√©rifier que √ßa marche)
docker-compose exec celery_worker python -c "
from app.tasks.readable_working_task import readable_working_task
result = readable_working_task.delay(land_id=1, job_id=999, limit=2)
print(f'Task ID: {result.id}')
import time; time.sleep(15)
print(f'Result: {result.get()}')"

# üìã PIPELINE COMPLET (via API - maintenant fonctionnel!)
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/readable" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"limit": 10}' \
  --max-time 60

# üîß LOGS EN TEMPS R√âEL
docker-compose logs celery_worker --tail=20 -f

# ‚úÖ R√âSULTATS ATTENDUS:
# - Content extraction avec Trafilatura + fallback Archive.org
# - Processing r√©ussi avec logs d√©taill√©s
# - Extraction de 3000+ caract√®res de contenu readable
# - Dur√©e: 15-20 secondes pour 2 URLs
# - Status: completed avec statistiques d√©taill√©es

# üìä STATISTIQUES TYPIQUES:
# - processed: 2 URLs
# - successful: 1-2 selon le contenu disponible  
# - errors: 0-1 (example.com peut √©chouer)
# - readable_length: 3000+ caract√®res extraits
# - source: "archive_org" (fallback utilis√©)
```

## üêõ D√©buggage Fr√©quent

### Erreurs SQL
- **Probl√®me:** `should be explicitly declared as text()`
- **Solution:** Ajouter `from sqlalchemy import text` et wrapper les requ√™tes avec `text()`

### Erreurs de Mod√®le
- **Probl√®me:** `'Land' object has no attribute 'user_id'`
- **Solution:** Utiliser `owner_id` au lieu de `user_id`

### Timeouts & Freeze
- **Probl√®me:** L'analyse media timeout ou freeze (pas de r√©ponse)
- **Cause:** Traitement lourd des images (PIL/sklearn) sur trop de m√©dias
- **Solutions √©prouv√©es:** 
  - **PRIORIT√â 1:** Utiliser `minrel: 3.0` ou plus pour r√©duire drastiquement le nombre d'expressions
  - Utiliser `depth: 0` pour analyser seulement les URLs initiales
  - Ajouter `--max-time 120` √† curl pour √©viter les timeouts infinis
  - V√©rifier les logs Celery: `docker logs mywebintelligenceapi_celery_1`
  - Red√©marrer le worker: `docker restart mywebintelligenceapi_celery_1`

### R√©sultats d'Analyse Typiques
- **Expressions totales:** ~1000+
- **Avec minrel=3.0:** ~2-5 expressions (filtrage tr√®s efficace)
- **M√©dias analys√©s:** 7-50 selon la pertinence
- **√âchecs fr√©quents:** Images 404, SVG non support√©s, URLs de tracking
- **Temps de traitement:** 60-120s avec filtrage, timeout sans filtrage

### üö® Probl√®me URLs de M√©dias Incorrectes
- **Probl√®me majeur:** URLs de m√©dias g√©n√©r√©es incorrectement durant le crawl
- **Causes principales:**
  - Proxies WordPress (i0.wp.com, i1.wp.com) qui ne fonctionnent plus
  - URLs relatives mal r√©solues
  - Manque de validation d'URLs
  - Attributs alternatifs (srcset, data-src) ignor√©s
- **Solution:** Nouveau syst√®me de nettoyage d'URLs dans MediaProcessor
- **Impact:** R√©duction drastique des erreurs 404 lors de l'analyse

### üö® PROBL√àME CRITIQUE : Dictionary Starvation - R√âSOLU ‚úÖ
- **Cause racine:** Lands cr√©√©s sans dictionnaires de mots-cl√©s peupl√©s
- **Sympt√¥mes:** 
  - Toutes les expressions ont pertinence = 0
  - Crawler suit tous les liens sans discrimination
  - Explosion du nombre d'expressions (1831 pour 10 URLs)
- **Impact:** Syst√®me de pertinence compl√®tement cass√©
- **Solution impl√©ment√©e:** 
  - Auto-population des dictionnaires lors de la cr√©ation des lands (`crud_land.py`)
  - Service `DictionaryService` pour g√©rer les dictionnaires
  - Endpoints `/populate-dictionary` et `/dictionary-stats` pour diagnostiquer
- **Code impact√©:** `text_processing.expression_relevance()` retourne 0 si dictionnaire vide

### üîß Pipeline de Crawl - Logique des Timestamps ‚úÖ

**LOGIQUE STRICTE DES DATES (CRITIQUE) :**

1. **`created_at`** ‚Üí Quand l'expression est **ajout√©e en base** (d√©couverte de l'URL)
   - Rempli automatiquement lors de l'INSERT
   - Permet de suivre l'ordre de d√©couverte des URLs

2. **`crawled_at`** ‚Üí Quand le **contenu HTTP a √©t√© r√©cup√©r√©** (fetch HTTP r√©ussi)
   - Rempli apr√®s un GET HTTP r√©ussi avec code HTTP valide
   - NULL si l'URL n'a jamais √©t√© fetch√©e

3. **`approved_at`** ‚Üí Quand le crawler a **trait√© la r√©ponse** (m√™me si erreur)
   - ‚ö†Ô∏è **CRIT√àRE DE S√âLECTION PRINCIPAL** : `approved_at IS NULL` = candidats au crawl
   - Rempli syst√©matiquement apr√®s traitement (succ√®s ou √©chec)
   - Marque l'expression comme "trait√©e par le crawler"

4. **`readable_at`** ‚Üí Quand le **contenu readable** a √©t√© extrait et enregistr√©
   - Rempli apr√®s extraction r√©ussie (Trafilatura/Mercury)
   - NULL si pas encore de contenu lisible

5. **`updated_at`** ‚Üí Quand le contenu **readable a √©t√© modifi√©**
   - Mis √† jour automatiquement √† chaque modification de `readable`
   - Permet de tracker les re-extractions

**HTTP_STATUS - R√àGLE STRICTE :**

- **TOUJOURS** un code HTTP valide (200, 404, 500, etc.)
- **OU** `000` pour erreur inconnue non-HTTP (timeout, DNS, etc.)
- **JAMAIS** NULL apr√®s traitement

**WORKFLOW TYPIQUE :**

```text
1. D√©couverte URL     ‚Üí created_at = NOW, approved_at = NULL, http_status = NULL
2. Fetch HTTP         ‚Üí crawled_at = NOW, http_status = 200 (ou 404, etc.)
3. Traitement r√©ponse ‚Üí approved_at = NOW
4. Extraction readable ‚Üí readable_at = NOW, updated_at = NOW
5. Modification readable ‚Üí updated_at = NOW (updated_at > readable_at)
```

**REQU√äTE DE S√âLECTION DES CANDIDATS AU CRAWL :**

```sql
SELECT * FROM expressions
WHERE land_id = ?
  AND approved_at IS NULL  -- ‚ö†Ô∏è Cl√© principale !
  AND depth <= ?           -- Filtre optionnel de profondeur
ORDER BY depth ASC, created_at ASC
LIMIT ?
```

**Endpoints de diagnostic du pipeline:**
- `GET /api/v2/lands/{id}/pipeline-stats` - Statistiques compl√®tes du pipeline
- `POST /api/v2/lands/{id}/fix-pipeline` - R√©pare les incoh√©rences de dates

### Container Docker
- **Probl√®me:** Changements de code non pris en compte
- **Solution:** `docker restart mywebintelligenceapi`

### Logs d'Analyse M√©dia
- **IMPORTANT:** L'analyse m√©dia est **ASYNCHRONE** (avec Celery)
- **Logs √† surveiller:** `docker logs mywebclient-celery_worker-1 -f`
- **Signaux d'activit√©:** 
  - `sklearn.base.py:1152: ConvergenceWarning` = Clustering couleurs dominantes
  - `PIL/Image.py:975: UserWarning` = Traitement d'images avec transparence
  - Task termin√© avec succ√®s dans les logs Celery

## üìä Donn√©es de Test

### Land 36 "giletsjaunes"
- **ID:** 36
- **URLs:** Blogs gilets jaunes (over-blog.com, etc.)
- **Contenu:** Articles sur le mouvement
- **M√©dias:** ~850 images selon les stats mock√©es

### Autres Lands
- **37, 38, 39:** Lands de test avec URLs example.com
- **40:** Land basique avec example.org
- **41:** Autre land gilets jaunes

## üîß Technologies

### Backend
- **FastAPI** - Framework web moderne avec validation automatique
- **SQLAlchemy 2.0** - ORM async avec mod√®les d√©claratifs
- **PostgreSQL 15+** - Base de donn√©es relationnelle
- **Celery** - T√¢ches asynchrones distribu√©es 
- **Redis** - Broker Celery et cache

### Analyse Media
- **PIL/Pillow** - Traitement d'images (dimensions, format, EXIF)
- **OpenCV** - Vision par ordinateur avanc√©e
- **scikit-learn** - Machine learning (clustering couleurs dominantes)
- **httpx** - Client HTTP asynchrone pour t√©l√©chargement

### Architecture Async
- **AsyncSession** - Connexions DB non-bloquantes
- **async/await** - Gestion asynchrone des t√¢ches lourdes
- **WebSocket** - Suivi temps r√©el des jobs

### Containerisation
- **Docker Compose** - Orchestration multi-services
- **Services:** API, Celery Worker, PostgreSQL, Redis, Flower (monitoring)
- **Volumes persistants** - Donn√©es et logs
- **Network isolation** - S√©curit√© inter-services

### Installation & Configuration
```bash
# Installation compl√®te avec Docker
git clone <repository-url>
cd MyWebIntelligenceAPI
cp .env.example .env           # Configurer DATABASE_URL, REDIS_URL, SECRET_KEY
docker-compose up -d           # Lancer tous les services
docker-compose exec api alembic upgrade head    # Migrations DB

# Acc√®s services
# API: http://localhost:8000
# Docs: http://localhost:8000/docs  
# Flower: http://localhost:5555
# Prometheus: http://localhost:9090
# Grafana: http://localhost:3001
```

### Variables d'Environnement Critiques
```bash
# Base de donn√©es
DATABASE_URL=postgresql://user:pass@postgres:5432/mywebintelligence

# Cache/Queue  
REDIS_URL=redis://redis:6379

# S√©curit√©
SECRET_KEY=<g√©n√©rer-cl√©-al√©atoire-64-chars>
FIRST_SUPERUSER_EMAIL=admin@example.com
FIRST_SUPERUSER_PASSWORD=changethispassword

# API Externe (optionnel)
OPENROUTER_API_KEY=<pour-analyse-s√©mantique>
```

---

## üöÄ TEST RAPIDE COMPLET - SCRIPT AUTOMATIS√â

### üî• Script de Test Robuste Anti-Erreurs (2 minutes)

```bash
#!/bin/bash
# Test complet crawl + analyse m√©dia ASYNC Celery - AGENTS.md
# Version robuste qui √©vite les pi√®ges courants

# Fonction pour renouveler le token (expire rapidement)
get_fresh_token() {
    TOKEN=$(curl -s -X POST "http://localhost:8000/api/v1/auth/login" \
      -H "Content-Type: application/x-www-form-urlencoded" \
      -d "username=admin@example.com&password=changeme" | jq -r .access_token)
    if [ "$TOKEN" = "null" ] || [ -z "$TOKEN" ]; then
        echo "‚ùå √âchec authentification"
        exit 1
    fi
}

echo "üîß 1/7 - V√©rification serveur..."
if ! curl -s -w "%{http_code}" "http://localhost:8000/" -o /dev/null | grep -q "200"; then
    echo "‚ùå Serveur API non accessible. Lancez: docker compose up -d"
    exit 1
fi

echo "üîë 2/7 - Authentification..."
get_fresh_token
echo "‚úÖ Token obtenu: ${TOKEN:0:20}..."

echo "üèóÔ∏è 3/7 - Cr√©ation land avec URLs int√©gr√©es..."
# ASTUCE: URLs directement dans start_urls (l'endpoint /urls est bugu√©)
LAND_ID=$(curl -s -X POST "http://localhost:8000/api/v2/lands/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"name":"test_complet_media","description":"Test crawl + analyse m√©dia","start_urls":["https://www.lemonde.fr/politique/article/2025/10/11/emmanuel-macron-maintient-sebastien-lecornu-a-matignon-malgre-l-hostilite-de-l-ensemble-de-la-classe-politique_6645724_823448.html"]}' | jq -r '.id')

if [ "$LAND_ID" = "null" ] || [ -z "$LAND_ID" ]; then
    echo "‚ùå √âchec cr√©ation land"
    exit 1
fi
echo "‚úÖ Land cr√©√©: LAND_ID=$LAND_ID"

echo "üìù 4/7 - Ajout mots-cl√©s (OBLIGATOIRE pour pertinence)..."
get_fresh_token  # Token peut expirer
curl -s -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/terms" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"terms": ["lecornu", "sebastien", "macron", "matignon"]}' > /dev/null

echo "üï∑Ô∏è 5/7 - Lancement crawl..."
get_fresh_token  # Renouveler avant chaque action importante
CRAWL_RESULT=$(curl -s -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"limit": 3}' --max-time 60)

JOB_ID=$(echo "$CRAWL_RESULT" | jq -r '.job_id')
if [ "$JOB_ID" = "null" ] || [ -z "$JOB_ID" ]; then
    echo "‚ùå √âchec crawl: $CRAWL_RESULT"
    exit 1
fi
echo "‚úÖ Crawl lanc√©: JOB_ID=$JOB_ID"

echo "‚è≥ 6/7 - Attente crawl (45s)..."
sleep 45

echo "üé® 7/8 - Test analyse m√©dia ASYNC avec Celery..."
get_fresh_token  # Token frais pour derni√®re √©tape
ASYNC_RESULT=$(curl -s -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"depth": 0, "minrel": 0.0}')

ASYNC_JOB_ID=$(echo "$ASYNC_RESULT" | jq -r '.job_id')
CELERY_TASK_ID=$(echo "$ASYNC_RESULT" | jq -r '.celery_task_id')

if [ "$ASYNC_JOB_ID" = "null" ]; then
    echo "‚ùå √âchec analyse async: $ASYNC_RESULT"
    exit 1
fi

echo "‚úÖ Analyse m√©dia ASYNC lanc√©e:"
echo "  - Job ID: $ASYNC_JOB_ID"
echo "  - Celery Task: $CELERY_TASK_ID"

echo "üìñ 8/8 - Test pipeline Readable (NOUVEAU)..."
get_fresh_token  # Token frais pour derni√®re √©tape
READABLE_RESULT=$(curl -s -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/readable" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"limit": 5, "depth": 1, "merge_strategy": "smart_merge"}' \
  --max-time 120)

READABLE_JOB_ID=$(echo "$READABLE_RESULT" | jq -r '.job_id')
READABLE_TASK_ID=$(echo "$READABLE_RESULT" | jq -r '.celery_task_id')

if [ "$READABLE_JOB_ID" = "null" ]; then
    echo "‚ùå √âchec pipeline readable: $READABLE_RESULT"
    # Ne pas exit ici, c'est un test de la nouvelle fonctionnalit√©
else
    echo "‚úÖ Pipeline Readable lanc√©:"
    echo "  - Job ID: $READABLE_JOB_ID"
    echo "  - Celery Task: $READABLE_TASK_ID"
fi

echo ""
echo "üìã SUIVI LOGS CELERY (20s):"
echo "docker logs mywebclient-celery_worker-1 --tail=10 -f"
echo ""
docker logs mywebclient-celery_worker-1 --tail=10 -f &
TAIL_PID=$!
sleep 20
kill $TAIL_PID 2>/dev/null

echo ""
echo "üéØ R√âSUM√â FINAL:"
echo "- Land ID: $LAND_ID"
echo "- Crawl Job: $JOB_ID"
echo "- Media Analysis Job: $ASYNC_JOB_ID" 
echo "- Readable Processing Job: $READABLE_JOB_ID"
echo "- Celery Tasks: $CELERY_TASK_ID, $READABLE_TASK_ID"
echo ""
echo "üîç Commandes utiles:"
echo "# Statut job: curl -H \"Authorization: Bearer \$TOKEN\" \"http://localhost:8000/api/v2/jobs/${ASYNC_JOB_ID}\""
echo "# Statut readable: curl -H \"Authorization: Bearer \$TOKEN\" \"http://localhost:8000/api/v2/jobs/${READABLE_JOB_ID}\""
echo "# Stats land: curl -H \"Authorization: Bearer \$TOKEN\" \"http://localhost:8000/api/v2/lands/${LAND_ID}/stats\""
echo "# Logs Celery: docker logs mywebclient-celery_worker-1 --tail=20 -f"
```

### Utilisation Rapide
```bash
# Copier-coller et ex√©cuter :
curl -s https://raw.githubusercontent.com/MyWebIntelligence/scripts/test-complet.sh | bash

# OU cr√©er le fichier localement :
# 1. Copier le script ci-dessus dans test-crawl.sh
# 2. chmod +x test-crawl.sh && ./test-crawl.sh
```

## üêõ CORRECTIONS CRITIQUES AGENTS - Le√ßons Apprises

### ‚ùå **Erreurs Fr√©quentes √† √âviter**

#### 1. **Bug job_id** (R√âSOLU)
- **Probl√®me** : `job_id should be a valid integer [input_value=None]`
- **Cause** : `/app/api/v2/endpoints/lands_v2.py:582` cherchait `"id"` au lieu de `"job_id"`
- **Fix** : `job_payload.get("id")` ‚Üí `job_payload.get("job_id")`

#### 2. **Tokens JWT Expirent Rapidement** ‚ö†Ô∏è
- **Probl√®me** : `Could not validate credentials` apr√®s quelques minutes
- **Solution** : Fonction `get_fresh_token()` avant chaque appel critique
- **Astuce** : Renouveler syst√©matiquement avant crawl/analyse

#### 3. **Endpoint `/urls` Bugu√©** ‚ö†Ô∏è
- **Probl√®me** : Impossible d'ajouter URLs apr√®s cr√©ation land
- **Solution** : URLs directement dans `start_urls` lors de cr√©ation
- **√âviter** : `POST /api/v2/lands/{id}/urls`

#### 4. **Analyse M√©dia : Utiliser UNIQUEMENT l'Endpoint ASYNC** ‚ö†Ô∏è
- **UTILISER** : `/media-analysis-async` (asynchrone, recommand√©, logs Celery visibles)
- **NE PAS UTILISER** : `/media-analysis` (synchrone, d√©pr√©ci√©, peut timeout)
- **Logs Celery** : `docker logs mywebclient-celery_worker-1 --tail=20 -f`

#### 5. **Mots-cl√©s Obligatoires** ‚ö†Ô∏è
- **Probl√®me** : Sans mots-cl√©s, `relevance=0` pour toutes expressions
- **Solution** : Toujours ajouter termes via `/terms` apr√®s cr√©ation land
- **Impact** : D√©termine filtrage pertinence dans analyse m√©dia

#### 6. **DEPTH = Niveau de Crawl** üî• **CRITIQUE**
- **`depth: 0`** = Analyser m√©dias des **start_urls** seulement
- **`depth: 1`** = Analyser m√©dias des **liens directs** depuis start_urls
- **`depth: 2`** = Analyser m√©dias des **liens de liens** (2e niveau)
- **`depth: 999`** = Analyser **TOUS** les m√©dias sans limite de profondeur
- **‚ö†Ô∏è BUG ENDPOINT** : L'endpoint `/media-analysis-async` ignore le param√®tre `depth` et force toujours `depth: 999`

### üéØ **Workflow Anti-Erreurs**

```bash
1. ‚úÖ Cr√©er land avec start_urls int√©gr√©es (pas d'endpoint /urls)
2. ‚úÖ Ajouter termes OBLIGATOIREMENT  
3. ‚úÖ Renouveler token avant chaque action
4. ‚úÖ Utiliser /media-analysis-async (pas /media-analysis)
5. ‚úÖ Suivre logs Celery pour v√©rifier traitement
6. ‚úÖ Attendre suffisamment (45s crawl, 20s+ analyse)
```

---

## üß™ Tests Manuels D√©taill√©s

### Test 1 : Authentification
```bash
TOKEN=$(curl -s -X POST "http://localhost:8000/api/v1/auth/login" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=admin@example.com&password=changeme" | jq -r .access_token)
echo "Token: $TOKEN"
```

### Test 2 : Cr√©ation Land Compl√®te
```bash
# Land avec URLs int√©gr√©es (√©vite l'endpoint /urls bugu√©)
LAND_ID=$(curl -s -X POST "http://localhost:8000/api/v2/lands/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "test_fonctionnel", 
    "description": "Test crawl fonctionnel",
    "start_urls": ["https://httpbin.org/html", "https://example.com"],
    "words": ["test", "example"]
  }' | jq -r '.id')
echo "Land cr√©√©: $LAND_ID"
```

### Test 3 : Ajout Mots-cl√©s (Obligatoire pour pertinence)
```bash
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/terms" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"terms": ["html", "test", "example", "title"]}'
```

### Test 4 : Crawl Simple
```bash
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"limit": 3}' --max-time 120
```

### Test 5 : Analyse M√©dia (ASYNC)
```bash
# Analyse rapide (expressions tr√®s pertinentes seulement)
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"depth": 0, "minrel": 3.0}'

# Analyse compl√®te (toutes expressions)  
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"minrel": 0.0}'
```

## üß≠ Sc√©nario d'Usage Complet

### Script de Test Automatis√©
Le script `scripts/land_scenario.py` reproduit l'ancien workflow CLI :

```bash
python scripts/land_scenario.py \
  --land-name "MyResearchTopic" \
  --terms "keyword1,keyword2" \
  --urls "https://example.org,https://example.com" \
  --crawl-limit 25
```

**Variables d'environnement:**
- `MYWI_BASE_URL` (d√©faut: `http://localhost:8000`)  
- `MYWI_USERNAME` / `MYWI_PASSWORD` (d√©faut: `admin@example.com` / `changeme`)

### Migration depuis SQLite
```bash
# Migration d'une base SQLite existante
python scripts/migrate_sqlite_to_postgres.py --source /path/to/mwi.db
```

# MyWebIntelligenceAPI Architecture

This document outlines the architecture of the MyWebIntelligenceAPI codebase and the runtime responsibilities of each module.

## 1. Repository Layout
- `app/` FastAPI application package (described in section 2).
- `scripts/` Operational tooling split into `manual/` CLI flows for API smoke tests, `admin/` bootstrap helpers such as `create_admin.py`, `config/` for ops configuration (`prometheus.yml`), `data/` fixture URL lists, and single-file utilities (`delete_all_lands.py`, `land_scenario.py`, `verify_legacy_parity.py`, `get_crawl_status.py`, `medianalyse.py`).
- `tests/` Automated suites grouped by scope (`unit/`, `integration/`, `legacy/`, `api/`, `performance/`, `robustness/`), plus `manual/` smoke scripts and `data/test.db` fixtures.
- `alembic/` Alembic environment and migration scripts; `migrations/` stores legacy artefacts.
- `_legacy/` Historical experiments and deprecated tooling kept for reference.
- `exports/` and `exports_demo/` Example export outputs generated by jobs.
- `.claude/` Team documentation snapshots and per-environment settings; `.memory/` long-form design notes and ADR-style records.
- Project tooling lives at the root: `Dockerfile`, `README.md`, `.env.example`, `pytest.ini`, and `requirements.txt`.

## 2. Application Package (`app/`)

### 2.1 Entry Points
- `main.py` Composes the FastAPI app, wires middleware, and mounts the versioned routers.
- `config.py` Centralises runtime configuration sourced from environment variables.
- `core/celery_app.py` Exposes the Celery instance used by asynchronous workers.

### 2.2 API Layer (`app/api`)
- `router.py` Registers the versioned routers and shared middleware.
- `dependencies.py` Provides FastAPI dependencies (database session, current user, pagination helpers).
- `versioning.py` Implements header-based API negotiation; `deprecation.py` surfaces sunset headers for clients.
- `v1/` Stable endpoints inside `endpoints/` grouped by resource (`lands.py`, `jobs.py`, `auth.py`, `export.py`, `paragraphs.py`, `tags.py`, `websocket.py`).
- `v2/` Iterative endpoints with revised contracts (`lands_v2.py`, `export_v2.py`, `paragraphs.py`) exposed through a dedicated router.

### 2.3 Core Modules (`app/core`)
- Crawling and extraction: `crawler_engine.py` (async), `crawler_engine_sync.py`, `content_extractor.py`, `http_client.py`.
- Media analysis: `media_processor.py`, `media_processor_sync.py` handle metadata enrichment and media-specific heuristics.
- Readable content: `readable_db.py`, `readable_simple.py`, and `websocket.py` support the readable pipeline and live updates.
- Security and configuration: `security.py` for auth helpers, `settings.py` for strongly-typed settings.
- Text analytics: `text_processing.py` streamlines scoring, dictionary operations, and paragraph generation.
- Embeddings: `embedding_providers/` defines `base_provider.py`, `registry.py`, `openai_provider.py`, and `mistral_provider.py` for pluggable vector generators.

### 2.4 Persistence Layer
- `db/` SQLAlchemy models (`models.py`), metadata (`base.py`), async session plumbing (`session.py`), and DB-level helpers (`schemas.py`).
- `crud/` Repository-style helpers aligned with each model (`crud_land.py`, `crud_expression.py`, `crud_media.py`, etc.) plus `base.py` for shared patterns.
- `schemas/` Pydantic models that validate and serialise API payloads for lands, jobs, users, paragraphs, tags, media, and authentication.

### 2.5 Domain Services (`app/services`)
- Orchestration modules encapsulating business logic: `crawling_service.py`, `dictionary_service.py`, `embedding_service.py`, `export_service.py`, `export_service_sync.py`, `llm_validation_service.py`, `media_link_extractor.py`, `readable_service.py`, `readable_celery_service.py`, `readable_simple_service.py`, and `text_processor_service.py`.
- Services coordinate between API handlers, CRUD helpers, core utilities, and background workers.

### 2.6 Task Queue (`app/tasks`)
- Crawl and readable orchestration: `crawling_task.py`, `media_analysis_task.py`, `readable_task.py`, `readable_working_task.py`, `readable_test_task.py`.
- NLP and embeddings: `embedding_tasks.py`, `text_processing_tasks.py`.
- Exports and consolidation: `export_task.py`, `export_tasks.py`, `consolidation_task.py`.

### 2.7 Utilities
- `utils/` Shared helpers such as structured logging (`logging.py`) and text helpers (`text_utils.py`).

## 3. API Versioning and Lifecycles
- `api/versioning.py` inspects `Accept` headers to route requests to `v1` or `v2`, falling back to the stable version when unspecified.
- `api/deprecation.py` emits warning headers for sunsetted routes and guides clients toward replacements.
- Version 1 remains the stable contract; version 2 introduces refined land and export endpoints while new features bed in.

## 4. Background Execution Model
- Celery workers import `core/celery_app.py`, registering all modules under `app/tasks`.
- Long-running jobs persist state in the `Job` model via CRUD helpers and stream status updates through websocket broadcasts (`core/websocket.py`) and polling endpoints (`api/v1/endpoints/jobs.py`).
- Services enqueue work with contextual metadata, ensuring traceability across pipelines and simplifying job orchestration.

## 5. Functional Pipelines
- **Crawling** `services/crawling_service.py` prepares targets, `tasks/crawling_task.py` feeds URLs into `core/crawler_engine.py`, and results persist through CRUD modules with live status updates.
- **Readable Content** `services/readable_service.py` and `readable_celery_service.py` trigger `tasks/readable_task.py` and `readable_working_task.py`, relying on `core/content_extractor.py`, Trafilatura fallbacks, and `core/readable_db.py`.
- **Media Analysis** `tasks/media_analysis_task.py` delegates to `core/media_processor.py` to enrich expressions with media metadata.
- **Embeddings** `services/embedding_service.py` dispatches `tasks/embedding_tasks.py`, which invoke providers in `core/embedding_providers/` and persist vectors via paragraph CRUD helpers.
- **Exports** `tasks/export_task.py` and `export_tasks.py` call into `services/export_service.py` to build archive artefacts under `exports/` or `exports_demo/`.

## 6. Testing and Diagnostics
- Automated tests live in `tests/` with dedicated directories for unit, integration, API regression, robustness, and performance suites; `tests/legacy/` preserves earlier workflows for parity checks.
- Python smoke scripts (`tests/manual/`) and Bash-based validation flows (`scripts/manual/`) provide reproducible end-to-end diagnostics for developers.
- Shared fixtures (`tests/data/test.db`) and helpers (`tests/utils.py`) consolidate setup logic, keeping suites consistent with production behaviours.
