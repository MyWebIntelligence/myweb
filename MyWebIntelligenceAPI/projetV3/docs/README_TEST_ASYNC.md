# Test Crawler Async - Guide d'Utilisation

**Date**: 19 octobre 2025
**Objectif**: Valider les corrections du crawler async (alignement avec sync)

---

## üéØ Objectif du Test

Ce script teste **sp√©cifiquement le crawler ASYNC** ([crawler_engine.py](../app/core/crawler_engine.py)) pour valider les **5 phases de corrections** appliqu√©es suite √† l'alignement avec le crawler sync.

### Corrections V√©rifi√©es

| Phase | Correction | V√©rification |
|-------|-----------|--------------|
| **1** | Extraction headers HTTP (`last_modified`, `etag`) | Pr√©sence champs en DB |
| **2** | Cr√©ation dict `metadata` | `title`, `lang`, `description` remplis |
| **3** | Parsing `published_at` | Date pars√©e et persist√©e |
| **4** | `update_data.update()` avec metadata | Utilisation dict metadata |
| **5** | **Fix NameError** `metadata_lang` ‚Üí `final_lang` | Calcul `relevance` r√©ussi |

---

## üìã Pr√©requis

### 1. Environnement Docker

```bash
# Services requis en cours d'ex√©cution
docker ps | grep -E "api|db|celery"
```

Vous devez avoir :
- ‚úÖ `mywebclient-api-1` (API FastAPI)
- ‚úÖ `mywebclient-db-1` (PostgreSQL)
- ‚úÖ `mywebclient-celery_worker-1` (Celery worker - optionnel)

### 2. Base de Donn√©es Initialis√©e

```bash
# V√©rifier la connexion DB
docker exec mywebclient-db-1 psql -U mwi_user -d mwi_db -c "SELECT 1"
```

### 3. Utilisateur Admin

Par d√©faut, le script utilise :
- **Email**: `admin@example.com`
- **Password**: `changethispassword`

Si vous avez d'autres credentials, modifiez le script ligne 6-8.

### 4. D√©pendances

```bash
# Le script n√©cessite:
- curl
- jq (parser JSON)
- docker
```

Installation si manquant :

```bash
# macOS
brew install jq

# Ubuntu/Debian
sudo apt-get install jq

# Arch
sudo pacman -S jq
```

---

## üöÄ Utilisation

### Lancement Simple

```bash
cd MyWebIntelligenceAPI/tests
./test-crawl-async.sh
```

### Avec Configuration Personnalis√©e

```bash
# Personnaliser l'URL de l'API
API_URL="http://localhost:8000" ./test-crawl-async.sh

# Personnaliser le container DB
DB_CONTAINER="mon-db-custom" ./test-crawl-async.sh

# Combinaison
API_URL="http://api.local:8080" \
DB_CONTAINER="custom-db" \
./test-crawl-async.sh
```

---

## üìä Ce que Fait le Script

### Phase 1 : Initialisation (√©tapes 1-3)
1. ‚úÖ V√©rifie que l'API est accessible
2. ‚úÖ V√©rifie que la DB est accessible
3. ‚úÖ S'authentifie et obtient un token JWT

### Phase 2 : Pr√©paration (√©tapes 4-5)
4. ‚úÖ Cr√©e un land de test avec 3 URLs stables
5. ‚úÖ Ajoute des mots-cl√©s pour le calcul de pertinence

### Phase 3 : Crawl (√©tape 6)
6. ‚úÖ Lance le crawl via l'API (utilise **crawler ASYNC**)

### Phase 4 : Validation (√©tape 7)

Le script v√©rifie **tous les champs critiques** en DB :

```sql
SELECT
    id, url, title, description, lang,
    published_at,           -- Phase 3 ‚úÖ
    last_modified, etag,    -- Phase 1 ‚úÖ
    relevance,              -- Phase 5 ‚úÖ (fix NameError)
    content, readable,
    word_count, approved_at
FROM expressions
WHERE land_id = <TEST_LAND_ID>;
```

### Phase 5 : Rapport
- üìä Statistiques de crawl
- ‚úÖ Validation de chaque phase
- üéØ Verdict final (succ√®s/√©chec)

---

## üìà Interpr√©tation des R√©sultats

### ‚úÖ Succ√®s Complet

```
‚úÖ SUCC√àS COMPLET - Crawler async 100% fonctionnel
‚úÖ Alignement sync/async confirm√©
‚úÖ Toutes les phases valid√©es
```

**Signification** :
- Tous les champs m√©tadonn√©es sont remplis
- Pas de NameError sur `metadata_lang`
- Le crawler async est align√© avec le sync

### ‚úÖ Succ√®s Partiel

```
‚úÖ SUCC√àS PARTIEL - Pas de NameError
‚ö†Ô∏è  Quelques champs optionnels manquants (normal)
```

**Signification** :
- Le bug critique (NameError) est r√©solu
- Quelques champs optionnels manquent (ex: `published_at`, `etag`)
- C'est **NORMAL** si les pages crawl√©es ne fournissent pas ces m√©tadonn√©es

### ‚ùå √âchec

```
‚ùå √âCHEC - NameError probable ou m√©tadonn√©es manquantes
‚ùå V√©rifier les logs du crawler async
```

**Actions** :
1. V√©rifier les logs de l'API :
   ```bash
   docker logs mywebclient-api-1 --tail 100 | grep -i "error\|nameerror"
   ```

2. V√©rifier la DB manuellement :
   ```bash
   docker exec mywebclient-db-1 psql -U mwi_user -d mwi_db -c \
     "SELECT url, lang, relevance FROM expressions ORDER BY created_at DESC LIMIT 5;"
   ```

---

## üîç Debugging

### Voir les Logs de l'API

```bash
# Logs temps r√©el
docker logs -f mywebclient-api-1

# Derni√®res erreurs
docker logs mywebclient-api-1 --tail 100 | grep -E "ERROR|NameError|metadata_lang"

# Logs de crawl
docker logs mywebclient-api-1 --tail 200 | grep -i "crawl"
```

### Requ√™tes DB Manuelles

```bash
# Se connecter √† la DB
docker exec -it mywebclient-db-1 psql -U mwi_user -d mwi_db

# V√©rifier les derni√®res expressions
SELECT id, url, title, lang, relevance, published_at
FROM expressions
ORDER BY created_at DESC
LIMIT 10;

# V√©rifier les champs NULL
SELECT
    COUNT(*) as total,
    COUNT(lang) as with_lang,
    COUNT(relevance) as with_relevance,
    COUNT(published_at) as with_published
FROM expressions
WHERE created_at > NOW() - INTERVAL '5 minutes';
```

### Tester Manuellement l'API

```bash
# 1. Login
TOKEN=$(curl -s -X POST "http://localhost:8000/api/v1/auth/login" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=admin@example.com&password=changethispassword" | jq -r .access_token)

# 2. Cr√©er un land
curl -X POST "http://localhost:8000/api/v2/lands/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "test_manual",
    "start_urls": ["https://www.example.com"]
  }' | jq '.'

# 3. Lancer crawl (remplacer LAND_ID)
curl -X POST "http://localhost:8000/api/v2/lands/{LAND_ID}/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"limit": 1}' | jq '.'
```

---

## üß™ Tests Compl√©mentaires

### Comparaison Sync vs Async

Pour v√©rifier la parit√© compl√®te :

```bash
# 1. Lancer test ASYNC
./test-crawl-async.sh > /tmp/result_async.txt

# 2. Lancer test SYNC
./test-crawl-simple.sh > /tmp/result_sync.txt

# 3. Comparer les r√©sultats
diff /tmp/result_async.txt /tmp/result_sync.txt
```

Les diff√©rences attendues :
- ‚úÖ Dur√©e de crawl (async peut √™tre plus rapide)
- ‚úÖ Job IDs diff√©rents
- ‚ùå Les champs `lang`, `relevance`, `title` doivent √™tre **identiques**

### Test de Charge

```bash
# Test avec plus d'URLs
# Modifier TEST_URLS dans le script pour ajouter:
TEST_URLS=(
    "https://www.example.com"
    "https://en.wikipedia.org/wiki/Web_crawler"
    "https://www.ietf.org/rfc/rfc2616.txt"
    "https://developer.mozilla.org/en-US/"
    "https://www.w3.org/standards/"
    # ... jusqu'√† 20 URLs
)
```

---

## üìù Personnalisation

### Modifier les URLs de Test

√âditez le script, ligne 26-30 :

```bash
TEST_URLS=(
    "https://votre-url-1.com"
    "https://votre-url-2.com"
    "https://votre-url-3.com"
)
```

**Recommandations** :
- Utilisez des URLs **stables** (qui ne changent pas)
- Privil√©giez des sites **bien structur√©s** (metadata compl√®tes)
- √âvitez les sites avec **anti-scraping** agressif

### Modifier les Mots-Cl√©s

Ligne 96 :

```bash
-d '{"terms": ["web", "crawler", "http", "metadata"]}'
```

Adaptez selon votre domaine de test.

---

## üéØ Crit√®res de R√©ussite

Pour que le test soit valid√©, **au minimum** :

1. ‚úÖ `relevance` calcul√©e (pas de NameError)
2. ‚úÖ `lang` d√©tect√©e
3. ‚úÖ `title` extrait
4. ‚úÖ `approved_at` rempli (expression valid√©e)

**Optionnels mais bons signes** :
- ‚úÖ `published_at` pars√©
- ‚úÖ `last_modified` ou `etag` pr√©sents
- ‚úÖ `description` et `keywords` remplis

---

## üêõ Probl√®mes Connus

### "NameError: name 'metadata_lang' is not defined"

**Cause** : Les corrections n'ont pas √©t√© appliqu√©es
**Solution** : V√©rifier que [crawler_engine.py](../app/core/crawler_engine.py) ligne 269 utilise bien `final_lang or "fr"` et **pas** `metadata_lang or 'fr'`

### "KeyError: 'title'" sur metadata['title']

**Cause** : Utilisation de `metadata['title']` au lieu de `metadata.get('title')`
**Solution** : V√©rifier ligne 265 du crawler_engine.py

### Tous les champs sont NULL

**Cause** : Le crawler a √©chou√© silencieusement
**Solution** :
1. V√©rifier les logs API
2. Tester avec une URL simple (https://www.example.com)
3. V√©rifier que l'API utilise bien le crawler async et pas sync

---

## üìö R√©f√©rences

- **Plan d'alignement** : [.claude/align_sync_async.md](../../.claude/align_sync_async.md)
- **Erreur double crawler** : [.claude/ERREUR_DOUBLE_CRAWLER.md](../../.claude/ERREUR_DOUBLE_CRAWLER.md)
- **Fichier corrig√©** : [app/core/crawler_engine.py](../app/core/crawler_engine.py)
- **R√©f√©rence sync** : [app/core/crawler_engine_sync.py](../app/core/crawler_engine_sync.py)

---

## ‚úÖ Checklist Avant de Lancer

- [ ] Services Docker en cours d'ex√©cution
- [ ] DB accessible et initialis√©e
- [ ] User admin cr√©√©
- [ ] `jq` install√©
- [ ] Script ex√©cutable (`chmod +x test-crawl-async.sh`)

---

**Derni√®re mise √† jour** : 19 octobre 2025
**Version script** : 1.0
**Auteur** : √âquipe MyWebIntelligence
