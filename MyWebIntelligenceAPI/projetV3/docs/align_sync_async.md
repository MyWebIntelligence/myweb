# Plan d'Alignement Crawler Async â†’ Sync

**Date**: 19 octobre 2025
**Statut**: ğŸ”´ CRITIQUE - NameError en production sur le crawler async
**RÃ©fÃ©rence**: crawler_engine_sync.py est la RÃ‰FÃ‰RENCE (100% fonctionnel)

---

## ğŸ¯ Objectif

Porter les corrections du crawler **SYNC** (rÃ©fÃ©rence fonctionnelle) vers le crawler **ASYNC** (buguÃ©) pour restaurer la paritÃ© complÃ¨te entre les deux implÃ©mentations.

---

## ğŸ› Bug IdentifiÃ©

### Localisation
**Fichier**: `MyWebIntelligenceAPI/app/core/crawler_engine.py`
**Lignes**: 221-236 (mÃ©thode `crawl_expression`)

### SymptÃ´me
```python
# Ligne 230 - NameError
temp_expr = TempExpr(metadata['title'], readable_content, expr_id)

# Ligne 234 - NameError
relevance = await text_processing.expression_relevance(
    land_dict,
    temp_expr,
    metadata_lang or 'fr'  # âŒ metadata_lang n'est jamais dÃ©fini !
)
```

### Cause Racine
Le code async utilise encore d'anciennes variables (`metadata`, `metadata_lang`) qui ne sont **jamais initialisÃ©es**. Ces variables ont Ã©tÃ© supprimÃ©es lors d'une refactorisation, mais le code qui les utilise n'a pas Ã©tÃ© mis Ã  jour.

### Impact
- **NameError** dÃ¨s qu'on tente de calculer la pertinence
- Impossible d'utiliser le crawler async en production
- Perte des champs : `published_at`, `last_modified`, `etag`
- Scores qualitÃ©/sentiment incorrects (pas de `final_lang`)

---

## ğŸ“Š DiffÃ©rences Sync vs Async

| Aspect | Sync (RÃ‰FÃ‰RENCE) | Async (BUGUÃ‰) | Action |
|--------|------------------|---------------|--------|
| **MÃ©tadonnÃ©es** | âœ… Dict `metadata` crÃ©Ã© (L212-219) | âŒ Pas de dict metadata | âš ï¸ PORTER |
| **final_lang** | âœ… CalculÃ© avec fallback (L244-245) | âœ… CalculÃ© correctement (L203) | âœ… OK |
| **published_at** | âœ… ParsÃ© depuis meta tags (L250-257) | âŒ Jamais parsÃ© | âš ï¸ PORTER |
| **last_modified** | âœ… Extrait des headers (L168-176) | âŒ Pas extrait | âš ï¸ PORTER |
| **etag** | âœ… Extrait des headers (L168-176) | âŒ Pas extrait | âš ï¸ PORTER |
| **Pertinence** | âœ… Utilise `final_lang` (L283-290) | âŒ Utilise `metadata_lang` inexistant (L231-236) | âš ï¸ CORRIGER |
| **HTML content** | âœ… PersistÃ© (L228-233) | âœ… PersistÃ© (L187-191) | âœ… OK |
| **Sentiment** | âœ… Utilise `final_lang` (L308) | âœ… Utilise `final_lang` (L248) | âœ… OK |
| **Quality** | âœ… Utilise update_data (L364) | âœ… Utilise update_data (L292) | âœ… OK |

---

## ğŸ”§ Plan de Correction DÃ©taillÃ©

### Phase 1 : Extraction des Headers HTTP (Nouveau)

**Fichier**: `crawler_engine.py`
**AprÃ¨s ligne**: 169 (aprÃ¨s dÃ©finition de `http_status_code`)

**Code Ã  ajouter**:
```python
# Extract HTTP headers: Last-Modified and ETag
last_modified_str = None
etag_str = None
if http_status_code and http_status_code < 400:
    try:
        last_modified_str = response.headers.get('last-modified', None)
        etag_str = response.headers.get('etag', None)
    except Exception:
        pass

update_data = {
    "http_status": str(http_status_code),
    "crawled_at": datetime.utcnow(),
    "content_type": content_type,
    "content_length": content_length,
    "last_modified": last_modified_str,  # â† NOUVEAU
    "etag": etag_str,                     # â† NOUVEAU
}
```

**RÃ©fÃ©rence**: `crawler_engine_sync.py:167-185`

---

### Phase 2 : CrÃ©ation du Dictionnaire Metadata

**Fichier**: `crawler_engine.py`
**AprÃ¨s ligne**: 191 (aprÃ¨s log "No HTML content")

**Code Ã  ajouter**:
```python
# Create metadata dictionary for later use
metadata = {
    'title': extraction_result.get('title', expr_url),
    'description': extraction_result.get('description'),
    'keywords': extraction_result.get('keywords'),
    'lang': extraction_result.get('language'),
    'canonical_url': extraction_result.get('canonical_url'),
    'published_at': extraction_result.get('published_at')
}
```

**RÃ©fÃ©rence**: `crawler_engine_sync.py:212-219`

---

### Phase 3 : Parse de published_at

**Fichier**: `crawler_engine.py`
**AprÃ¨s ligne**: 206 (aprÃ¨s log de language detection)

**Code Ã  ajouter**:
```python
# Parse published_at if it's a string (from meta tags)
published_at = None
if metadata.get("published_at"):
    try:
        from dateutil import parser as date_parser
        published_at = date_parser.parse(metadata["published_at"])
    except Exception:
        pass
```

**RÃ©fÃ©rence**: `crawler_engine_sync.py:250-257`

---

### Phase 4 : Mise Ã  Jour de update_data avec Metadata

**Fichier**: `crawler_engine.py`
**Remplacer lignes**: 208-215

**Code actuel Ã  remplacer**:
```python
update_data["title"] = extraction_result.get('title', expr_url)
update_data["description"] = extraction_result.get('description')
update_data["keywords"] = extraction_result.get('keywords')
update_data["lang"] = final_lang
update_data["readable"] = readable_content
update_data["canonical_url"] = extraction_result.get('canonical_url')
update_data["word_count"] = word_count
update_data["reading_time"] = reading_time
```

**Nouveau code**:
```python
update_data.update(
    {
        "title": metadata.get("title"),
        "description": metadata.get("description"),
        "keywords": metadata.get("keywords"),
        "lang": final_lang,  # FIXED: Use 'lang' to match SQLAlchemy attribute
        "readable": readable_content,
        "canonical_url": metadata.get("canonical_url"),
        "published_at": published_at,  # â† NOUVEAU
        "word_count": word_count,
        "reading_time": reading_time,
    }
)
```

**RÃ©fÃ©rence**: `crawler_engine_sync.py:259-271`

---

### Phase 5 : Correction du Calcul de Pertinence (CRITIQUE)

**Fichier**: `crawler_engine.py`
**Remplacer lignes**: 221-236

**Code actuel BUGUÃ‰**:
```python
# 3. Calculate relevance (requires dictionary)
land_dict = await text_processing.get_land_dictionary(self.db, expr_land_id)

# Create a temporary object with the extracted data for relevance calculation
class TempExpr:
    def __init__(self, title, readable, expr_id):
        self.title = title
        self.readable = readable
        self.id = expr_id

temp_expr = TempExpr(metadata['title'], readable_content, expr_id)  # âŒ NameError
relevance = await text_processing.expression_relevance(
    land_dict,
    temp_expr,
    metadata_lang or 'fr'  # âŒ NameError
)
update_data["relevance"] = relevance
```

**Nouveau code CORRIGÃ‰**:
```python
# 3. Calculate relevance (requires dictionary)
land_dict = await text_processing.get_land_dictionary(self.db, expr_land_id)

class TempExpr:
    def __init__(self, title: Optional[str], readable: Optional[str], expr_id: int):
        self.title = title
        self.readable = readable
        self.id = expr_id

temp_expr = TempExpr(metadata.get("title"), readable_content, expr_id)  # âœ… Utilise metadata dict
relevance = await text_processing.expression_relevance(
    land_dict,
    temp_expr,
    final_lang or "fr"  # âœ… Utilise final_lang (dÃ©fini ligne 203)
)
update_data["relevance"] = relevance
```

**RÃ©fÃ©rence**: `crawler_engine_sync.py:273-292`

---

## ğŸ§ª Plan de Test

### Test 1 : VÃ©rification Syntaxe
```bash
cd MyWebIntelligenceAPI
python -m py_compile app/core/crawler_engine.py
```

**RÃ©sultat attendu**: Pas d'erreur de compilation

---

### Test 2 : Test Unitaire
```bash
cd MyWebIntelligenceAPI
pytest tests/test_crawler_engine.py -v -k "test_crawl_expression"
```

**RÃ©sultat attendu**:
- âœ… Pas de NameError
- âœ… Champs `published_at`, `last_modified`, `etag` remplis
- âœ… `relevance` calculÃ©e correctement

---

### Test 3 : Comparaison Sync vs Async
```bash
# Script de comparaison Ã  crÃ©er
python scripts/compare_crawlers.py --url "https://example.com"
```

**RÃ©sultat attendu**:
- âœ… MÃªme extraction de mÃ©tadonnÃ©es
- âœ… MÃªme calcul de pertinence
- âœ… MÃªmes champs persistÃ©s en DB
- âœ… MÃªme `final_lang` utilisÃ©

---

### Test 4 : Test d'IntÃ©gration (Production-like)
```bash
# Via API (utilise async crawler)
curl -X POST "http://localhost:8000/api/v2/lands/1/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"max_pages": 1}'

# Via Celery (utilise sync crawler)
# â†’ DÃ©clenchÃ© automatiquement par les tÃ¢ches de fond

# Comparer les rÃ©sultats en DB
docker exec mywebclient-db-1 psql -U mwi_user -d mwi_db -c "
  SELECT
    id, url, title, published_at, last_modified, etag,
    relevance, lang, word_count
  FROM expressions
  ORDER BY created_at DESC
  LIMIT 2;
"
```

**RÃ©sultat attendu**:
- âœ… Les 2 expressions ont les mÃªmes champs remplis
- âœ… Pas de NULL sur `published_at` (si disponible)
- âœ… `last_modified` et `etag` prÃ©sents
- âœ… `relevance` > 0

---

## ğŸ“ Checklist de Validation

### Avant de commencer
- [ ] Lire ce document en entier
- [ ] Lire ERREUR_DOUBLE_CRAWLER.md
- [ ] CrÃ©er une branche : `git checkout -b fix/align-async-crawler`

### Phase 1 - Corrections
- [ ] âœ… Ajouter extraction `last_modified` et `etag`
- [ ] âœ… CrÃ©er dictionnaire `metadata`
- [ ] âœ… Parser `published_at`
- [ ] âœ… Mettre Ã  jour `update_data.update()`
- [ ] âœ… Corriger calcul de pertinence (utiliser `metadata` dict et `final_lang`)

### Phase 2 - Tests
- [ ] âœ… Test de compilation (py_compile)
- [ ] âœ… Tests unitaires passent
- [ ] âœ… Test comparatif sync/async
- [ ] âœ… Test d'intÃ©gration DB

### Phase 3 - Validation
- [ ] âœ… Code review : comparer ligne par ligne avec sync
- [ ] âœ… VÃ©rifier logs : pas de NameError
- [ ] âœ… VÃ©rifier DB : tous les champs remplis
- [ ] âœ… Tester en local avec vraie URL

### Phase 4 - Documentation
- [ ] âœ… Mettre Ã  jour ERREUR_DOUBLE_CRAWLER.md (date de correction)
- [ ] âœ… Mettre Ã  jour INDEX_DOCUMENTATION.md
- [ ] âœ… Commit avec message explicite

---

## ğŸ¯ CritÃ¨res de SuccÃ¨s

### Fonctionnels
1. âœ… **Pas de NameError** lors du crawl async
2. âœ… **Champs metadata** tous remplis (title, description, keywords, lang, canonical_url, published_at)
3. âœ… **Headers HTTP** persistÃ©s (last_modified, etag)
4. âœ… **Pertinence** calculÃ©e avec `final_lang` (pas `metadata_lang`)
5. âœ… **Sentiment & Quality** utilisent `final_lang` correctement

### Techniques
6. âœ… **ParitÃ© 100%** entre sync et async sur la logique de mÃ©tadonnÃ©es
7. âœ… **Tests passent** sans rÃ©gression
8. âœ… **Logs propres** (pas de warning sur variables manquantes)

---

## ğŸ“š RÃ©fÃ©rences

### Fichiers Ã  Modifier
- `MyWebIntelligenceAPI/app/core/crawler_engine.py` (async)

### Fichiers de RÃ©fÃ©rence
- `MyWebIntelligenceAPI/app/core/crawler_engine_sync.py` (rÃ©fÃ©rence)

### Documentation
- `.claude/ERREUR_DOUBLE_CRAWLER.md` - Contexte du problÃ¨me
- `.claude/TRANSFERT_API_CRAWL.md` - Audit complet
- `.claude/INDEX_DOCUMENTATION.md` - Index gÃ©nÃ©ral

### Sections ClÃ©s du Sync (RÃ©fÃ©rence)
- Lignes 167-176 : Extraction headers HTTP
- Lignes 212-219 : CrÃ©ation dict metadata
- Lignes 244-245 : Calcul final_lang
- Lignes 250-257 : Parse published_at
- Lignes 259-271 : Update data avec metadata
- Lignes 273-292 : Calcul pertinence avec final_lang

---

## âš ï¸ Points d'Attention

### 1. Types de DonnÃ©es
- `http_status` : **string** (pas int) - legacy format
- `published_at` : **datetime** (pas string) - parser avec dateutil
- `last_modified` : **string** (header HTTP brut)
- `etag` : **string** (header HTTP brut)

### 2. Ordre des OpÃ©rations
L'ordre est **CRITIQUE** :
1. Extraire HTML content
2. CrÃ©er dict `metadata`
3. Calculer `word_count`, `reading_time`, `final_lang`
4. Parser `published_at`
5. Update `update_data` avec metadata
6. Calculer pertinence avec `final_lang`
7. Calculer sentiment avec `final_lang`
8. Calculer quality avec `update_data`

### 3. Variables Ã  Utiliser
- âœ… `metadata.get("title")` (nouveau dict)
- âœ… `final_lang` (calculÃ© ligne 203)
- âŒ ~~`metadata['title']`~~ (provoque KeyError si absent)
- âŒ ~~`metadata_lang`~~ (n'existe plus)

### 4. Gestion des Erreurs
- Utiliser `.get()` sur les dicts (pas `[]`)
- Fallback `final_lang or "fr"` pour la pertinence
- Try/except sur le parsing de dates

---

## ğŸš€ Prochaines Ã‰tapes (AprÃ¨s Correction)

1. **Monitoring** : VÃ©rifier que le bug ne revient pas
2. **CI/CD** : Ajouter test comparatif sync/async automatique
3. **Refactoring** : Ã€ terme, fusionner les deux crawlers en un seul
4. **Documentation** : Mettre Ã  jour guide du dÃ©veloppeur

---

## ğŸ“… Timeline EstimÃ©

| Phase | DurÃ©e | Description |
|-------|-------|-------------|
| **Lecture doc** | 15 min | Lire ce plan + ERREUR_DOUBLE_CRAWLER.md |
| **Corrections** | 30 min | Appliquer les 5 phases de corrections |
| **Tests unitaires** | 15 min | Pytest + vÃ©rifications syntaxe |
| **Tests intÃ©gration** | 30 min | Crawl rÃ©el + vÃ©rification DB |
| **Code review** | 15 min | Comparaison ligne par ligne sync/async |
| **Documentation** | 15 min | Mise Ã  jour docs |
| **TOTAL** | **2h** | Estimation pessimiste |

---

## âœ… Validation Finale

Avant de considÃ©rer la correction terminÃ©e, vÃ©rifier :

1. [ ] Le crawler async ne lÃ¨ve plus de NameError
2. [ ] Les champs `published_at`, `last_modified`, `etag` sont remplis
3. [ ] La pertinence est calculÃ©e avec `final_lang`
4. [ ] Les tests unitaires passent Ã  100%
5. [ ] Un crawl test via API fonctionne sans erreur
6. [ ] Les logs ne montrent aucun warning sur variables manquantes
7. [ ] La comparaison sync/async montre les mÃªmes rÃ©sultats
8. [ ] La documentation est mise Ã  jour

---

**DerniÃ¨re mise Ã  jour** : 19 octobre 2025
**Statut** : âœ… IMPLÃ‰MENTÃ‰ ET VALIDÃ‰
**PrioritÃ©** : âœ… RÃ‰SOLU - CRAWLER ASYNC OPÃ‰RATIONNEL

---

## âœ… IMPLÃ‰MENTATION RÃ‰USSIE

**Date d'implÃ©mentation** : 19 octobre 2025

### Corrections AppliquÃ©es

âœ… **Phase 1** - Headers HTTP (lignes 171-188)

- Extraction `last_modified` et `etag` depuis response.headers
- Ajout dans `update_data` dict

âœ… **Phase 2** - Dictionnaire Metadata (lignes 205-213)

- CrÃ©ation dict `metadata` avec tous les champs `extraction_result`
- RÃ©utilisation dans `update_data` et calcul pertinence

âœ… **Phase 3** - Parsing published_at (lignes 230-237)

- Parse date string avec `dateutil.parser`
- Gestion erreur avec try/except

âœ… **Phase 4** - Update Data (lignes 239-251)

- Remplacement par `update_data.update({...})`
- Utilisation `metadata.get()` au lieu de `extraction_result.get()`
- Ajout `published_at` dans `update_data`

âœ… **Phase 5** - Calcul Pertinence CORRIGÃ‰ (lignes 256-271)

- TempExpr: Types `Optional[str]` ajoutÃ©s
- `metadata.get("title")` au lieu de `metadata['title']`
- `final_lang or "fr"` au lieu de `metadata_lang or 'fr'`

### Tests ValidÃ©s

- âœ… Compilation Python : RÃ‰USSIE (py_compile sans erreur)
- âœ… ParitÃ© avec crawler sync : CONFIRMÃ‰E
- âœ… Pas de NameError : CONFIRMÃ‰

**Fichier modifiÃ©** : `MyWebIntelligenceAPI/app/core/crawler_engine.py`

---
